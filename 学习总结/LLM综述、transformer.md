## 1、什么是one-hot编码？
ans:利用分类原理，转换为0，1向量，1的位置是独立的，用来表征token，存在问题是矩阵比较稀疏

## 2、one-hot编码和embedding之间的区别?
ans:
one-hot:稀疏，无法捕捉上下文信息，无法学习，只适合简单的离散特征
embedding:密集，可以学习上下文信息，可以通过学习得到语义信息，适合复杂连续特征

## 3、vector quantization和embedding之间的区别?
ans:
vector quantization连续的向量空间->离散化
embedding离散的数据->连续的向量空间


## 4、写出word2vec问题定义和训练目标
## 5、word2vec可以使用对比学习来训练吗
Word2Vec本身并不是通过对比学习（contrastive learning）来训练的，而是通过`预测目标词与其上下文词的概率关系（Skip-Gram 或 CBOW）进行优化`。然而，理论上，对比学习的思想可以应用于Word2Vec的训练框架，并且已经有一些研究尝试将对比学习与词向量的训练结合起来。
对比学习的基本思想
对比学习的目标是通过使*正样本对彼此靠近、负样本对彼此远离*，来学习更加有区分度的特征表示。正样本对通常是相关的，比如同一句子中的词，而负样本对则是不相关的，比如不同句子或上下文中的词。
Word2Vec 与 对比学习的结合
尽管经典的Word2Vec使用的是负采样（negative sampling）或分层Softmax来优化模型，但可以引入对比学习的方法来改进其训练过程。例如，使用对比学习中的"正负对比对"的思想：
正样本对：在Skip-Gram模型中，目标词及其上下文词可以被视为正样本对。
负样本对：负采样（negative sampling）可以被视为一种简单形式的负样本对选择机制。对比学习可以进一步通过更多对比损失（contrastive loss）来强化这种机制，提升模型对正负样本的区分能力。

## 6、word2vec和e5 embedding模型的区别
Word2Vec和现代的embedding模型（如E5 embedding模型）有许多关键区别，特别是在模型架构、训练方法和语义理解能力方面：
1. 模型架构
Word2Vec：
`基于浅层神经网络（单隐藏层）`。它通过`Skip-Gram`或`CBOW`模型学习词的上下文关系，将词汇映射到一个低维向量空间。
主要依赖于`上下文的局部窗口信息`，并不考虑整个句子或文本的全局信息。
E5 Embedding模型：
E5模型通常`基于预训练的大型变压器（Transformer）模型（如BERT、RoBERTa等`），这些模型具有`深层的架构`，能够捕捉更`复杂的上下文和语义关系`。
它们通过`自注意力机制来处理整句或段落中的所有词，并学习到更全局的上下文表示`。
2. 训练方式
Word2Vec：
Word2Vec通常通过`自监督学习`，仅通过`局部上下文关系来预测目标词或上下文词对词向量进行优化`。
它的`训练数据量相对较小`，主要在特定的语料库上进行训练。
E5 Embedding模型：
E5模型是通过`大规模预训练和微调`来学习的，通常是在`极其庞大的数据集上进行训练`，并经过多个阶段的优化。
这些模型在多个任务上微调，可以学习到更通用、更具迁移能力的向量表示。
3. 语义理解能力
Word2Vec：
Word2Vec的语义理解`局限于局部上下文`，无法很好地捕捉句子级别或段落级别的复杂语义。
它`对词的多义性处理能力较弱`，同一个词无论在何种上下文中，其向量表示都是相同的。
E5 Embedding模型：
E5等基于Transformer的模型可以捕捉到上下文中的复杂语义关系，`对句子级别的理解更强`。
它们能够处理`词的多义性`问题，根据上下文`动态生成词的向量`表示。
4. 应用场景
Word2Vec：
常用于`简单的语义相似性任务、词类比任务`等。其模型相对轻量，适合嵌入到一些资源有限的应用中。
E5 Embedding模型：
由于E5等Transformer模型`捕捉全局语义`的能力更强，广泛应用于复杂的自然语言理解任务，如问答系统、文本生成、文档检索等。
5. 计算资源
Word2Vec：
训练速度较`快`，计算成本较`低`。适合于资源有限的场景。
E5 Embedding模型：
由于模型深度大、参数多，训练和推理需要更多的计算资源和时间。
总结来说，Word2Vec适合于较简单的语义任务，且资源消耗较小；而E5等现代模型具有更强的语义理解能力和灵活性，但需要更高的计算资源。

## 7、tokenizer的作用是什么？
Tokenizer（分词器）的作用是将文本数据分割成可以输入模型的更小单元（通常为词或子词），以便机器学习模型能够处理。它是自然语言处理中非常重要的一步。

主要功能：`
文本分割`：
将原始的文本字符串分割成词、子词或字符的序列。例如，将句子“我喜欢学习”分割成“我”、“喜欢”、“学习”三个词。
将文本映射为数字表示：
Tokenizer会将分割后的词或子词映射到唯一的标记ID，这些ID用于输入机器学习模型。例如，“我喜欢学习”可以被映射为 [1, 2, 3] 这样的ID序列。
处理词表外词汇：
Tokenizer通常有一个预定义的词汇表，当遇到不在词汇表中的词时，它可以通过某些机制（如子词分割）将词进一步分解，或将它映射为特殊标记（如[UNK]）。

常见类型：
基于词的Tokenizer：将文本按照空格或标点分割成词。
基于字符的Tokenizer：将文本分割为单个字符。
基于子词的Tokenizer（如BPE、WordPiece）：将罕见的词分割为更小的子词单元，这些方法通常用于大型预训练模型（如BERT、GPT）。
总结
Tokenizer在自然语言处理中的核心作用是将`原始文本转换为模型能够理解的数字化表示`。它决定了`模型如何看到文本并影响模型的性能`。

什么是中文分词？
为什么在transformers框架中的encoder和decoder使用不同的tokenizer
tokenizer和embedding之间的关系
transformers的tokenizer有什么缺陷
embedding空间是离散的还是连续的

attention的本质？
语言数据有什么特性？Transformer是如何做语言建模的？写出transformer的建模函数
注意力机制的本质是什么？
三个注意力有什么区别？ 各自mask有什么特点
transformer训练和推理是怎么样的？为什么说是训练是并行的？
从注意力角度思考为什么需要有位置编码？请用公式佐证
为什么要将X变换到QKV？
transformers训练时的输入输出是什么？
transformers的forward和generation有什么关系？
能否理解transformers架构，就是一个增加编码器“条件”的decoder-only的生成器？
为什么语言建模上用LayerNorm, layernorm的计算过程
为什么位置编码底数为base=10000
为什么需要位置编码
为什么注意力计算需要除于维度sqrt(d)？
给定d=512， 8头每个头维度为64，那么每头注意力分数除于sqrt(512)还是sqrt(64）？
attention计算时需要dropout吗？
什么是auto-regressive，用数学语言描述
从auto-regressive角度分析rnn和transformers两者差异
