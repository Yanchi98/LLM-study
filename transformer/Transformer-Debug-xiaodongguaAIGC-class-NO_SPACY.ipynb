{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手撕Transformer-小冬瓜AIGC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![contetn](image/content.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 预处理requirements/configure/tokenizer/dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext==0.6.0 in /opt/anaconda3/lib/python3.8/site-packages (0.6.0)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.8/site-packages (from torchtext==0.6.0) (2.0.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.8/site-packages (from torchtext==0.6.0) (1.19.2)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.8/site-packages (from torchtext==0.6.0) (4.50.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.8/site-packages (from torchtext==0.6.0) (2.24.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.8/site-packages (from torchtext==0.6.0) (0.2.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.8/site-packages (from torchtext==0.6.0) (1.15.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.8/site-packages (from torch->torchtext==0.6.0) (1.6.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.8/site-packages (from torch->torchtext==0.6.0) (3.0.12)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.8/site-packages (from torch->torchtext==0.6.0) (2.11.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.8/site-packages (from torch->torchtext==0.6.0) (2.5)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.8/site-packages (from torch->torchtext==0.6.0) (4.10.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests->torchtext==0.6.0) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests->torchtext==0.6.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests->torchtext==0.6.0) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.8/site-packages (from sympy->torch->torchtext==0.6.0) (1.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.8/site-packages (from jinja2->torch->torchtext==0.6.0) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from networkx->torch->torchtext==0.6.0) (4.4.2)\n",
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.8/site-packages (3.7.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (4.50.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (50.3.1.post20201107)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.11.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0; python_version < \"3.9\" in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.19.2)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.6.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (20.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/anaconda3/lib/python3.8/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (1.15.0)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.8/site-packages (2.0.1)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.8/site-packages (from torch) (1.6.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.8/site-packages (from torch) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.8/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.8/site-packages (from torch) (2.11.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.8/site-packages (from torch) (2.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.8/site-packages (from sympy->torch) (1.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.8/site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from networkx->torch) (4.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchtext==0.6.0\n",
    "!pip3 install spacy\n",
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 159, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/connection.py\", line 84, in create_connection\n",
      "    raise err\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/connection.py\", line 74, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 61] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 670, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 381, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 978, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 309, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 171, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fc74105e340>: Failed to establish a new connection: [Errno 61] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 726, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\", line 446, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/compatibility.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc74105e340>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/anaconda3/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/spacy/__main__.py\", line 4, in <module>\n",
      "    setup_cli()\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/spacy/cli/_util.py\", line 87, in setup_cli\n",
      "    command(prog_name=COMMAND)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/typer/core.py\", line 778, in main\n",
      "    return _main(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/typer/core.py\", line 216, in _main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/typer/main.py\", line 683, in wrapper\n",
      "    return callback(**use_params)  # type: ignore\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/spacy/cli/download.py\", line 43, in download_cli\n",
      "    download(model, direct, sdist, *ctx.args)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/spacy/cli/download.py\", line 77, in download\n",
      "    compatibility = get_compatibility()\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/spacy/cli/download.py\", line 122, in get_compatibility\n",
      "    r = requests.get(about.__compatibility__)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 76, in get\n",
      "    return request('get', url, params=params, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/compatibility.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc74105e340>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 159, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/connection.py\", line 84, in create_connection\n",
      "    raise err\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/connection.py\", line 74, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 61] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 670, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 381, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 978, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 309, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 171, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fa1fabc03d0>: Failed to establish a new connection: [Errno 61] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 726, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\", line 446, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/compatibility.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fa1fabc03d0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/anaconda3/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/spacy/__main__.py\", line 4, in <module>\n",
      "    setup_cli()\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/spacy/cli/_util.py\", line 87, in setup_cli\n",
      "    command(prog_name=COMMAND)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/typer/core.py\", line 778, in main\n",
      "    return _main(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/typer/core.py\", line 216, in _main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/typer/main.py\", line 683, in wrapper\n",
      "    return callback(**use_params)  # type: ignore\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/spacy/cli/download.py\", line 43, in download_cli\n",
      "    download(model, direct, sdist, *ctx.args)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/spacy/cli/download.py\", line 77, in download\n",
      "    compatibility = get_compatibility()\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/spacy/cli/download.py\", line 122, in get_compatibility\n",
      "    r = requests.get(about.__compatibility__)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 76, in get\n",
      "    return request('get', url, params=params, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/compatibility.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fa1fabc03d0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download de_core_news_sm\n",
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.optim import Adam\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 configure配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 配置参数\n",
    "# GPU device setting\n",
    " \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 模型参数\n",
    "batch_size = 128 # 训练批次 句话\n",
    "max_len = 256    # 单句最大长度 \n",
    "##\n",
    "# padding=10\n",
    "\n",
    "d_model = 512    # 词嵌入向量维度\n",
    "n_layers = 6     # encoder/decoder层数量\n",
    "n_heads = 8      # 注意力头数： 假如有词嵌入维度d_model = 512 / n_heads = 8 => 单头向量维度 512 / 8 = 64，即QKV维度\n",
    "ffn_hidden = 2048 # 前向传播维度。 512 -> 2048 -> 512, 通常也称作proj\n",
    "drop_prob = 0.1  # dropout提升鲁棒性，随机失活一些节点\n",
    "n_hidden = ffn_hidden\n",
    "\n",
    "# optimizer parameter setting\n",
    "init_lr = 1e-5\n",
    "factor = 0.9\n",
    "adam_eps = 5e-9\n",
    "patience = 10\n",
    "warmup = 100\n",
    "epoch = 100\n",
    "clip = 1.0\n",
    "weight_decay = 5e-4\n",
    "inf = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tokenizer 英德文tokenzier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example sentence.\n",
      "['This', 'is', 'an', 'example', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.spacy_de = spacy.load('de_core_news_sm')\n",
    "        self.spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def tokenize_de(self, text):\n",
    "        return [tok.text for tok in self.spacy_de.tokenizer(text)]\n",
    "\n",
    "    def tokenize_en(self, text):\n",
    "        return [tok.text for tok in self.spacy_en.tokenizer(text)]\n",
    "        # example\n",
    "        # doc = nlp('This is an example sentence.')\n",
    "        # tokens = [token.text for token in doc]\n",
    "        # print(tokens)\n",
    "        # ['This', 'is', 'an', 'example', 'sentence', '.']\n",
    "\n",
    "# 加载Token\n",
    "tokenizer = Tokenizer()\n",
    "example = 'This is an example sentence.'\n",
    "tokens = tokenizer.tokenize_en(example)\n",
    "# tokenizer将句子按照单词分成list\n",
    "print(example)\n",
    "print(tokens)\n",
    "# ['This', 'is', 'an', 'example', 'sentence', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two young, white males are outside near many bushes\n",
      "['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes']\n"
     ]
    }
   ],
   "source": [
    "example = 'two young, white males are outside near many bushes'\n",
    "tokens = tokenizer.tokenize_en(example)\n",
    "print(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Dataloader创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset initializing start\n",
      "\n",
      "--------0. 根据spacy mutli30k 创建数据集-------\n",
      "['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n",
      "['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.']\n",
      "29000\n",
      "1000\n",
      "1014\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.datasets.translation import Multi30k\n",
    "class DataLoader:\n",
    "    source: Field = None\n",
    "    target: Field = None\n",
    "    def __init__(self, ext, tokenize_en, tokenize_de, init_token, eos_token):\n",
    "        self.ext = ext\n",
    "        self.tokenize_en = tokenize_en\n",
    "        self.tokenize_de = tokenize_de\n",
    "        self.init_token = init_token\n",
    "        self.eos_token = eos_token\n",
    "        print('dataset initializing start')\n",
    "\n",
    "    def make_dataset(self):\n",
    "        if self.ext == ('.de', '.en'):\n",
    "            self.source = Field(tokenize=self.tokenize_de, init_token=self.init_token, eos_token=self.eos_token,\n",
    "                                lower=True, batch_first=True)\n",
    "            self.target = Field(tokenize=self.tokenize_en, init_token=self.init_token, eos_token=self.eos_token,\n",
    "                                lower=True, batch_first=True)\n",
    "\n",
    "        elif self.ext == ('.en', '.de'):\n",
    "            # Field() 函数返回一个 Field 类的实例，该实例有以下常用方法\n",
    "            # build_vocab：根据数据集构建词汇表。\n",
    "            self.source = Field(tokenize=self.tokenize_en, init_token=self.init_token, eos_token=self.eos_token,\n",
    "                                lower=True, batch_first=True)\n",
    "            self.target = Field(tokenize=self.tokenize_de, init_token=self.init_token, eos_token=self.eos_token,\n",
    "                                lower=True, batch_first=True)\n",
    "        # 拆分数据集\n",
    "        train_data, valid_data, test_data = Multi30k.splits(exts=self.ext, fields=(self.source, self.target))\n",
    "        return train_data, valid_data, test_data\n",
    "\n",
    "    def build_vocab(self, train_data, min_freq):\n",
    "        self.source.build_vocab(train_data, min_freq=min_freq)\n",
    "        self.target.build_vocab(train_data, min_freq=min_freq)\n",
    "\n",
    "    def make_iter(self, train, validate, test, batch_size, device):\n",
    "        train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train, validate, test),\n",
    "                                                                              batch_size=batch_size,\n",
    "                                                                              device=device)\n",
    "        print('dataset initializing done')\n",
    "        return train_iterator, valid_iterator, test_iterator\n",
    "\n",
    "# 需要对整句加上句头句尾token [<sos>, 'This', 'is', 'an', 'example', 'sentence', '.',  <eos>] \n",
    "loader = DataLoader(ext=('.en', '.de'),\n",
    "                    tokenize_en=tokenizer.tokenize_en,\n",
    "                    tokenize_de=tokenizer.tokenize_de,\n",
    "                    init_token='<sos>',\n",
    "                    eos_token='<eos>')\n",
    "\n",
    "# 创建 source/target Field实例（包含数据）\n",
    "print('\\n--------0. 根据spacy mutli30k 创建数据集-------')\n",
    "train, valid, test = loader.make_dataset()\n",
    "print(train.examples[0].src)\n",
    "print(train.examples[0].trg)\n",
    "print(len(train.examples))\n",
    "print(len(test.examples))\n",
    "print(len(valid.examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------1. 查看词表大小-------\n",
      "src vocab size: 5893\n",
      "trg vocab size: 7853\n"
     ]
    }
   ],
   "source": [
    "loader.build_vocab(train_data=train, min_freq=2)\n",
    "print('--------1. 查看词表大小-------')\n",
    "print('src vocab size:', len(loader.source.vocab.stoi)) \n",
    "print('trg vocab size:', len(loader.target.vocab.stoi)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------2. 建立词表后，如何将单词转成token数值-------\n",
      "word \t -> \t token\n",
      "<sos> \t \t 2\n",
      "two \t \t 16\n",
      "young \t \t 24\n",
      ", \t \t 15\n",
      "<eos> \t \t 3\n",
      "<pad> \t \t 1\n"
     ]
    }
   ],
   "source": [
    "print('--------2. 建立词表后，如何将单词转成token数值-------')\n",
    "# print('查看词表:', loader.source.vocab.stoi)\n",
    "print('word \\t -> \\t token')\n",
    "print('<sos> \\t \\t',loader.source.vocab.stoi['<sos>'])\n",
    "print('two \\t \\t',loader.source.vocab.stoi['two'])\n",
    "print('young \\t \\t',loader.source.vocab.stoi['young'])\n",
    "print(', \\t \\t',loader.source.vocab.stoi[','])\n",
    "print('<eos> \\t \\t',loader.source.vocab.stoi['<eos>'])\n",
    "print('<pad> \\t \\t',loader.source.vocab.stoi['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset initializing done\n",
      "----3. 从迭代器中取一对，可见其开头为<sos>2, 结尾<eos>3， 剩余为<pad>1---------------\n",
      "padding的作用：一个batch中有不同的句子， 句子里最大句长为l, 小于l的句子都填充<pad>1\n",
      "tensor([   2,   64,   56,   75,   76,    7,  225,    6, 3853,    5,    3,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1])\n",
      "tensor([  2,   8,  67, 146,   0,  42,  39, 235,   4,   3,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1])\n"
     ]
    }
   ],
   "source": [
    "train_iter, valid_iter, test_iter = loader.make_iter(train, valid, test,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     device=device)\n",
    "print('----3. 从迭代器中取一对，可见其开头为<sos>2, 结尾<eos>3， 剩余为<pad>1---------------')\n",
    "print('padding的作用：一个batch中有不同的句子， 句子里最大句长为l, 小于l的句子都填充<pad>1')\n",
    "for batch in train_iter:\n",
    "    print(batch.src[0])\n",
    "    print(batch.trg[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('----4. 以下词表参数也是模型中重要的部分----')\n",
    "# src_pad_idx = loader.source.vocab.stoi['<pad>']\n",
    "# trg_pad_idx = loader.target.vocab.stoi['<pad>']\n",
    "# trg_sos_idx = loader.target.vocab.stoi['<sos>']\n",
    "src_pad_idx = 1\n",
    "trg_pad_idx = 1\n",
    "trg_sos_idx = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "enc_voc_size = 5893\n",
    "dec_voc_size = 7853\n",
    "\n",
    "# enc_voc_size = len(loader.source.vocab)\n",
    "# print(\"嵌入层的输入参数 {} x 维度 {}\".format(enc_voc_size,d_model))\n",
    "# dec_voc_size = len(loader.target.vocab)\n",
    "# print(\"全链接层输出维度 {} x 输出词表{}：\".format(d_model,dec_voc_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load src shape torch.Size([128, 28])\n",
      "load trg shape torch.Size([128, 26])\n"
     ]
    }
   ],
   "source": [
    "# 从data中获取数据\n",
    "# 仅运行一次，保证测试时使用同一组数据\n",
    "\n",
    "# for i, batch in enumerate(train_iter):\n",
    "#     src = batch.src\n",
    "#     trg = batch.trg\n",
    "#     print(\"save src shape:\",src.shape)\n",
    "#     print(\"save trg shape\",trg.shape)\n",
    "#     torch.save(src, 'tensor_src.pt')\n",
    "#     torch.save(trg, 'tensor_trg.pt')\n",
    "#     break\n",
    "\n",
    "test_src = torch.load('tensor_src.pt')\n",
    "test_trg = torch.load('tensor_trg.pt')\n",
    "print(\"load src shape\", test_src.shape)\n",
    "print(\"load trg shape\", test_trg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    reference = [reference.split()]\n",
    "    candidate = candidate.split()\n",
    "    smoothing_function = nltk.translate.bleu_score.SmoothingFunction()\n",
    "    bleu_score = nltk.translate.bleu_score.sentence_bleu(reference, candidate, smoothing_function=smoothing_function.method1)\n",
    "    return bleu_score\n",
    "\n",
    "# 示例用法\n",
    "reference_sentence = \"The cat is on the mat\"\n",
    "# candidate_sentence = \"The cat is sitting on the mat\"\n",
    "candidate_sentence = \"The cat is on the mat\"\n",
    "bleu = calculate_bleu(reference_sentence, candidate_sentence)\n",
    "print(\"BLEU score:\", bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 score: 0.9230769181065088\n",
      "ROUGE-2 score: 0.7272727223140496\n",
      "ROUGE-L score: 0.9230769181065088\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "def calculate_rouge(reference, candidate):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(candidate, reference)\n",
    "    rouge_1 = scores[0]['rouge-1']['f']\n",
    "    rouge_2 = scores[0]['rouge-2']['f']\n",
    "    rouge_l = scores[0]['rouge-l']['f']\n",
    "    return rouge_1, rouge_2, rouge_l\n",
    "\n",
    "# 示例用法\n",
    "reference_summary = \"The cat is on the mat\"\n",
    "candidate_summary = \"The cat is sitting on the mat\"\n",
    "rouge_1, rouge_2, rouge_l = calculate_rouge(reference_summary, candidate_summary)\n",
    "print(\"ROUGE-1 score:\", rouge_1)\n",
    "print(\"ROUGE-2 score:\", rouge_2)\n",
    "print(\"ROUGE-L score:\", rouge_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER score: 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "import jiwer\n",
    "\n",
    "def calculate_wer(reference, candidate):\n",
    "    wer = jiwer.wer(reference, candidate)\n",
    "    return wer\n",
    "\n",
    "# 示例用法\n",
    "reference_transcription = \"The cat is on the mat\"\n",
    "candidate_transcription = \"The cat is sitting on the mat\"\n",
    "wer = calculate_wer(reference_transcription, candidate_transcription)\n",
    "print(\"WER score:\", wer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 手撕Transformer模型\n",
    "\n",
    "这个章节主要理解模型构造的过程，第3章会自顶向下debug 数据流"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Token Embedding\n",
    "目的将1个token转成一串向量\n",
    "参照Word2Vec算法原理如下图示\n",
    "\n",
    "Embdding Vec\n",
    "数据类型流向 word(string) -> 【token(int) -> vec(list(float))】\n",
    "\n",
    "以下为两个词对应的vec进行比较， 越相近的向量，词性相同\n",
    "![0](image/embeddings-cosine-personality.png)\n",
    "\n",
    "Word2Vec embedding\n",
    "\n",
    "纵轴词表数量， 横轴vec词向量维度， 期望找出当前单词和右边相近的单词向量\n",
    "\n",
    "\n",
    "![1](image/word2vec-lookup-embeddings.png)\n",
    "\n",
    "SkipGram: \n",
    "\n",
    "假设\"我是小冬瓜\", 对于\"冬\"单词与\"小\"和\"瓜\"相近positive，与\"我\"间隔较远\n",
    "![2](image/skipgram-sliding-window-5.png)\n",
    "\n",
    "Data and model\n",
    "\n",
    "则对于\"冬\"则与\"冬-小\"和\"冬-瓜\"相近label则为1， 人为构造负样本\"冬-控\",\"冬-龙\",\"冬-抗\",\"冬-狼\"设置label为0\n",
    "![3](image/word2vec-training-example-2.png)\n",
    "\n",
    "根据所构造的样本，即可训练词表\n",
    "\n",
    "Train error\n",
    "![4](image/word2vec-training-update.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding 实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight torch.Size([14, 512])\n",
      "embedding.weight: tensor([-1.0223,  0.1552, -1.4122,  2.6777,  0.4931,  0.1991, -0.3551, -1.3700,\n",
      "        -0.3048,  0.4387], grad_fn=<SliceBackward0>)\n",
      "tensor([-1.3547,  2.5431, -0.3253,  1.6128,  0.7355, -0.0051, -0.1773,  0.3957,\n",
      "         2.2820, -0.2608], grad_fn=<SliceBackward0>)\n",
      "输入数据 torch.Size([3, 10])\n",
      "输入数据的embedding torch.Size([3, 10, 512])\n",
      "tensor([-1.3547,  2.5431, -0.3253,  1.6128,  0.7355, -0.0051, -0.1773,  0.3957,\n",
      "         2.2820, -0.2608], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "embd_layer = torch.nn.Embedding(14, 512)\n",
    "print('embedding.weight', embd_layer.weight.shape)\n",
    "print('embedding.weight:', embd_layer.weight[3,:10])\n",
    "\n",
    "print(embd_layer.weight[4][:10])\n",
    "\n",
    "input_id = torch.tensor([[2, 4, 5, 6, 7, 8, 3, 1, 1, 1], \n",
    "                      [2, 4, 9, 10,11,12,13,3, 1, 1],\n",
    "                      [2, 6, 7, 8, 9, 10,11,12,13,3]])\n",
    "\n",
    "\n",
    "print(\"输入数据\",input_id.shape)\n",
    "print(\"输入数据的embedding\", embd_layer(input_id).shape)\n",
    "\n",
    "print(embd_layer(input_id)[0][1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding更多直接了解word2vec:\n",
      "按照以上理论可以直接，通过torch创建embedding表\n",
      "torch.Size([5893, 512])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "print(\"embedding更多直接了解word2vec:\")\n",
    "print(\"按照以上理论可以直接，通过torch创建embedding表\")\n",
    "a = nn.Embedding(enc_voc_size, d_model)\n",
    "# embedding_layer = nn.Embedding(14, 128)\n",
    "print(a.weight.shape) # 14 * 128\n",
    "print(input_id.shape) # \n",
    "x = a(input_id)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenEmbedding(5893, 512, padding_idx=1)\n",
      "TokenEmbedding(7853, 512, padding_idx=1)\n"
     ]
    }
   ],
   "source": [
    "# 创建Token embedding类\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(TokenEmbedding, self).__init__(vocab_size, d_model, padding_idx=1)\n",
    "        \n",
    "test_src_token = TokenEmbedding(enc_voc_size, d_model) #对 src：en 进行embedding\n",
    "test_trg_token = TokenEmbedding(dec_voc_size, d_model) #对 trg：de 进行embedding\n",
    "print(test_src_token) \n",
    "print(test_trg_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 position encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Position 编码公式\n",
    "\n",
    "十进制13  ->  二进制(1,1,0,1) 这是一种位置编码向量: transformer中则使用连续函数描述向量的生成。\n",
    "\n",
    "可直接记住公式， 也可以尝试通俗理解以下过程\n",
    "\n",
    "(1,1,0,1)  两两成组 (1,1) (0,1) -> 4维/2=2组： 两组index为 i+1, i \n",
    "\n",
    "position encoding后为： (sin(13/(i+1)),cos/(13(i+1))、 ((sin(13/i),cos(13/i)))\n",
    "\n",
    "则最后 (1,1,0,1) ->  (sin(13/(i+1)),cos(13(i+1))、 ((sin(13/i),cos(13/i)))\n",
    "\n",
    "\n",
    "![title](image/positional_encoding.jpg)\n",
    "\n",
    "\n",
    "以下为一种可视化理解如何从p,i变量生成位置编码\n",
    "\n",
    "![pos](image/position_embeding_pos.png)\n",
    "![pos_i](image/Fhc4M.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 512])\n",
      "tensor([-0.5064, -0.8623,  0.8102,  0.5862, -0.9944,  0.1054,  0.4133, -0.9106,\n",
      "         0.7891,  0.6142, -0.5736,  0.8192, -0.9598, -0.2807, -0.3029, -0.9530,\n",
      "         0.4024, -0.9155,  0.7761, -0.6306,  0.9018, -0.4321,  0.9040, -0.4274,\n",
      "         0.7908, -0.6121,  0.4624, -0.8867, -0.1569, -0.9876, -0.8389, -0.5443,\n",
      "        -0.8985,  0.4391,  0.0994,  0.9950,  0.9971,  0.0763,  0.0795, -0.9968,\n",
      "        -0.9965,  0.0837,  0.3968,  0.9179,  0.6316, -0.7753, -0.9985, -0.0547,\n",
      "         0.6582,  0.7528, -0.0600, -0.9982, -0.4476,  0.8942,  0.7570, -0.6534,\n",
      "        -0.9037,  0.4281,  0.9573, -0.2891, -0.9663,  0.2576,  0.9428, -0.3334,\n",
      "        -0.8641,  0.5033,  0.6826, -0.7308, -0.3510,  0.9364, -0.1308, -0.9914,\n",
      "         0.6554,  0.7553, -0.9834, -0.1812,  0.8371, -0.5471, -0.1461,  0.9893,\n",
      "        -0.7031, -0.7111,  0.9773, -0.2120, -0.2734,  0.9619, -0.7682, -0.6402,\n",
      "         0.8635, -0.5043,  0.2464,  0.9692, -0.9994,  0.0346,  0.1163, -0.9932,\n",
      "         0.9787,  0.2055, -0.2364,  0.9717, -0.9773, -0.2120,  0.1337, -0.9910,\n",
      "         1.0000,  0.0018,  0.1804,  0.9836, -0.9157,  0.4018, -0.6388, -0.7694,\n",
      "         0.5250, -0.8511,  0.9832,  0.1826,  0.2260,  0.9741, -0.7772,  0.6292,\n",
      "        -0.9218, -0.3877, -0.1316, -0.9913,  0.7655, -0.6434,  0.9639,  0.2661,\n",
      "         0.3591,  0.9333, -0.5089,  0.8608, -0.9860,  0.1669, -0.7832, -0.6217,\n",
      "        -0.0908, -0.9959,  0.6361, -0.7716,  0.9917, -0.1283,  0.8267,  0.5626,\n",
      "         0.2694,  0.9630, -0.3922,  0.9199, -0.8698,  0.4933, -0.9936, -0.1127,\n",
      "        -0.7521, -0.6590, -0.2630, -0.9648,  0.2940, -0.9558,  0.7466, -0.6653,\n",
      "         0.9796, -0.2012,  0.9541,  0.2995,  0.7017,  0.7125,  0.3014,  0.9535,\n",
      "        -0.1482,  0.9890, -0.5543,  0.8323, -0.8473,  0.5312, -0.9885,  0.1510,\n",
      "        -0.9708, -0.2399, -0.8125, -0.5829, -0.5496, -0.8354, -0.2263, -0.9741,\n",
      "         0.1126, -0.9936,  0.4277, -0.9039,  0.6884, -0.7253,  0.8751, -0.4840,\n",
      "         0.9781, -0.2083,  0.9972,  0.0751,  0.9394,  0.3429,  0.8167,  0.5771,\n",
      "         0.6441,  0.7650,  0.4377,  0.8991,  0.2133,  0.9770, -0.0147,  0.9999,\n",
      "        -0.2340,  0.9722, -0.4349,  0.9005, -0.6100,  0.7924, -0.7545,  0.6563,\n",
      "        -0.8658,  0.5004, -0.9431,  0.3326, -0.9871,  0.1599, -0.9999, -0.0117,\n",
      "        -0.9842, -0.1768, -0.9434, -0.3316, -0.8811, -0.4729, -0.8011, -0.5986,\n",
      "        -0.7070, -0.7072, -0.6024, -0.7982, -0.4904, -0.8715, -0.3741, -0.9274,\n",
      "        -0.2560, -0.9667, -0.1383, -0.9904, -0.0228, -0.9997,  0.0889, -0.9960,\n",
      "         0.1956, -0.9807,  0.2964, -0.9551,  0.3907, -0.9205,  0.4778, -0.8785,\n",
      "         0.5577, -0.8301,  0.6301, -0.7765,  0.6952, -0.7189,  0.7529, -0.6581,\n",
      "         0.8036, -0.5951,  0.8476, -0.5307,  0.8851, -0.4654,  0.9165, -0.4000,\n",
      "         0.9423, -0.3348,  0.9627, -0.2704,  0.9783, -0.2072,  0.9894, -0.1453,\n",
      "         0.9964, -0.0850,  0.9996, -0.0266,  0.9996,  0.0298,  0.9964,  0.0842,\n",
      "         0.9907,  0.1364,  0.9825,  0.1864,  0.9722,  0.2342,  0.9601,  0.2798,\n",
      "         0.9464,  0.3231,  0.9313,  0.3643,  0.9150,  0.4034,  0.8978,  0.4404,\n",
      "         0.8797,  0.4754,  0.8610,  0.5085,  0.8418,  0.5397,  0.8222,  0.5692,\n",
      "         0.8023,  0.5969,  0.7823,  0.6230,  0.7621,  0.6475,  0.7419,  0.6705,\n",
      "         0.7218,  0.6921,  0.7018,  0.7124,  0.6819,  0.7314,  0.6623,  0.7492,\n",
      "         0.6429,  0.7659,  0.6238,  0.7816,  0.6050,  0.7962,  0.5866,  0.8099,\n",
      "         0.5685,  0.8227,  0.5508,  0.8346,  0.5335,  0.8458,  0.5166,  0.8562,\n",
      "         0.5000,  0.8660,  0.4839,  0.8751,  0.4682,  0.8836,  0.4530,  0.8915,\n",
      "         0.4381,  0.8989,  0.4236,  0.9058,  0.4096,  0.9123,  0.3959,  0.9183,\n",
      "         0.3827,  0.9239,  0.3698,  0.9291,  0.3573,  0.9340,  0.3452,  0.9385,\n",
      "         0.3335,  0.9427,  0.3222,  0.9467,  0.3112,  0.9503,  0.3005,  0.9538,\n",
      "         0.2902,  0.9570,  0.2803,  0.9599,  0.2706,  0.9627,  0.2613,  0.9653,\n",
      "         0.2522,  0.9677,  0.2435,  0.9699,  0.2351,  0.9720,  0.2269,  0.9739,\n",
      "         0.2190,  0.9757,  0.2114,  0.9774,  0.2040,  0.9790,  0.1969,  0.9804,\n",
      "         0.1901,  0.9818,  0.1834,  0.9830,  0.1770,  0.9842,  0.1708,  0.9853,\n",
      "         0.1648,  0.9863,  0.1591,  0.9873,  0.1535,  0.9882,  0.1481,  0.9890,\n",
      "         0.1429,  0.9897,  0.1379,  0.9904,  0.1330,  0.9911,  0.1284,  0.9917,\n",
      "         0.1239,  0.9923,  0.1195,  0.9928,  0.1153,  0.9933,  0.1112,  0.9938,\n",
      "         0.1073,  0.9942,  0.1035,  0.9946,  0.0999,  0.9950,  0.0964,  0.9953,\n",
      "         0.0930,  0.9957,  0.0897,  0.9960,  0.0865,  0.9962,  0.0835,  0.9965,\n",
      "         0.0806,  0.9968,  0.0777,  0.9970,  0.0750,  0.9972,  0.0723,  0.9974,\n",
      "         0.0698,  0.9976,  0.0673,  0.9977,  0.0649,  0.9979,  0.0626,  0.9980,\n",
      "         0.0604,  0.9982,  0.0583,  0.9983,  0.0562,  0.9984,  0.0543,  0.9985,\n",
      "         0.0523,  0.9986,  0.0505,  0.9987,  0.0487,  0.9988,  0.0470,  0.9989,\n",
      "         0.0453,  0.9990,  0.0437,  0.9990,  0.0422,  0.9991,  0.0407,  0.9992,\n",
      "         0.0393,  0.9992,  0.0379,  0.9993,  0.0365,  0.9993,  0.0352,  0.9994,\n",
      "         0.0340,  0.9994,  0.0328,  0.9995,  0.0316,  0.9995,  0.0305,  0.9995,\n",
      "         0.0294,  0.9996,  0.0284,  0.9996,  0.0274,  0.9996,  0.0264,  0.9997])\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, device):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
    "        self.encoding.requires_grad = False  \n",
    "        pos = torch.arange(0, max_len, device=device)\n",
    "        pos = pos.float().unsqueeze(dim=1)\n",
    "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "        # 512\n",
    "        # 2x256 cos sin\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        return self.encoding[:seq_len, :]\n",
    "\n",
    "test_pos_encoding = PositionalEncoding(d_model, max_len, device)\n",
    "print(test_pos_encoding.encoding.shape)\n",
    "print(test_pos_encoding.encoding[255,:]) # 255 is position \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer norm 公式\n",
    "\n",
    "原图公式与主要四行代码一一对应\n",
    "\n",
    "layernorm作用在最后一维进行归一化\n",
    "\n",
    "![layer](image/layer_norm.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # layernorm作用在(-1) 最后一维进行归一化\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, unbiased=False, keepdim=True)\n",
    "        out = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        out = self.gamma * out + self.beta\n",
    "        return out\n",
    "    \n",
    "test_ln = LayerNorm(d_model)\n",
    "print(test_ln.gamma.shape)\n",
    "print(test_ln.beta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Scaled-Dot-Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaled dot product 图示\n",
    "class ScaleDotProductAttention(nn.Module)\n",
    "![attention](image/scale_dot_product_attention.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单头注意力机制\n",
    "# 图-代码-公式完全对应， 第3章节有详细推导\n",
    "# 先记住实现\n",
    "class ScaleDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaleDotProductAttention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None, e=1e-12):\n",
    "        batch_size, head, length, d_tensor = k.size() # /n_embd/8\n",
    "        k_t = k.transpose(2, 3) \n",
    "        score = (q @ k_t) / math.sqrt(d_tensor) #qk^t/dk\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask == 0, -10000)\n",
    "        score = self.softmax(score) #softmax(qk^t/dk)\n",
    "        v = score @ v #softmax(qk^t/dk)*V\n",
    "        return v, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 position wise feed forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ffn\n",
    "![layer](image/positionwise_feed_forward.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PositionwiseFeedForward(\n",
      "  (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 前向传播，当成神经网络全链接层 + 隐含层理解\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "ffw = PositionwiseFeedForward(d_model, ffn_hidden)\n",
    "print(ffw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Multi-Head-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multi-head-attention\n",
    "![multiheadattention](image/multi_head_attention.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention(\n",
      "  (attention): ScaleDotProductAttention(\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
      ")\n",
      "512 8\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    # dmodel_n_embed; 512 8\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.attention = ScaleDotProductAttention()\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_concat = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)  # 对应图里liner：先对QKV投影\n",
    "        q, k, v = self.split(q), self.split(k), self.split(v) # Q->Q0, Q1, ... \n",
    "        out, attention = self.attention(q, k, v, mask=mask) # 每一头计算attention，z0, z1, ...\n",
    "        out = self.concat(out) # 将每一头拼接 z0 z1 .. = z\n",
    "        out = self.w_concat(out) # z -> linner -> output\n",
    "        return out\n",
    "\n",
    "    # 先不用看实现，后面会讲\n",
    "    def split(self, tensor):\n",
    "        batch_size, length, d_model = tensor.size()\n",
    "        d_tensor = d_model // self.n_head\n",
    "        tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(1, 2)\n",
    "        return tensor\n",
    "\n",
    "    # 先不用看实现，后面会讲\n",
    "    def concat(self, tensor):\n",
    "        batch_size, head, length, d_tensor = tensor.size()\n",
    "        d_model = head * d_tensor\n",
    "        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n",
    "        return tensor\n",
    "    \n",
    "test_multihead_attention = MultiHeadAttention(d_model, n_heads)\n",
    "print(test_multihead_attention)\n",
    "print(d_model, n_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Transformer Embeding\n",
    "\n",
    "model.png：见input后的操作符token+position\n",
    "\n",
    "![model](image/model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerEmbedding(\n",
      "  (tok_emb): TokenEmbedding(5893, 512, padding_idx=1)\n",
      "  (pos_emb): PositionalEncoding()\n",
      "  (drop_out): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Transformer—embedding数据流：【嵌入向量+位置编码 ->  X】 -> QKV -> X\n",
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob, device):\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model, max_len, device)\n",
    "        self.drop_out = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tok_emb = self.tok_emb(x)\n",
    "        pos_emb = self.pos_emb(x)\n",
    "        # 记住这里还有个Dropout\n",
    "        return self.drop_out(tok_emb + pos_emb)\n",
    "    \n",
    "test_embedding = TransformerEmbedding(enc_voc_size, d_model, max_len, drop_prob, device)\n",
    "print(test_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Transformer Encode Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编解码：enc-dec\n",
    "\n",
    "特别注意【每个 decoder block】都需要接受encoder的输出\n",
    "\n",
    "![enc-dec](image/enc_dec.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderLayer(\n",
      "  (attention): MultiHeadAttention(\n",
      "    (attention): ScaleDotProductAttention(\n",
      "      (softmax): Softmax(dim=-1)\n",
      "    )\n",
      "    (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (norm1): LayerNorm()\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (ffn): PositionwiseFeedForward(\n",
      "    (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (norm2): LayerNorm()\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 单独一个encoder block\n",
    "# 多个 encoder block 组成一个 encoder\n",
    "\n",
    "# 可以叫encoder-layer 也可以叫 encoder-block\n",
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm1 = LayerNorm(d_model=d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = LayerNorm(d_model=d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x, s_mask):\n",
    "        # 1. compute self attention\n",
    "        # print(\"encoder layer x: \", x.shape)\n",
    "        _x = x\n",
    "        x = self.attention(q=x, k=x, v=x, mask=s_mask)\n",
    "        \n",
    "        # 2. add and norm\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + _x)\n",
    "        \n",
    "        # 3. positionwise feed forward network\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "      \n",
    "        # 4. add and norm\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x + _x)\n",
    "        return x\n",
    "test_encoder_block = EncoderLayer(d_model, ffn_hidden, n_heads, drop_prob)\n",
    "print(test_encoder_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Transformer Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecoderLayer(\n",
      "  (self_attention): MultiHeadAttention(\n",
      "    (attention): ScaleDotProductAttention(\n",
      "      (softmax): Softmax(dim=-1)\n",
      "    )\n",
      "    (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (norm1): LayerNorm()\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (enc_dec_attention): MultiHeadAttention(\n",
      "    (attention): ScaleDotProductAttention(\n",
      "      (softmax): Softmax(dim=-1)\n",
      "    )\n",
      "    (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (norm2): LayerNorm()\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  (ffn): PositionwiseFeedForward(\n",
      "    (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (norm3): LayerNorm()\n",
      "  (dropout3): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm1 = LayerNorm(d_model=d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        # enc_dec_attention使用encoder的 Q， decoder的 K，V\n",
    "        self.enc_dec_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm2 = LayerNorm(d_model=d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm3 = LayerNorm(d_model=d_model)\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, dec, enc, t_mask, s_mask):\n",
    "        # 1. compute self attention\n",
    "        _x = dec\n",
    "        x = self.self_attention(q=dec, k=dec, v=dec, mask=t_mask)#下三角矩阵\n",
    "        \n",
    "        # 2. add and norm\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + _x)\n",
    "\n",
    "        if enc is not None:\n",
    "            # 3. compute encoder - decoder attention\n",
    "            _x = x\n",
    "            x = self.enc_dec_attention(q=x, k=enc, v=enc, mask=s_mask) # \n",
    "            \n",
    "            # 4. add and norm\n",
    "            x = self.dropout2(x)\n",
    "            x = self.norm2(x + _x)\n",
    "\n",
    "        # 5. positionwise feed forward network\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        # 6. add and norm\n",
    "        x = self.dropout3(x)\n",
    "        x = self.norm3(x + _x)\n",
    "        return x\n",
    "test_decoder_block = DecoderLayer(d_model, ffn_hidden, n_heads, drop_prob)\n",
    "print(test_decoder_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder block size :  6\n",
      "Encoder(\n",
      "  (emb): TransformerEmbedding(\n",
      "    (tok_emb): TokenEmbedding(5893, 512, padding_idx=1)\n",
      "    (pos_emb): PositionalEncoding()\n",
      "    (drop_out): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0-5): 6 x EncoderLayer(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (attention): ScaleDotProductAttention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (relu): ReLU()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEmbedding(d_model=d_model,\n",
    "                                        max_len=max_len,\n",
    "                                        vocab_size=enc_voc_size,\n",
    "                                        drop_prob=drop_prob,\n",
    "                                        device=device)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model,\n",
    "                                                  ffn_hidden=ffn_hidden,\n",
    "                                                  n_head=n_head,\n",
    "                                                  drop_prob=drop_prob)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, s_mask):\n",
    "        x = self.emb(x)\n",
    "        # 每个encoder block的输入输出tensor是一致的\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, s_mask)\n",
    "        return x\n",
    "    \n",
    "\n",
    "test_encoder = Encoder(enc_voc_size, max_len, d_model, ffn_hidden, n_heads, n_layers, drop_prob, device)\n",
    "print(\"encoder block size : \", len(test_encoder.layers))\n",
    "print(test_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder block size :  6\n",
      "Encoder(\n",
      "  (emb): TransformerEmbedding(\n",
      "    (tok_emb): TokenEmbedding(5893, 512, padding_idx=1)\n",
      "    (pos_emb): PositionalEncoding()\n",
      "    (drop_out): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0-5): 6 x EncoderLayer(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (attention): ScaleDotProductAttention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (relu): ReLU()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEmbedding(d_model=d_model,\n",
    "                                        drop_prob=drop_prob,\n",
    "                                        max_len=max_len,\n",
    "                                        vocab_size=dec_voc_size,\n",
    "                                        device=device)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model=d_model,\n",
    "                                                  ffn_hidden=ffn_hidden,\n",
    "                                                  n_head=n_head,\n",
    "                                                  drop_prob=drop_prob)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "        self.linear = nn.Linear(d_model, dec_voc_size)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        trg = self.emb(trg)\n",
    "\n",
    "        # 这里的每个layer，都有decoder的enc_src输入\n",
    "        for layer in self.layers:\n",
    "            trg = layer(trg, enc_src, trg_mask, src_mask) # src_trg_mask\n",
    "\n",
    "        # pass to LM head\n",
    "        output = self.linear(trg)\n",
    "        return output\n",
    "\n",
    "test_decoder = Decoder(dec_voc_size, max_len, d_model, ffn_hidden, n_heads, n_layers, drop_prob, device)\n",
    "print(\"decoder block size : \", len(test_decoder.layers))\n",
    "print(test_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Transformer结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整的Transfomer 类， 创建encoder / decoder\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, src_pad_idx, trg_pad_idx, trg_sos_idx, enc_voc_size, dec_voc_size, d_model, n_head, max_len,\n",
    "                 ffn_hidden, n_layers, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.trg_sos_idx = trg_sos_idx\n",
    "        self.device = device\n",
    "        self.encoder = Encoder(d_model=d_model,\n",
    "                               n_head=n_head,\n",
    "                               max_len=max_len,\n",
    "                               ffn_hidden=ffn_hidden,\n",
    "                               enc_voc_size=enc_voc_size,\n",
    "                               drop_prob=drop_prob,\n",
    "                               n_layers=n_layers,\n",
    "                               device=device)\n",
    "\n",
    "        self.decoder = Decoder(d_model=d_model,\n",
    "                               n_head=n_head,\n",
    "                               max_len=max_len,\n",
    "                               ffn_hidden=ffn_hidden,\n",
    "                               dec_voc_size=dec_voc_size,\n",
    "                               drop_prob=drop_prob,\n",
    "                               n_layers=n_layers,\n",
    "                               device=device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_pad_mask(src, src, self.src_pad_idx, self.src_pad_idx)\n",
    "\n",
    "        src_trg_mask = self.make_pad_mask(trg, src, self.trg_pad_idx, self.src_pad_idx)\n",
    "\n",
    "        trg_mask = self.make_pad_mask(trg, trg, self.trg_pad_idx, self.trg_pad_idx) * \\\n",
    "                   self.make_no_peak_mask(trg, trg)\n",
    "        # encoder计算流程 src -> encoder -> enc_src\n",
    "        # decoder计算流程 enc_src + trg -> decoder  -> output\n",
    "        # 关于Mask后面会讲解\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        output = self.decoder(trg, enc_src, trg_mask, src_trg_mask)\n",
    "        return output\n",
    "\n",
    "    def make_pad_mask(self, q, k, q_pad_idx, k_pad_idx):\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "\n",
    "        # batch_size x 1 x 1 x len_k\n",
    "        k = k.ne(k_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # batch_size x 1 x len_q x len_k\n",
    "        k = k.repeat(1, 1, len_q, 1)\n",
    "\n",
    "        # batch_size x 1 x len_q x 1\n",
    "        q = q.ne(q_pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "        # batch_size x 1 x len_q x len_k\n",
    "        q = q.repeat(1, 1, 1, len_k)\n",
    "\n",
    "        mask = k & q\n",
    "        return mask\n",
    "\n",
    "    def make_no_peak_mask(self, q, k):\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "        # len_q x len_k\n",
    "        mask = torch.tril(torch.ones(len_q, len_k)).type(torch.BoolTensor).to(self.device)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 调试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 创建Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-237bd4a0389f>:19: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  nn.init.kaiming_uniform(m.weight.data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (emb): TransformerEmbedding(\n",
       "      (tok_emb): TokenEmbedding(5893, 512, padding_idx=1)\n",
       "      (pos_emb): PositionalEncoding()\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (emb): TransformerEmbedding(\n",
       "      (tok_emb): TokenEmbedding(7853, 512, padding_idx=1)\n",
       "      (pos_emb): PositionalEncoding()\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (enc_dec_attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm3): LayerNorm()\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=512, out_features=7853, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformer为./models/transformer.py里的模型类，包含多个对象和方法\n",
    "\n",
    "model = Transformer(src_pad_idx=src_pad_idx,\n",
    "                    trg_pad_idx=trg_pad_idx,\n",
    "                    trg_sos_idx=trg_sos_idx,\n",
    "                    d_model=d_model,\n",
    "                    enc_voc_size=enc_voc_size,\n",
    "                    dec_voc_size=dec_voc_size,\n",
    "                    max_len=max_len,\n",
    "                    ffn_hidden=ffn_hidden,\n",
    "                    n_head=n_heads,\n",
    "                    n_layers=n_layers,\n",
    "                    drop_prob=drop_prob,\n",
    "                    device=device).to(device)\n",
    "\n",
    "# 使用kaiming_uniform对model初始化\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.kaiming_uniform(m.weight.data)\n",
    "        \n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 创建调试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load src shape torch.Size([128, 28])\n",
      "load trg shape torch.Size([128, 26])\n"
     ]
    }
   ],
   "source": [
    "# # 从data中获取数据\n",
    "# for i, batch in enumerate(train_iter):\n",
    "#     src = batch.src\n",
    "#     trg = batch.trg\n",
    "#     print(\"save src shape:\",src.shape)\n",
    "#     print(\"save trg shape\",trg.shape)\n",
    "#     torch.save(src, 'tensor_src.pt')\n",
    "#     torch.save(trg, 'tensor_trg.pt')\n",
    "#     break\n",
    "\n",
    "test_src = torch.load('tensor_src.pt')\n",
    "test_trg = torch.load('tensor_trg.pt')\n",
    "print(\"load src shape\", test_src.shape)\n",
    "print(\"load trg shape\", test_trg.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load src shape torch.Size([128, 28])\n",
      "load trg shape torch.Size([128, 26])\n",
      "batch size : 128 and src length: 28 \n",
      "batch size : 128 and trg length: 26 \n",
      "src [0]:  tensor([   2,  781,  636, 1151,   51,    8,    4,  266, 3532,   11,    0,    4,\n",
      "           0, 3942,    5,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1])\n",
      "trg [0]:  tensor([   2,    5,  959,    0,   19,   28,  382, 2431,   10, 7061,    5,    0,\n",
      "           4,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1])\n",
      "src_pad_idx: 1\n",
      "trg_pad_idx: 1\n",
      "trg_sos_idx: 2\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集, 从dataloader中获取\n",
    "# 接下来所有数据计算，都基于batch(128)\n",
    "\n",
    "src = torch.load('tensor_src.pt')\n",
    "trg = torch.load('tensor_trg.pt')\n",
    "print(\"load src shape\", src.shape)\n",
    "print(\"load trg shape\", trg.shape)\n",
    "print('batch size : {} and src length: {} '.format(src.shape[0], src.shape[1]))\n",
    "print('batch size : {} and trg length: {} '.format(trg.shape[0], trg.shape[1]))\n",
    "print('src [0]: ', src[0])\n",
    "print('trg [0]: ', trg[0])\n",
    "print('src_pad_idx:',src_pad_idx)\n",
    "print('trg_pad_idx:',trg_pad_idx)\n",
    "print('trg_sos_idx:',trg_sos_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 创建mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_mask: torch.Size([128, 1, 28, 28])\n",
      "src_trg_mask: torch.Size([128, 1, 26, 28])\n",
      "trg_mask: torch.Size([128, 1, 26, 26])\n"
     ]
    }
   ],
   "source": [
    "# 根据pad信息，创建mask，先忽略实现细节\n",
    "src_mask = model.make_pad_mask(src, src, src_pad_idx, src_pad_idx)\n",
    "src_trg_mask = model.make_pad_mask(trg, src, trg_pad_idx, src_pad_idx)\n",
    "trg_mask = model.make_pad_mask(trg, trg, trg_pad_idx, trg_pad_idx) * \\\n",
    "            model.make_no_peak_mask(trg, trg)\n",
    "print(\"src_mask:\", src_mask.shape)\n",
    "print(\"src_trg_mask:\", src_trg_mask.shape)\n",
    "print(\"trg_mask:\", trg_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(src_mask[0][0].int())\n",
    "# print(src_trg_mask[0][0].int())\n",
    "# # trg.Q.shape() * src.K^T.shape()\n",
    "# print(trg_mask[0][0].int()) # 下三角"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 图解Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![all](image/the_transformer_3.png)\n",
    "![all](image/The_transformer_encoders_decoders.png)\n",
    "![all](image/The_transformer_encoder_decoder_stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 计算src->[encoder->decoder]->target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 28])\n",
      "torch.Size([128, 28, 512])\n",
      "torch.Size([128, 26, 7853])\n",
      "decode voc size: 7853\n",
      "d_model: 512\n"
     ]
    }
   ],
   "source": [
    "# # transformer 编码层和解码层计算\n",
    "# print(\"查看模型：\", model)\n",
    "enc_src = model.encoder(src, src_mask)\n",
    "output = model.decoder(trg, enc_src, trg_mask, src_trg_mask)\n",
    "print(src.shape)\n",
    "print(enc_src.shape)\n",
    "print(output.shape)\n",
    "print(\"decode voc size:\", dec_voc_size)\n",
    "print(\"d_model:\", d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![embedding](image/transformer_positional_encoding_vectors.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: torch.Size([128, 28, 512])\n",
      "emb_src: torch.Size([128, 28, 512])\n",
      "n_layers: 6\n",
      "encode layers: 6\n",
      "encoder_src: torch.Size([128, 28, 512])\n",
      "encoder_src: torch.Size([128, 28, 512])\n",
      "encoder_src: torch.Size([128, 28, 512])\n",
      "encoder_src: torch.Size([128, 28, 512])\n",
      "encoder_src: torch.Size([128, 28, 512])\n",
      "encoder_src: torch.Size([128, 28, 512])\n"
     ]
    }
   ],
   "source": [
    "# encoder 编码层计算\n",
    "# encoder包含emb和n_layers层\n",
    "\n",
    "emb_src = model.encoder.emb(src)\n",
    "print('src:', emb_src.shape)\n",
    "print('emb_src:', emb_src.shape)\n",
    "print('n_layers:', n_layers)\n",
    "print('encode layers:', len(model.encoder.layers))\n",
    "# encoder0 -> encoder1\n",
    "for layer in model.encoder.layers:\n",
    "    encoder_src = layer(emb_src, src_mask)\n",
    "    print('encoder_src:', encoder_src.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 计算input->embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数值position\n",
    "![embedding-sample](image/transformer_positional_encoding_example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerEmbedding(\n",
      "  (tok_emb): TokenEmbedding(5893, 512, padding_idx=1)\n",
      "  (pos_emb): PositionalEncoding()\n",
      "  (drop_out): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "src: torch.Size([128, 28])\n",
      "tok_emb: torch.Size([128, 28, 512])\n",
      "pos_emb: torch.Size([28, 512])\n",
      "emb_out: torch.Size([128, 28, 512])\n",
      "\n",
      "-----------------------手撕position编码-----------------------\n",
      "位置编码向量tensor:  torch.Size([256, 512])\n",
      "pos: torch.Size([256])\n",
      "pos 增加一个维度后: torch.Size([256, 1])\n",
      "_2i  torch.Size([256])\n",
      "_2i[0:10]  tensor([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18.])\n",
      "赋值pos_embeding\n",
      "--------------: torch.Size([256, 512])\n",
      "打印前10个数据 tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000],\n",
      "        [ 0.8415,  0.5403,  0.8219,  0.5697,  0.8020],\n",
      "        [ 0.9093, -0.4161,  0.9364, -0.3509,  0.9581],\n",
      "        [ 0.1411, -0.9900,  0.2451, -0.9695,  0.3428],\n",
      "        [-0.7568, -0.6536, -0.6572, -0.7537, -0.5486]])\n",
      "128\n",
      "28\n",
      "torch.Size([28, 512])\n"
     ]
    }
   ],
   "source": [
    "# embedding 嵌入层计算\n",
    "# models/embedding/transformer_embedding.py\n",
    "# class TransformerEmbedding(nn.Module)\n",
    "\n",
    "emb = model.encoder.emb\n",
    "print(emb)\n",
    "tok_emb = emb.tok_emb(src)\n",
    "pos_emb = emb.pos_emb(src)\n",
    "emb_out = emb.drop_out(tok_emb + pos_emb)\n",
    "print('src:', src.shape)\n",
    "print('tok_emb:', tok_emb.shape)\n",
    "print('pos_emb:', pos_emb.shape)\n",
    "print('emb_out:', emb_out.shape)\n",
    "\n",
    "# tok_emb 使用 nn.embedding\n",
    "# pos_emb 计算如下\n",
    "# 512 / 2[cos/sin] -> i 256\n",
    "print('\\n-----------------------手撕position编码-----------------------')\n",
    "# 位置编码仅计算一次\n",
    "emb.pos_emb.encoding = torch.zeros(max_len, d_model)\n",
    "print(\"位置编码向量tensor: \",emb.pos_emb.encoding.shape)\n",
    "\n",
    "emb.pos_emb.encoding.requires_grad = False  # we don't need to compute gradient\n",
    "pos = torch.arange(0, max_len)\n",
    "print('pos:', pos.shape)\n",
    "pos = pos.float().unsqueeze(dim=1)\n",
    "print('pos 增加一个维度后:', pos.shape)\n",
    "\n",
    "_2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "print('_2i ', _2i.shape)\n",
    "print('_2i[0:10] ', _2i[:10])\n",
    "\n",
    "print('赋值pos_embeding')\n",
    "emb.pos_emb.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "emb.pos_emb.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "print('--------------:', emb.pos_emb.encoding.shape)\n",
    "print(\"打印前10个数据\", emb.pos_emb.encoding[0:5, 0:5])\n",
    "\n",
    "# 使用时\n",
    "batch_size, seq_len = src.size()\n",
    "print(batch_size)\n",
    "print(seq_len)\n",
    "print(emb.pos_emb.encoding[:seq_len, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2454, -0.2193,  0.9135, -0.2342],\n",
      "        [-1.1709, -1.0669, -1.1716, -0.6057],\n",
      "        [-1.7809,  0.2532,  0.3304, -0.5647],\n",
      "        [-1.8282, -0.3987,  0.3774, -0.9318],\n",
      "        [ 0.2312,  0.8713,  0.4660, -0.7696]])\n",
      "tensor([-1.8282, -0.3987,  0.3774, -0.9318])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "PE = torch.randn(5, 4)\n",
    "\n",
    "print(PE)\n",
    "# print(PE[0,:])\n",
    "# print(PE[1,:])\n",
    "print(PE[3,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 计算 embedding->[encoder block]->output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder block 主要包含multi-head attention 和 feed forward position两个主要模块\n",
    "\n",
    "![encoder_block](image/Transformer_encoder.png)\n",
    "\n",
    "----\n",
    "\n",
    "encoder block更加具体为\n",
    "\n",
    "![encoder_block detail](image/transformer_resideual_layer_norm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 请尝试独自debug各层类\n",
    "\n",
    "# 获取encode中的一个blocks\n",
    "layer=model.encoder.layers[0]\n",
    "# print(layer)\n",
    "\n",
    "# 0. 保留输入向量, 用于short cut\n",
    "emb_src = emb_out\n",
    "_emb_src = emb_src \n",
    "\n",
    "# 1. 编码层 多头-自注意力机制（后面会详细介绍）\n",
    "x = layer.attention(q=emb_src, k=emb_src, v=emb_src, mask=src_mask)\n",
    "\n",
    "# 2. dropout和layer-norm（后面会介绍）\n",
    "x = layer.dropout1(x)\n",
    "x = layer.norm1(x + _emb_src) # shorcut连接\n",
    "\n",
    "# 3. 基于位置的前向传播将维度512->2048->512\n",
    "_x = x\n",
    "x = layer.ffn(x)\n",
    "\n",
    "# 4. dropout + shortcut + layer-norm\n",
    "x = layer.dropout2(x)\n",
    "x = layer.norm2(x + _x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shortcut目的在以保留信息，防止信息损失，见ResNet\n",
    "\n",
    "![shortcut](image/transformer_resideual_layer_norm_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4 计算 embeding->[multi-head-attention]->score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "输入输出\n",
    "![multi-head](image/transformer_attention_heads_z.png)\n",
    "\n",
    "\n",
    "------\n",
    "输出拼接\n",
    "![multi-concate](image/transformer_attention_heads_weight_matrix_o.png)\n",
    "\n",
    "-----\n",
    "Multi-head-attention计算流程\n",
    "![multi-head-attention-pipeline](image/transformer_multi-headed_self-attention-recap.png)\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "![wq](image/self-attention-matrix-calculation.png)\n",
    "---\n",
    "\n",
    "\n",
    "![8头](image/transformer_attention_heads_qkv.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_head_attention层包含: MultiHeadAttention(\n",
      "  (attention): ScaleDotProductAttention(\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
      ")\n",
      "emb_src: torch.Size([128, 28, 512])\n",
      "x_attention_out: torch.Size([128, 28, 512])\n",
      "以下为多头注意力forward分解步骤：\n",
      "\n",
      " 0. 输入向量emb_src: torch.Size([128, 28, 512])\n",
      "q.shape: torch.Size([128, 28, 512])\n",
      "k.shape: torch.Size([128, 28, 512])\n",
      "v.shape: torch.Size([128, 28, 512])\n",
      "\n",
      " 1. 对qkv liner 转化\n",
      "q=f(q):  torch.Size([128, 28, 512])\n",
      "\n",
      " 2. 将输入向量拆成n_head\n",
      "n_heads: 8\n",
      "multi_head_attention.n_head: 8\n",
      "*-------multi_head_attention.split()-------------*\n",
      "d_model:512 / n_heads:8 = d_tensor:64\n",
      "单头向量维度为: 64\n",
      "_q_split: torch.Size([128, 8, 28, 64])\n",
      "*-------multi_head_attention.split()-------------*\n",
      "shape = [batch_size:128, heads:8, length:29, d_tensor:64]\n",
      "multi_head_attention.split(q): torch.Size([128, 8, 28, 64])\n",
      "multi_head_attention.split(k): torch.Size([128, 8, 28, 64])\n",
      "multi_head_attention.split(v): torch.Size([128, 8, 28, 64])\n",
      "\n",
      " 3. 计算单头注意力, scale and dot attention\n",
      "上面将512维度分成8头64维\n",
      "会独立介绍单头注意力的计算\n",
      "对每一头进行自注意力后的结果: torch.Size([128, 8, 28, 64])\n",
      "\n",
      " 4. 将8头64维拼接成512维度向量\n",
      "*-------multi_head_attention.concat()-------------*\n",
      "multi_head_attention.concat() 函数示例\n",
      "concat 操作后 torch.Size([128, 28, 512])\n",
      "*-------multi_head_attention.concat()-------------*\n",
      "after concat out shape: torch.Size([128, 28, 512])\n",
      "对多头注意力输出再进行前向传播 torch.Size([128, 28, 512])\n"
     ]
    }
   ],
   "source": [
    "# multihead多头注意力计算\n",
    "\n",
    "# encode multi-attention直接计算多头注意力分数\n",
    "multi_head_attention = model.encoder.layers[0].attention\n",
    "print(\"multi_head_attention层包含:\", multi_head_attention)\n",
    "x_attention_out = multi_head_attention(q=emb_src, k=emb_src, v=emb_src, mask=src_mask)\n",
    "print(\"emb_src:\", emb_src.shape)\n",
    "print(\"x_attention_out:\", x_attention_out.shape)\n",
    "print(\"以下为多头注意力forward分解步骤：\")\n",
    "\n",
    "# 0. 自注意力向量\n",
    "q = k = v = emb_src # embdedding+positional = x\n",
    "print(\"\\n 0. 输入向量emb_src:\", emb_src.shape)\n",
    "print(\"q.shape:\", q.shape)\n",
    "print(\"k.shape:\", k.shape)\n",
    "print(\"v.shape:\", v.shape)\n",
    "\n",
    "# 1. liner转化\n",
    "print(\"\\n 1. 对qkv liner 转化\")\n",
    "q = multi_head_attention.w_q(q)\n",
    "k = multi_head_attention.w_k(k)\n",
    "v = multi_head_attention.w_v(v)\n",
    "print(\"q=f(q): \", q.shape)\n",
    "\n",
    "# 2. 将输入向量拆成n_head\n",
    "print(\"\\n 2. 将输入向量拆成n_head\")\n",
    "print(\"n_heads:\", n_heads)\n",
    "print(\"multi_head_attention.n_head:\", multi_head_attention.n_head)\n",
    "_q = q\n",
    "\n",
    "# do split multi_head_attention.split()\n",
    "print('*-------multi_head_attention.split()-------------*')\n",
    "batch_size, length, d_model = _q.size()\n",
    "d_tensor = d_model // multi_head_attention.n_head\n",
    "print(\"d_model:{} / n_heads:{} = d_tensor:{}\".format(d_model, n_heads, d_tensor))\n",
    "print(\"单头向量维度为:\", d_tensor)\n",
    "_q_split = _q.view(batch_size, length, multi_head_attention.n_head, d_tensor).transpose(1, 2)\n",
    "print(\"_q_split:\", _q_split.shape)\n",
    "print('*-------multi_head_attention.split()-------------*')\n",
    "\n",
    "q, k, v = multi_head_attention.split(q), multi_head_attention.split(k), multi_head_attention.split(v)\n",
    "print(\"shape = [batch_size:128, heads:8, length:29, d_tensor:64]\")\n",
    "print(\"multi_head_attention.split(q):\", q.shape)\n",
    "print(\"multi_head_attention.split(k):\", k.shape)\n",
    "print(\"multi_head_attention.split(v):\", v.shape)\n",
    "\n",
    "\n",
    "# 3. do scale dot product to compute similarity\n",
    "# 计算每一头的attention（scale and dot attention）\n",
    "print(\"\\n 3. 计算单头注意力, scale and dot attention\")\n",
    "print(\"上面将512维度分成8头64维\")\n",
    "print(\"会独立介绍单头注意力的计算\")\n",
    "_q_single = q\n",
    "_k_single = k\n",
    "_v_single = v\n",
    "out, attention = multi_head_attention.attention(q, k, v, mask=src_mask)\n",
    "print(\"对每一头进行自注意力后的结果:\", out.shape)\n",
    "\n",
    "# 4. concat and pass to linear layer\n",
    "print(\"\\n 4. 将8头64维拼接成512维度向量\")\n",
    "_out = out \n",
    "# do concat \n",
    "\n",
    "print('*-------multi_head_attention.concat()-------------*')\n",
    "print(\"multi_head_attention.concat() 函数示例\")\n",
    "batch_size, head, length, d_tensor = _out.size()\n",
    "d_model = head * d_tensor\n",
    "_out_concat = _out.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n",
    "print(\"concat 操作后\", _out_concat.shape)\n",
    "print('*-------multi_head_attention.concat()-------------*')\n",
    "\n",
    "out = multi_head_attention.concat(out)\n",
    "print(\"after concat out shape:\", out.shape)\n",
    "out = multi_head_attention.w_concat(out)\n",
    "print(\"对多头注意力输出再进行前向传播\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.5 计算 [scale-dot-production] :  mask(q@k^t/scaled)@v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pipeline](image/self-attention-matrix-calculation-2.png)\n",
    "![pipeline_qkv2](image/self-attention-output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaleDotProductAttention(\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "tensor中的格式： 只关注length句长， d_tensor向量长度\n",
      "[batch_size:128, head:8, length:28, d_tensor:64]\n",
      "q: torch.Size([128, 8, 28, 64])\n",
      "k_t: torch.Size([128, 8, 64, 28])\n",
      "通过计算两个向量的点积dot操作:score=q@k_t:  torch.Size([128, 8, 28, 28])\n",
      "每个词与词之间计算相关性\n",
      "score代表注意力分数 \n",
      "src_mask: torch.Size([128, 1, 28, 28])\n",
      "v: torch.Size([128, 8, 28, 64])\n",
      "score * v: torch.Size([128, 8, 28, 64])\n",
      "score * v: 代表注意力特征向量，即每个词在当前这个句子中的特征表达\n"
     ]
    }
   ],
   "source": [
    "# attention, 单头注意力计算\n",
    "# models/layer/scale_dot_product_attention.py\n",
    "# class ScaleDotProductAttention(nn.Module)\n",
    "\n",
    "attention = multi_head_attention.attention\n",
    "print(attention)\n",
    "\n",
    "# input is 4 dimension tensor\n",
    "# [batch_size, head, length, d_tensor]\n",
    "k = _k_single\n",
    "q = _q_single\n",
    "v = _v_single\n",
    "batch_size, head, length, d_tensor = k.size()\n",
    "\n",
    "print('tensor中的格式： 只关注length句长， d_tensor向量长度')\n",
    "print('[batch_size:{}, head:{}, length:{}, d_tensor:{}]'.format(batch_size,head,length,d_tensor))\n",
    "\n",
    "# 1. dot product Query with Key^T to compute similarity\n",
    "k_t = k.transpose(2, 3)  # transpose\n",
    "\n",
    "print(\"q:\", q.shape)\n",
    "print(\"k_t:\", k_t.shape)\n",
    "score = (q @ k_t) / math.sqrt(d_tensor)  # scaled dot product\n",
    "########   dot   ####### scaled #######\n",
    "print(\"通过计算两个向量的点积dot操作:score=q@k_t: \", score.shape)\n",
    "print(\"每个词与词之间计算相关性\")\n",
    "print(\"score代表注意力分数 \")\n",
    "print(\"src_mask:\", src_mask.shape)\n",
    "# 2. apply masking (opt)\n",
    "if src_mask is not None:\n",
    "    score = score.masked_fill(src_mask == 0, -10000)\n",
    "# 3. pass them softmax to make [0, 1] range\n",
    "score = attention.softmax(score)\n",
    "# 4. multiply with Value\n",
    "print(\"v:\", v.shape)\n",
    "v = score @ v\n",
    "print(\"score * v:\", v.shape)\n",
    "print(\"score * v: 代表注意力特征向量，即每个词在当前这个句子中的特征表达\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "一个句子中：关于'it'单词的 单头自注意力score 30个词 [1,30,1] 'it'\n",
    "![vis-1](image/transformer_self-attention_visualization.png)\n",
    "\n",
    "一个句子中：关于'it'单词的 两头自注意力score 30个词 [2, 30,1] 'it'\n",
    "![vis-2](image/transformer_self-attention_visualization_2.png)\n",
    "\n",
    "\n",
    "一个句子中：关于'it'单词的 八头自注意力score 30个词 [8, 30,1] 'it'\n",
    "![vis-3](image/transformer_self-attention_visualization_3.png)\n",
    "\n",
    "\n",
    "一个句子中：关于30个单词的 八头自注意力score 30个词 [8, 30,30] \n",
    "\n",
    "128个句子中：关于30个单词的 八头自注意力score 30个词 [128, 8, 30,30] \n",
    "\n",
    "\n",
    "\n",
    "QK可视化 score\n",
    "![vis-gpt](image/gpt2-self-attention-scoring-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.6 计算 emb_src->[layer normaliztion] ->multihead attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer norm 公式\n",
    "class LayerNorm(nn.Module)\n",
    "![layer](image/layer_norm.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm()\n",
      "==============LayerNorm===========\n",
      "LayerNorm gamma:  torch.Size([512])\n",
      "LayerNorm beta:  torch.Size([512])\n",
      "LayerNorm eps:  1e-12\n",
      "LayerNorm mean:  torch.Size([128, 28, 1])\n",
      "LayerNorm var:  torch.Size([128, 28, 1])\n",
      "LayerNorm norm out:  torch.Size([128, 28, 512])\n",
      "LayerNorm norm out offset:  torch.Size([128, 28, 512])\n"
     ]
    }
   ],
   "source": [
    "# Layer Normalization, 层归一化\n",
    "# models/layer/layer_norm.py\n",
    "# class LayerNorm(nn.Module)\n",
    "\n",
    "norm = model.encoder.layers[0].norm1\n",
    "print(norm)\n",
    "\n",
    "x = emb_src\n",
    "print(\"==============LayerNorm===========\")\n",
    "print(\"LayerNorm gamma: \", norm.gamma.shape)\n",
    "print(\"LayerNorm beta: \", norm.beta.shape)\n",
    "print(\"LayerNorm eps: \", norm.eps)\n",
    "\n",
    "mean = x.mean(-1, keepdim=True)\n",
    "print(\"LayerNorm mean: \", mean.shape)\n",
    "\n",
    "var = x.var(-1, unbiased=False, keepdim=True)\n",
    "print(\"LayerNorm var: \", var.shape)\n",
    "# '-1' means last dimension. \n",
    "\n",
    "out = (x - mean) / torch.sqrt(var + norm.eps)\n",
    "print(\"LayerNorm norm out: \", out.shape)\n",
    "\n",
    "out = norm.gamma * out + norm.beta\n",
    "print(\"LayerNorm norm out offset: \", out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.7 计算attention-> [position-wise-feed-forward]->layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PositionwiseFeedForward, 位置前向传播\n",
      "PositionwiseFeedForward(\n",
      "  (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "n_hidden:  2048\n",
      "1. before linear: torch.Size([128, 28, 512])\n",
      "2. after linear1: torch.Size([128, 28, 2048])\n",
      "3. after linear2: torch.Size([128, 28, 512])\n"
     ]
    }
   ],
   "source": [
    "# PositionwiseFeedForward, 位置前向传播\n",
    "# models/layer/position_wise_feed_forward.py\n",
    "# class PositionwiseFeedForward(nn.Module)\n",
    "print(\"PositionwiseFeedForward, 位置前向传播\")\n",
    "ffn = model.encoder.layers[0].ffn\n",
    "print(ffn)\n",
    "print(\"n_hidden: \", ffn_hidden)\n",
    "\n",
    "_x = emb_src\n",
    "print(\"1. before linear:\", _x.shape)\n",
    "\n",
    "_x = ffn.linear1(_x)\n",
    "print(\"2. after linear1:\", _x.shape)\n",
    "\n",
    "_x = ffn.relu(_x)\n",
    "_x = ffn.dropout(_x)\n",
    "_x = ffn.linear2(_x)\n",
    "print(\"3. after linear2:\", _x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.8 计算enc_src+emb_trg->[decoder]->output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![decoder_dataflow_block](image/The_transformer_encoder_decoder_stack.png)\n",
    "![decoder_dataflow](image/transformer_resideual_layer_norm_3.png)\n",
    "![decoder_pipeline_single](image/transformer_decoding_2.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "解码层结构：\n",
      "解码层输入target和编码层一样做embeding\n",
      "trg输入 torch.Size([128, 26])\n",
      "trg embding torch.Size([128, 26, 512])\n",
      "解码层数: 6\n",
      "编码层输出： torch.Size([128, 26, 512])\n",
      "编码层liner处理： torch.Size([128, 26, 7853])\n"
     ]
    }
   ],
   "source": [
    "# Decoder, 解码结构\n",
    "# models/model/decoder.py\n",
    "# class Decoder(nn.Module)\n",
    "print(\"解码层结构：\")\n",
    "# print(model.decoder)\n",
    "\n",
    "print(\"解码层输入target和编码层一样做embeding\")\n",
    "print(\"trg输入\", trg.shape)\n",
    "emb_trg = model.decoder.emb(trg) # target -> Label mask\n",
    "print(\"trg embding\", emb_trg.shape)\n",
    "print(\"解码层数:\", len(model.decoder.layers))\n",
    "\n",
    "# encoder - > encoder K encoder V\n",
    "# decoder Q\n",
    "\n",
    "for layer in model.decoder.layers:\n",
    "    # 注意这里需要有编码层的输入\n",
    "    decode_trg = layer(emb_trg, enc_src, trg_mask, src_trg_mask)\n",
    "print(\"编码层输出：\", decode_trg.shape)\n",
    "# pass to LM head\n",
    "output_decode = model.decoder.linear(decode_trg)\n",
    "print(\"编码层liner处理：\", output_decode.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.9 计算[decoder block]: decode-self-attention -> enc-dec-attention ->ffn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![encoder-decoder](image/transformer_resideual_layer_norm_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: trg_x: torch.Size([128, 26, 512])\n",
      "k: enc: torch.Size([128, 28, 512])\n",
      "v: enc: torch.Size([128, 28, 512])\n",
      "mask: src_trg_mask: torch.Size([128, 1, 26, 28])\n",
      "enc->dec 注意力后:  torch.Size([128, 26, 512])\n"
     ]
    }
   ],
   "source": [
    "# DecoderLayer, 解码层\n",
    "# models/blocks/decoder_layer.py\n",
    "# class DecoderLayer(nn.Module)\n",
    "\n",
    "# decode layer\n",
    "layer = model.decoder.layers[0]\n",
    "# print(\"decode layer结构：\")\n",
    "# print(layer)\n",
    "\n",
    "dec = emb_trg\n",
    "enc = enc_src\n",
    "_x = dec\n",
    "\n",
    "x = layer.self_attention(q=dec, k=dec, v=dec, mask=trg_mask)\n",
    "x = layer.dropout1(x)\n",
    "x = layer.norm1(x + _x)\n",
    "\n",
    "if enc is not None:\n",
    "    # 3. compute encoder - decoder attention\n",
    "    _x = x\n",
    "    # 多头注意力机制\n",
    "    print('q: trg_x:', x.shape)\n",
    "    print('k: enc:', enc.shape)\n",
    "    print('v: enc:', enc.shape)\n",
    "    print('mask: src_trg_mask:', src_trg_mask.shape)\n",
    "    x = layer.enc_dec_attention(q=x, k=enc, v=enc, mask=src_trg_mask)\n",
    "    print(\"enc->dec 注意力后: \", x.shape)\n",
    "    # 4. add and norm\n",
    "    x = layer.dropout2(x)\n",
    "    x = layer.norm2(x + _x)\n",
    "\n",
    "# 5. positionwise feed forward network\n",
    "_x = x\n",
    "x = layer.ffn(x)\n",
    "\n",
    "# 6. add and norm\n",
    "x = layer.dropout3(x)\n",
    "x = layer.norm3(x + _x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.10 计算loss : output->[Cross Entropy loss]->logits->loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![loss](image/transformer_decoder_output_softmax.png)\n",
    "![loss_vocab](image/output_trained_model_probability_distributions.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失计算，使用交叉损失：\n",
      "src: torch.Size([128, 28])\n",
      "trg: torch.Size([128, 26])\n",
      "trg[:, :-1]: torch.Size([128, 25])\n",
      "output: torch.Size([128, 25, 7853])\n",
      "output_reshape: torch.Size([3200, 7853])\n",
      "trg.view(-1): torch.Size([3200])\n",
      "loss: tensor(10.0478, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## 损失计算\n",
    "print('损失计算，使用交叉损失：')\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_pad_idx)\n",
    "print('src:', src.shape)\n",
    "print('trg:', trg.shape)\n",
    "print('trg[:, :-1]:', trg[:, :-1].shape)\n",
    "output = model(src, trg[:, :-1])\n",
    "print('output:', output.shape)\n",
    "output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "print('output_reshape:', output_reshape.shape)\n",
    "\n",
    "trg_view = trg[:, 1:].contiguous().view(-1)\n",
    "print('trg.view(-1):', trg_view.shape)\n",
    "loss = criterion(output_reshape, trg_view)\n",
    "print('loss:', loss)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.11 编解码Mask计算原理enc-dec-mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_mask: torch.Size([128, 1, 28, 28])\n",
      "src_trg_mask: torch.Size([128, 1, 26, 28])\n",
      "trg_mask: torch.Size([128, 1, 26, 26])\n",
      "tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1]], dtype=torch.int32)\n",
      "tensor([[1, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1]], dtype=torch.int32)\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "## mask机制\n",
    "print(\"src_mask:\", src_mask.shape)\n",
    "print(\"src_trg_mask:\", src_trg_mask.shape)\n",
    "print(\"trg_mask:\", trg_mask.shape)\n",
    "# print(src_mask[0][0].int())\n",
    "# print(src_trg_mask[0][0].int())\n",
    "print(src_mask[0,0,:5,:5].int())\n",
    "print(trg_mask[0,0,:5,:5].int())\n",
    "print(src_trg_mask[0,0,:20,:20].int())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: torch.Size([128, 8, 26, 64])\n",
      "k_t: torch.Size([128, 8, 64, 28])\n",
      "score: torch.Size([128, 8, 26, 28])\n",
      "enc-dec-mask: torch.Size([128, 1, 26, 28])\n",
      "<class 'torch.Tensor'>\n",
      "v: torch.Size([128, 8, 28, 64])\n",
      "score * v: torch.Size([128, 8, 26, 64])\n"
     ]
    }
   ],
   "source": [
    "# encode-decode-mask\n",
    "layer = model.decoder.layers[0]\n",
    "# print(\"decode layer结构：\")\n",
    "# print(layer)\n",
    "\n",
    "dec = emb_trg\n",
    "enc = enc_src\n",
    "\n",
    "# _x = dec\n",
    "# 1. decode self attention for target \n",
    "# x = layer.self_attention(q=dec, k=dec, v=dec, mask=trg_mask)\n",
    "# x = layer.dropout1(x)\n",
    "# x = layer.norm1(x + _x)\n",
    "\n",
    "# 2. ecode-decode-attention + mask\n",
    "# if enc is not None:\n",
    "#     # 3. compute encoder - decoder attention\n",
    "#     _x = x\n",
    "#     # 多头注意力机制\n",
    "#     print('q: trg_x:', x.shape)\n",
    "#     print('k: enc:', enc.shape)\n",
    "#     print('v: enc:', enc.shape)\n",
    "#     print('mask: src_trg_mask:', src_trg_mask.shape)\n",
    "#     x = layer.enc_dec_attention(q=x, k=enc, v=enc, mask=src_trg_mask)\n",
    "\n",
    "# layer.enc_dec_attention 多头\n",
    "# layer.enc_dec_attention.attention() 单头\n",
    "\n",
    "\n",
    "q_dec = dec\n",
    "q = q_dec = layer.enc_dec_attention.split(dec)\n",
    "k = k_enc = _k_single\n",
    "v = v_enc = _v_single\n",
    "\n",
    "batch_size, head, length, d_tensor = k.size()\n",
    "\n",
    "# 1. dot product Query with Key^T to compute similarity\n",
    "k_t = k.transpose(2, 3)  # transpose\n",
    "\n",
    "\n",
    "print(\"q:\", q.shape)\n",
    "print(\"k_t:\", k_t.shape)\n",
    "score = (q @ k_t) / math.sqrt(d_tensor)  # scaled dot product\n",
    "print(\"score:\", score.shape)\n",
    "# 2. apply masking (opt)\n",
    "if src_trg_mask is not None: # 实际预测时，没有mask，会预测出终止标志符号\n",
    "    print(\"enc-dec-mask:\",src_trg_mask.shape)\n",
    "    score = score.masked_fill(src_trg_mask == 0, -10000)\n",
    "# 3. pass them softmax to make [0, 1] range\n",
    "score = attention.softmax(score)\n",
    "print(type(score))\n",
    "# 4. multiply with Value\n",
    "print(\"v:\", v.shape)\n",
    "v = score @ v\n",
    "print(\"score * v:\", v.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(params=model.parameters(),\n",
    "                 lr=init_lr,\n",
    "                 weight_decay=weight_decay,\n",
    "                 eps=adam_eps)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 verbose=True,\n",
    "                                                 factor=factor,\n",
    "                                                 patience=patience)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_pad_idx)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src \n",
    "        trg = batch.trg \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg[:, :-1])\n",
    "        '''\n",
    "        trg[:, :-1] 表示选取目标序列的每一个序列（除了最后一个元素），这是因为模型在训练过程中，对于每个时间步t，\n",
    "        它都会基于到目前为止已解码的序列（即t-1时刻的预测结果和源序列信息）预测出t时刻的词。\n",
    "        所以在计算损失时，我们只使用到目标序列的每个词之前的词作为监督信息。'''\n",
    "        output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output_reshape, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        print('step :', round((i / len(iterator)) * 100, 2), '% , loss :', loss.item())\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0.0 % , loss : 9.998688697814941\n",
      "step : 0.44 % , loss : 9.757180213928223\n",
      "step : 0.88 % , loss : 9.553743362426758\n",
      "step : 1.32 % , loss : 9.339875221252441\n",
      "step : 1.76 % , loss : 9.092477798461914\n",
      "step : 2.2 % , loss : 9.060367584228516\n",
      "step : 2.64 % , loss : 8.910896301269531\n",
      "step : 3.08 % , loss : 8.706787109375\n",
      "step : 3.52 % , loss : 8.574522018432617\n",
      "step : 3.96 % , loss : 8.55985164642334\n",
      "step : 4.41 % , loss : 8.345579147338867\n",
      "step : 4.85 % , loss : 8.336215019226074\n",
      "step : 5.29 % , loss : 8.28457260131836\n",
      "step : 5.73 % , loss : 8.278319358825684\n",
      "step : 6.17 % , loss : 8.091436386108398\n",
      "step : 6.61 % , loss : 8.020792007446289\n",
      "step : 7.05 % , loss : 8.027411460876465\n",
      "step : 7.49 % , loss : 7.986093997955322\n",
      "step : 7.93 % , loss : 8.006268501281738\n",
      "step : 8.37 % , loss : 7.9191508293151855\n",
      "step : 8.81 % , loss : 7.952389240264893\n",
      "step : 9.25 % , loss : 7.819626331329346\n",
      "step : 9.69 % , loss : 7.739791393280029\n",
      "step : 10.13 % , loss : 7.700322151184082\n",
      "step : 10.57 % , loss : 7.703452110290527\n",
      "step : 11.01 % , loss : 7.63346529006958\n",
      "step : 11.45 % , loss : 7.502734661102295\n",
      "step : 11.89 % , loss : 7.645949363708496\n",
      "step : 12.33 % , loss : 7.530333995819092\n",
      "step : 12.78 % , loss : 7.524205207824707\n",
      "step : 13.22 % , loss : 7.460716247558594\n",
      "step : 13.66 % , loss : 7.47508430480957\n",
      "step : 14.1 % , loss : 7.487890720367432\n",
      "step : 14.54 % , loss : 7.489870548248291\n",
      "step : 14.98 % , loss : 7.37359619140625\n",
      "step : 15.42 % , loss : 7.322293758392334\n",
      "step : 15.86 % , loss : 7.325257301330566\n",
      "step : 16.3 % , loss : 7.396024227142334\n",
      "step : 16.74 % , loss : 7.2187652587890625\n",
      "step : 17.18 % , loss : 7.399685382843018\n",
      "step : 17.62 % , loss : 7.33582067489624\n",
      "step : 18.06 % , loss : 7.273746967315674\n",
      "step : 18.5 % , loss : 7.156857490539551\n",
      "step : 18.94 % , loss : 7.1813836097717285\n",
      "step : 19.38 % , loss : 7.258833408355713\n",
      "step : 19.82 % , loss : 7.264618396759033\n",
      "step : 20.26 % , loss : 7.143808364868164\n",
      "step : 20.7 % , loss : 7.10053014755249\n",
      "step : 21.15 % , loss : 7.131144046783447\n",
      "step : 21.59 % , loss : 7.0770745277404785\n",
      "step : 22.03 % , loss : 7.085639953613281\n",
      "step : 22.47 % , loss : 7.041990756988525\n",
      "step : 22.91 % , loss : 7.129288196563721\n",
      "step : 23.35 % , loss : 7.047952175140381\n",
      "step : 23.79 % , loss : 7.113166332244873\n",
      "step : 24.23 % , loss : 7.11832332611084\n",
      "step : 24.67 % , loss : 7.046982765197754\n",
      "step : 25.11 % , loss : 7.06007194519043\n",
      "step : 25.55 % , loss : 7.129330158233643\n",
      "step : 25.99 % , loss : 7.074661731719971\n",
      "step : 26.43 % , loss : 6.99032735824585\n",
      "step : 26.87 % , loss : 7.019162178039551\n",
      "step : 27.31 % , loss : 7.024319171905518\n",
      "step : 27.75 % , loss : 6.998516082763672\n",
      "step : 28.19 % , loss : 7.05229377746582\n",
      "step : 28.63 % , loss : 6.979384422302246\n",
      "step : 29.07 % , loss : 7.078419208526611\n",
      "step : 29.52 % , loss : 6.939293384552002\n",
      "step : 29.96 % , loss : 6.933383941650391\n",
      "step : 30.4 % , loss : 6.939211368560791\n",
      "step : 30.84 % , loss : 6.935976505279541\n",
      "step : 31.28 % , loss : 6.850990295410156\n",
      "step : 31.72 % , loss : 6.88207483291626\n",
      "step : 32.16 % , loss : 6.846299171447754\n",
      "step : 32.6 % , loss : 6.875698089599609\n",
      "step : 33.04 % , loss : 6.817417144775391\n",
      "step : 33.48 % , loss : 6.904076099395752\n",
      "step : 33.92 % , loss : 6.848805904388428\n",
      "step : 34.36 % , loss : 6.899985313415527\n",
      "step : 34.8 % , loss : 6.737658977508545\n",
      "step : 35.24 % , loss : 6.964312553405762\n",
      "step : 35.68 % , loss : 6.879166126251221\n",
      "step : 36.12 % , loss : 6.801446914672852\n",
      "step : 36.56 % , loss : 6.845944881439209\n",
      "step : 37.0 % , loss : 6.835647106170654\n",
      "step : 37.44 % , loss : 6.886661529541016\n",
      "step : 37.89 % , loss : 6.82444953918457\n",
      "step : 38.33 % , loss : 6.898538589477539\n",
      "step : 38.77 % , loss : 6.873547554016113\n",
      "step : 39.21 % , loss : 6.8221116065979\n",
      "step : 39.65 % , loss : 6.782952308654785\n",
      "step : 40.09 % , loss : 6.841006278991699\n",
      "step : 40.53 % , loss : 6.759559631347656\n",
      "step : 40.97 % , loss : 6.764366626739502\n",
      "step : 41.41 % , loss : 6.82237434387207\n",
      "step : 41.85 % , loss : 6.728903293609619\n",
      "step : 42.29 % , loss : 6.796202659606934\n",
      "step : 42.73 % , loss : 6.72404146194458\n",
      "step : 43.17 % , loss : 6.716434955596924\n",
      "step : 43.61 % , loss : 6.664124011993408\n",
      "step : 44.05 % , loss : 6.717203617095947\n",
      "step : 44.49 % , loss : 6.704171180725098\n",
      "step : 44.93 % , loss : 6.651621341705322\n",
      "step : 45.37 % , loss : 6.7501349449157715\n",
      "step : 45.81 % , loss : 6.719880104064941\n",
      "step : 46.26 % , loss : 6.746646881103516\n",
      "step : 46.7 % , loss : 6.700796604156494\n",
      "step : 47.14 % , loss : 6.575867176055908\n",
      "step : 47.58 % , loss : 6.729630947113037\n",
      "step : 48.02 % , loss : 6.718740940093994\n",
      "step : 48.46 % , loss : 6.65916633605957\n",
      "step : 48.9 % , loss : 6.67338228225708\n",
      "step : 49.34 % , loss : 6.643345832824707\n",
      "step : 49.78 % , loss : 6.6193671226501465\n",
      "step : 50.22 % , loss : 6.678275108337402\n",
      "step : 50.66 % , loss : 6.658833026885986\n",
      "step : 51.1 % , loss : 6.729587554931641\n",
      "step : 51.54 % , loss : 6.5758957862854\n",
      "step : 51.98 % , loss : 6.594191551208496\n",
      "step : 52.42 % , loss : 6.638609886169434\n",
      "step : 52.86 % , loss : 6.56488561630249\n",
      "step : 53.3 % , loss : 6.683400630950928\n",
      "step : 53.74 % , loss : 6.5406694412231445\n",
      "step : 54.19 % , loss : 6.55685567855835\n",
      "step : 54.63 % , loss : 6.687986850738525\n",
      "step : 55.07 % , loss : 6.561254978179932\n",
      "step : 55.51 % , loss : 6.61774206161499\n",
      "step : 55.95 % , loss : 6.5228962898254395\n",
      "step : 56.39 % , loss : 6.569606781005859\n",
      "step : 56.83 % , loss : 6.634975910186768\n",
      "step : 57.27 % , loss : 6.564518928527832\n",
      "step : 57.71 % , loss : 6.556535720825195\n",
      "step : 58.15 % , loss : 6.566861152648926\n",
      "step : 58.59 % , loss : 6.487499237060547\n",
      "step : 59.03 % , loss : 6.613926887512207\n",
      "step : 59.47 % , loss : 6.486558437347412\n",
      "step : 59.91 % , loss : 6.510456085205078\n",
      "step : 60.35 % , loss : 6.528234481811523\n",
      "step : 60.79 % , loss : 6.554044246673584\n",
      "step : 61.23 % , loss : 6.590901851654053\n",
      "step : 61.67 % , loss : 6.586488246917725\n",
      "step : 62.11 % , loss : 6.526578903198242\n",
      "step : 62.56 % , loss : 6.509584426879883\n",
      "step : 63.0 % , loss : 6.471611976623535\n",
      "step : 63.44 % , loss : 6.482824325561523\n",
      "step : 63.88 % , loss : 6.431921482086182\n",
      "step : 64.32 % , loss : 6.39497709274292\n",
      "step : 64.76 % , loss : 6.440361022949219\n",
      "step : 65.2 % , loss : 6.480970859527588\n",
      "step : 65.64 % , loss : 6.4956374168396\n",
      "step : 66.08 % , loss : 6.452697277069092\n",
      "step : 66.52 % , loss : 6.5749053955078125\n",
      "step : 66.96 % , loss : 6.468926906585693\n",
      "step : 67.4 % , loss : 6.545961856842041\n",
      "step : 67.84 % , loss : 6.544915676116943\n",
      "step : 68.28 % , loss : 6.498673439025879\n",
      "step : 68.72 % , loss : 6.533612251281738\n",
      "step : 69.16 % , loss : 6.496365547180176\n",
      "step : 69.6 % , loss : 6.409890651702881\n",
      "step : 70.04 % , loss : 6.528832912445068\n",
      "step : 70.48 % , loss : 6.473941326141357\n",
      "step : 70.93 % , loss : 6.448774337768555\n",
      "step : 71.37 % , loss : 6.453918933868408\n",
      "step : 71.81 % , loss : 6.57008695602417\n",
      "step : 72.25 % , loss : 6.339817047119141\n",
      "step : 72.69 % , loss : 6.4785261154174805\n",
      "step : 73.13 % , loss : 6.398798942565918\n",
      "step : 73.57 % , loss : 6.483205318450928\n",
      "step : 74.01 % , loss : 6.430422782897949\n",
      "step : 74.45 % , loss : 6.401966094970703\n",
      "step : 74.89 % , loss : 6.453545570373535\n",
      "step : 75.33 % , loss : 6.5575737953186035\n",
      "step : 75.77 % , loss : 6.367879390716553\n",
      "step : 76.21 % , loss : 6.344699382781982\n",
      "step : 76.65 % , loss : 6.354394435882568\n",
      "step : 77.09 % , loss : 6.358504772186279\n",
      "step : 77.53 % , loss : 6.387339115142822\n",
      "step : 77.97 % , loss : 6.43045711517334\n",
      "step : 78.41 % , loss : 6.391985893249512\n",
      "step : 78.85 % , loss : 6.455271244049072\n",
      "step : 79.3 % , loss : 6.35917854309082\n",
      "step : 79.74 % , loss : 6.409173488616943\n",
      "step : 80.18 % , loss : 6.384275436401367\n",
      "step : 80.62 % , loss : 6.429436206817627\n",
      "step : 81.06 % , loss : 6.400548458099365\n",
      "step : 81.5 % , loss : 6.4517388343811035\n",
      "step : 81.94 % , loss : 6.360436916351318\n",
      "step : 82.38 % , loss : 6.402230739593506\n",
      "step : 82.82 % , loss : 6.353440284729004\n",
      "step : 83.26 % , loss : 6.3212361335754395\n",
      "step : 83.7 % , loss : 6.387618064880371\n",
      "step : 84.14 % , loss : 6.386776924133301\n",
      "step : 84.58 % , loss : 6.339077472686768\n",
      "step : 85.02 % , loss : 6.34194278717041\n",
      "step : 85.46 % , loss : 6.3689470291137695\n",
      "step : 85.9 % , loss : 6.365699291229248\n",
      "step : 86.34 % , loss : 6.372188568115234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 86.78 % , loss : 6.408270359039307\n",
      "step : 87.22 % , loss : 6.2366437911987305\n",
      "step : 87.67 % , loss : 6.334761619567871\n",
      "step : 88.11 % , loss : 6.378795146942139\n",
      "step : 88.55 % , loss : 6.382861614227295\n",
      "step : 88.99 % , loss : 6.244999885559082\n",
      "step : 89.43 % , loss : 6.283051013946533\n",
      "step : 89.87 % , loss : 6.352443695068359\n",
      "step : 90.31 % , loss : 6.326788425445557\n",
      "step : 90.75 % , loss : 6.271821022033691\n",
      "step : 91.19 % , loss : 6.289643287658691\n",
      "step : 91.63 % , loss : 6.264233112335205\n",
      "step : 92.07 % , loss : 6.371068954467773\n",
      "step : 92.51 % , loss : 6.217995643615723\n",
      "step : 92.95 % , loss : 6.325382709503174\n",
      "step : 93.39 % , loss : 6.360666275024414\n",
      "step : 93.83 % , loss : 6.298946380615234\n",
      "step : 94.27 % , loss : 6.315149784088135\n",
      "step : 94.71 % , loss : 6.247846603393555\n",
      "step : 95.15 % , loss : 6.26665735244751\n",
      "step : 95.59 % , loss : 6.277987957000732\n",
      "step : 96.04 % , loss : 6.295995712280273\n",
      "step : 96.48 % , loss : 6.2831549644470215\n",
      "step : 96.92 % , loss : 6.272954940795898\n",
      "step : 97.36 % , loss : 6.268092155456543\n",
      "step : 97.8 % , loss : 6.219967365264893\n",
      "step : 98.24 % , loss : 6.228821277618408\n",
      "step : 98.68 % , loss : 6.233220100402832\n",
      "step : 99.12 % , loss : 6.237015247344971\n",
      "step : 99.56 % , loss : 6.256226062774658\n",
      "\tTrain Loss: 6.879\n",
      "step : 0.0 % , loss : 6.213214874267578\n",
      "step : 0.44 % , loss : 6.235585689544678\n",
      "step : 0.88 % , loss : 6.34515905380249\n",
      "step : 1.32 % , loss : 6.356490612030029\n",
      "step : 1.76 % , loss : 6.267234802246094\n",
      "step : 2.2 % , loss : 6.202134609222412\n",
      "step : 2.64 % , loss : 6.187347412109375\n",
      "step : 3.08 % , loss : 6.245565891265869\n",
      "step : 3.52 % , loss : 6.254941940307617\n",
      "step : 3.96 % , loss : 6.213858604431152\n",
      "step : 4.41 % , loss : 6.229341506958008\n",
      "step : 4.85 % , loss : 6.320181846618652\n",
      "step : 5.29 % , loss : 6.177175521850586\n",
      "step : 5.73 % , loss : 6.330494403839111\n",
      "step : 6.17 % , loss : 6.208110332489014\n",
      "step : 6.61 % , loss : 6.271526336669922\n",
      "step : 7.05 % , loss : 6.20203971862793\n",
      "step : 7.49 % , loss : 6.1590752601623535\n",
      "step : 7.93 % , loss : 6.229216575622559\n",
      "step : 8.37 % , loss : 6.16806173324585\n",
      "step : 8.81 % , loss : 6.223469257354736\n",
      "step : 9.25 % , loss : 6.181513786315918\n",
      "step : 9.69 % , loss : 6.165190696716309\n",
      "step : 10.13 % , loss : 6.170816898345947\n",
      "step : 10.57 % , loss : 6.162298679351807\n",
      "step : 11.01 % , loss : 6.144359588623047\n",
      "step : 11.45 % , loss : 6.156900882720947\n",
      "step : 11.89 % , loss : 6.194739818572998\n",
      "step : 12.33 % , loss : 6.135351657867432\n",
      "step : 12.78 % , loss : 6.06740665435791\n",
      "step : 13.22 % , loss : 6.1539130210876465\n",
      "step : 13.66 % , loss : 6.131808280944824\n",
      "step : 14.1 % , loss : 6.184649467468262\n",
      "step : 14.54 % , loss : 6.1121931076049805\n",
      "step : 14.98 % , loss : 6.105320453643799\n",
      "step : 15.42 % , loss : 6.2718305587768555\n",
      "step : 15.86 % , loss : 6.110170364379883\n",
      "step : 16.3 % , loss : 6.206287860870361\n",
      "step : 16.74 % , loss : 6.207505702972412\n",
      "step : 17.18 % , loss : 6.128014087677002\n",
      "step : 17.62 % , loss : 6.132104396820068\n",
      "step : 18.06 % , loss : 6.206187725067139\n",
      "step : 18.5 % , loss : 6.1189703941345215\n",
      "step : 18.94 % , loss : 6.126975059509277\n",
      "step : 19.38 % , loss : 6.053742408752441\n",
      "step : 19.82 % , loss : 6.105935573577881\n",
      "step : 20.26 % , loss : 6.046072959899902\n",
      "step : 20.7 % , loss : 6.0716962814331055\n",
      "step : 21.15 % , loss : 6.061950206756592\n",
      "step : 21.59 % , loss : 6.155876636505127\n",
      "step : 22.03 % , loss : 6.20966100692749\n",
      "step : 22.47 % , loss : 6.125322341918945\n",
      "step : 22.91 % , loss : 6.211553573608398\n",
      "step : 23.35 % , loss : 6.158936977386475\n",
      "step : 23.79 % , loss : 6.117674350738525\n",
      "step : 24.23 % , loss : 6.049041271209717\n",
      "step : 24.67 % , loss : 6.188667297363281\n",
      "step : 25.11 % , loss : 6.095845699310303\n",
      "step : 25.55 % , loss : 6.079883098602295\n",
      "step : 25.99 % , loss : 6.117400646209717\n",
      "step : 26.43 % , loss : 6.090465545654297\n",
      "step : 26.87 % , loss : 6.089478969573975\n",
      "step : 27.31 % , loss : 6.125405788421631\n",
      "step : 27.75 % , loss : 6.144066333770752\n",
      "step : 28.19 % , loss : 6.107265949249268\n",
      "step : 28.63 % , loss : 6.056821823120117\n",
      "step : 29.07 % , loss : 6.179427623748779\n",
      "step : 29.52 % , loss : 6.163684844970703\n",
      "step : 29.96 % , loss : 6.135319232940674\n",
      "step : 30.4 % , loss : 6.11986780166626\n",
      "step : 30.84 % , loss : 6.00020170211792\n",
      "step : 31.28 % , loss : 6.126756191253662\n",
      "step : 31.72 % , loss : 6.097323417663574\n",
      "step : 32.16 % , loss : 6.095292091369629\n",
      "step : 32.6 % , loss : 6.085397720336914\n",
      "step : 33.04 % , loss : 6.095069885253906\n",
      "step : 33.48 % , loss : 5.958820343017578\n",
      "step : 33.92 % , loss : 6.003125190734863\n",
      "step : 34.36 % , loss : 6.073864936828613\n",
      "step : 34.8 % , loss : 6.102385997772217\n",
      "step : 35.24 % , loss : 6.0283637046813965\n",
      "step : 35.68 % , loss : 6.077075004577637\n",
      "step : 36.12 % , loss : 5.933225631713867\n",
      "step : 36.56 % , loss : 5.949158668518066\n",
      "step : 37.0 % , loss : 6.088870525360107\n",
      "step : 37.44 % , loss : 6.112721920013428\n",
      "step : 37.89 % , loss : 5.93593168258667\n",
      "step : 38.33 % , loss : 6.084488868713379\n",
      "step : 38.77 % , loss : 5.986796855926514\n",
      "step : 39.21 % , loss : 6.017037391662598\n",
      "step : 39.65 % , loss : 6.02800178527832\n",
      "step : 40.09 % , loss : 5.9552459716796875\n",
      "step : 40.53 % , loss : 6.05675745010376\n",
      "step : 40.97 % , loss : 5.961008071899414\n",
      "step : 41.41 % , loss : 6.02001428604126\n",
      "step : 41.85 % , loss : 5.947037220001221\n",
      "step : 42.29 % , loss : 6.079134464263916\n",
      "step : 42.73 % , loss : 5.922317028045654\n",
      "step : 43.17 % , loss : 6.065954685211182\n",
      "step : 43.61 % , loss : 5.954277515411377\n",
      "step : 44.05 % , loss : 6.0288405418396\n",
      "step : 44.49 % , loss : 6.042496204376221\n",
      "step : 44.93 % , loss : 6.085354804992676\n",
      "step : 45.37 % , loss : 6.029073238372803\n",
      "step : 45.81 % , loss : 6.003158092498779\n",
      "step : 46.26 % , loss : 6.015820026397705\n",
      "step : 46.7 % , loss : 5.944983959197998\n",
      "step : 47.14 % , loss : 6.000960826873779\n",
      "step : 47.58 % , loss : 5.964735507965088\n",
      "step : 48.02 % , loss : 5.974442005157471\n",
      "step : 48.46 % , loss : 5.965799808502197\n",
      "step : 48.9 % , loss : 5.962108135223389\n",
      "step : 49.34 % , loss : 5.929787635803223\n",
      "step : 49.78 % , loss : 6.091912746429443\n",
      "step : 50.22 % , loss : 5.953597068786621\n",
      "step : 50.66 % , loss : 5.956872940063477\n",
      "step : 51.1 % , loss : 5.993363857269287\n",
      "step : 51.54 % , loss : 6.083477020263672\n",
      "step : 51.98 % , loss : 5.905752182006836\n",
      "step : 52.42 % , loss : 5.982018947601318\n",
      "step : 52.86 % , loss : 6.018414497375488\n",
      "step : 53.3 % , loss : 6.009613513946533\n",
      "step : 53.74 % , loss : 5.933014869689941\n",
      "step : 54.19 % , loss : 5.963477611541748\n",
      "step : 54.63 % , loss : 5.9328837394714355\n",
      "step : 55.07 % , loss : 5.977750301361084\n",
      "step : 55.51 % , loss : 6.0932793617248535\n",
      "step : 55.95 % , loss : 5.964171886444092\n",
      "step : 56.39 % , loss : 6.045289993286133\n",
      "step : 56.83 % , loss : 5.941356658935547\n",
      "step : 57.27 % , loss : 5.972121238708496\n",
      "step : 57.71 % , loss : 5.971149921417236\n",
      "step : 58.15 % , loss : 5.82014274597168\n",
      "step : 58.59 % , loss : 5.988574028015137\n",
      "step : 59.03 % , loss : 6.056122779846191\n",
      "step : 59.47 % , loss : 6.013647556304932\n",
      "step : 59.91 % , loss : 5.908033847808838\n",
      "step : 60.35 % , loss : 5.9798808097839355\n",
      "step : 60.79 % , loss : 5.99512243270874\n",
      "step : 61.23 % , loss : 5.893521785736084\n",
      "step : 61.67 % , loss : 5.887853622436523\n",
      "step : 62.11 % , loss : 5.850966930389404\n",
      "step : 62.56 % , loss : 5.989408016204834\n",
      "step : 63.0 % , loss : 5.9462103843688965\n",
      "step : 63.44 % , loss : 6.06028413772583\n",
      "step : 63.88 % , loss : 5.97013521194458\n",
      "step : 64.32 % , loss : 5.863497257232666\n",
      "step : 64.76 % , loss : 5.953660488128662\n",
      "step : 65.2 % , loss : 5.896711826324463\n",
      "step : 65.64 % , loss : 6.001398086547852\n",
      "step : 66.08 % , loss : 5.824167251586914\n",
      "step : 66.52 % , loss : 5.888323783874512\n",
      "step : 66.96 % , loss : 5.866277694702148\n",
      "step : 67.4 % , loss : 5.920823097229004\n",
      "step : 67.84 % , loss : 5.837961196899414\n",
      "step : 68.28 % , loss : 5.8719987869262695\n",
      "step : 68.72 % , loss : 5.882598876953125\n",
      "step : 69.16 % , loss : 5.925854682922363\n",
      "step : 69.6 % , loss : 5.942880630493164\n",
      "step : 70.04 % , loss : 5.870490074157715\n",
      "step : 70.48 % , loss : 5.861012935638428\n",
      "step : 70.93 % , loss : 5.918719291687012\n",
      "step : 71.37 % , loss : 5.929183483123779\n",
      "step : 71.81 % , loss : 5.941011905670166\n",
      "step : 72.25 % , loss : 5.825392723083496\n",
      "step : 72.69 % , loss : 5.880280017852783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 73.13 % , loss : 5.91168212890625\n",
      "step : 73.57 % , loss : 5.9505109786987305\n",
      "step : 74.01 % , loss : 5.7729411125183105\n",
      "step : 74.45 % , loss : 5.869996070861816\n",
      "step : 74.89 % , loss : 5.855356216430664\n",
      "step : 75.33 % , loss : 5.835970401763916\n",
      "step : 75.77 % , loss : 5.838210105895996\n",
      "step : 76.21 % , loss : 5.768115520477295\n",
      "step : 76.65 % , loss : 5.887077808380127\n",
      "step : 77.09 % , loss : 5.907290458679199\n",
      "step : 77.53 % , loss : 5.839370250701904\n",
      "step : 77.97 % , loss : 5.874125957489014\n",
      "step : 78.41 % , loss : 5.820931434631348\n",
      "step : 78.85 % , loss : 5.8001484870910645\n",
      "step : 79.3 % , loss : 5.778008460998535\n",
      "step : 79.74 % , loss : 5.801072597503662\n",
      "step : 80.18 % , loss : 5.724045753479004\n",
      "step : 80.62 % , loss : 5.8209638595581055\n",
      "step : 81.06 % , loss : 5.831459999084473\n",
      "step : 81.5 % , loss : 5.8325605392456055\n",
      "step : 81.94 % , loss : 5.7292890548706055\n",
      "step : 82.38 % , loss : 5.82082986831665\n",
      "step : 82.82 % , loss : 5.774631977081299\n",
      "step : 83.26 % , loss : 5.76260232925415\n",
      "step : 83.7 % , loss : 5.794399261474609\n",
      "step : 84.14 % , loss : 5.8600358963012695\n",
      "step : 84.58 % , loss : 5.75962495803833\n",
      "step : 85.02 % , loss : 5.779818534851074\n",
      "step : 85.46 % , loss : 5.7753472328186035\n",
      "step : 85.9 % , loss : 5.695950508117676\n",
      "step : 86.34 % , loss : 5.7331223487854\n",
      "step : 86.78 % , loss : 5.777165412902832\n",
      "step : 87.22 % , loss : 5.797525882720947\n",
      "step : 87.67 % , loss : 5.778219223022461\n",
      "step : 88.11 % , loss : 5.726635456085205\n",
      "step : 88.55 % , loss : 5.741787433624268\n",
      "step : 88.99 % , loss : 5.747659206390381\n",
      "step : 89.43 % , loss : 5.786812782287598\n",
      "step : 89.87 % , loss : 5.742818832397461\n",
      "step : 90.31 % , loss : 5.668386936187744\n",
      "step : 90.75 % , loss : 5.745150566101074\n",
      "step : 91.19 % , loss : 5.778225421905518\n",
      "step : 91.63 % , loss : 5.757619380950928\n",
      "step : 92.07 % , loss : 5.778160572052002\n",
      "step : 92.51 % , loss : 5.785686016082764\n",
      "step : 92.95 % , loss : 5.703861236572266\n",
      "step : 93.39 % , loss : 5.724875450134277\n",
      "step : 93.83 % , loss : 5.7409162521362305\n",
      "step : 94.27 % , loss : 5.644792079925537\n",
      "step : 94.71 % , loss : 5.7518486976623535\n",
      "step : 95.15 % , loss : 5.725786209106445\n",
      "step : 95.59 % , loss : 5.732931613922119\n",
      "step : 96.04 % , loss : 5.756255149841309\n",
      "step : 96.48 % , loss : 5.824436187744141\n",
      "step : 96.92 % , loss : 5.694856643676758\n",
      "step : 97.36 % , loss : 5.758425712585449\n",
      "step : 97.8 % , loss : 5.728261470794678\n",
      "step : 98.24 % , loss : 5.737244129180908\n",
      "step : 98.68 % , loss : 5.774542808532715\n",
      "step : 99.12 % , loss : 5.687410354614258\n",
      "step : 99.56 % , loss : 5.648322105407715\n",
      "\tTrain Loss: 5.982\n",
      "step : 0.0 % , loss : 5.714197635650635\n",
      "step : 0.44 % , loss : 5.656002998352051\n",
      "step : 0.88 % , loss : 5.6640496253967285\n",
      "step : 1.32 % , loss : 5.673001766204834\n",
      "step : 1.76 % , loss : 5.604131698608398\n",
      "step : 2.2 % , loss : 5.683906078338623\n",
      "step : 2.64 % , loss : 5.650947570800781\n",
      "step : 3.08 % , loss : 5.678565979003906\n",
      "step : 3.52 % , loss : 5.689762592315674\n",
      "step : 3.96 % , loss : 5.690808296203613\n",
      "step : 4.41 % , loss : 5.79467248916626\n",
      "step : 4.85 % , loss : 5.679057598114014\n",
      "step : 5.29 % , loss : 5.657715320587158\n",
      "step : 5.73 % , loss : 5.661027908325195\n",
      "step : 6.17 % , loss : 5.691802501678467\n",
      "step : 6.61 % , loss : 5.770042896270752\n",
      "step : 7.05 % , loss : 5.602329254150391\n",
      "step : 7.49 % , loss : 5.78611421585083\n",
      "step : 7.93 % , loss : 5.633469581604004\n",
      "step : 8.37 % , loss : 5.7060770988464355\n",
      "step : 8.81 % , loss : 5.622170925140381\n",
      "step : 9.25 % , loss : 5.604513645172119\n",
      "step : 9.69 % , loss : 5.7493133544921875\n",
      "step : 10.13 % , loss : 5.600107669830322\n",
      "step : 10.57 % , loss : 5.681927680969238\n",
      "step : 11.01 % , loss : 5.624072551727295\n",
      "step : 11.45 % , loss : 5.754572868347168\n",
      "step : 11.89 % , loss : 5.708670139312744\n",
      "step : 12.33 % , loss : 5.656991004943848\n",
      "step : 12.78 % , loss : 5.634615898132324\n",
      "step : 13.22 % , loss : 5.68637752532959\n",
      "step : 13.66 % , loss : 5.748178482055664\n",
      "step : 14.1 % , loss : 5.57725191116333\n",
      "step : 14.54 % , loss : 5.601355075836182\n",
      "step : 14.98 % , loss : 5.65306282043457\n",
      "step : 15.42 % , loss : 5.696187973022461\n",
      "step : 15.86 % , loss : 5.686806678771973\n",
      "step : 16.3 % , loss : 5.650774955749512\n",
      "step : 16.74 % , loss : 5.645116806030273\n",
      "step : 17.18 % , loss : 5.691868305206299\n",
      "step : 17.62 % , loss : 5.67777681350708\n",
      "step : 18.06 % , loss : 5.601654052734375\n",
      "step : 18.5 % , loss : 5.744524002075195\n",
      "step : 18.94 % , loss : 5.622241497039795\n",
      "step : 19.38 % , loss : 5.64677095413208\n",
      "step : 19.82 % , loss : 5.6477580070495605\n",
      "step : 20.26 % , loss : 5.625091075897217\n",
      "step : 20.7 % , loss : 5.605902671813965\n",
      "step : 21.15 % , loss : 5.649337291717529\n",
      "step : 21.59 % , loss : 5.670040130615234\n",
      "step : 22.03 % , loss : 5.611520290374756\n",
      "step : 22.47 % , loss : 5.6048784255981445\n",
      "step : 22.91 % , loss : 5.5881266593933105\n",
      "step : 23.35 % , loss : 5.698357105255127\n",
      "step : 23.79 % , loss : 5.709288597106934\n",
      "step : 24.23 % , loss : 5.634185314178467\n",
      "step : 24.67 % , loss : 5.763326644897461\n",
      "step : 25.11 % , loss : 5.685920715332031\n",
      "step : 25.55 % , loss : 5.632061004638672\n",
      "step : 25.99 % , loss : 5.626733303070068\n",
      "step : 26.43 % , loss : 5.659029483795166\n",
      "step : 26.87 % , loss : 5.7037272453308105\n",
      "step : 27.31 % , loss : 5.7372541427612305\n",
      "step : 27.75 % , loss : 5.605839252471924\n",
      "step : 28.19 % , loss : 5.676767826080322\n",
      "step : 28.63 % , loss : 5.606472969055176\n",
      "step : 29.07 % , loss : 5.627572059631348\n",
      "step : 29.52 % , loss : 5.614459037780762\n",
      "step : 29.96 % , loss : 5.545835971832275\n",
      "step : 30.4 % , loss : 5.616404056549072\n",
      "step : 30.84 % , loss : 5.6692986488342285\n",
      "step : 31.28 % , loss : 5.576045989990234\n",
      "step : 31.72 % , loss : 5.525839328765869\n",
      "step : 32.16 % , loss : 5.570446968078613\n",
      "step : 32.6 % , loss : 5.64945650100708\n",
      "step : 33.04 % , loss : 5.648772716522217\n",
      "step : 33.48 % , loss : 5.622693061828613\n",
      "step : 33.92 % , loss : 5.580005645751953\n",
      "step : 34.36 % , loss : 5.587371349334717\n",
      "step : 34.8 % , loss : 5.626707553863525\n",
      "step : 35.24 % , loss : 5.707244873046875\n",
      "step : 35.68 % , loss : 5.652294635772705\n",
      "step : 36.12 % , loss : 5.602982521057129\n",
      "step : 36.56 % , loss : 5.562346458435059\n",
      "step : 37.0 % , loss : 5.531713962554932\n",
      "step : 37.44 % , loss : 5.600745677947998\n",
      "step : 37.89 % , loss : 5.57283353805542\n",
      "step : 38.33 % , loss : 5.6781463623046875\n",
      "step : 38.77 % , loss : 5.6765971183776855\n",
      "step : 39.21 % , loss : 5.6210150718688965\n",
      "step : 39.65 % , loss : 5.644642353057861\n",
      "step : 40.09 % , loss : 5.624977111816406\n",
      "step : 40.53 % , loss : 5.540139675140381\n",
      "step : 40.97 % , loss : 5.63791036605835\n",
      "step : 41.41 % , loss : 5.692868709564209\n",
      "step : 41.85 % , loss : 5.594111442565918\n",
      "step : 42.29 % , loss : 5.605328559875488\n",
      "step : 42.73 % , loss : 5.5840559005737305\n",
      "step : 43.17 % , loss : 5.568784713745117\n",
      "step : 43.61 % , loss : 5.5692973136901855\n",
      "step : 44.05 % , loss : 5.597078800201416\n",
      "step : 44.49 % , loss : 5.554465293884277\n",
      "step : 44.93 % , loss : 5.530840873718262\n",
      "step : 45.37 % , loss : 5.622992992401123\n",
      "step : 45.81 % , loss : 5.587284088134766\n",
      "step : 46.26 % , loss : 5.594305992126465\n",
      "step : 46.7 % , loss : 5.589125633239746\n",
      "step : 47.14 % , loss : 5.55499792098999\n",
      "step : 47.58 % , loss : 5.54573917388916\n",
      "step : 48.02 % , loss : 5.639516353607178\n",
      "step : 48.46 % , loss : 5.7177348136901855\n",
      "step : 48.9 % , loss : 5.740905284881592\n",
      "step : 49.34 % , loss : 5.59799337387085\n",
      "step : 49.78 % , loss : 5.615935802459717\n",
      "step : 50.22 % , loss : 5.568789958953857\n",
      "step : 50.66 % , loss : 5.610685348510742\n",
      "step : 51.1 % , loss : 5.532636642456055\n",
      "step : 51.54 % , loss : 5.555478096008301\n",
      "step : 51.98 % , loss : 5.614632606506348\n",
      "step : 52.42 % , loss : 5.735636234283447\n",
      "step : 52.86 % , loss : 5.554892539978027\n",
      "step : 53.3 % , loss : 5.50407075881958\n",
      "step : 53.74 % , loss : 5.56602668762207\n",
      "step : 54.19 % , loss : 5.4957146644592285\n",
      "step : 54.63 % , loss : 5.732285976409912\n",
      "step : 55.07 % , loss : 5.598170757293701\n",
      "step : 55.51 % , loss : 5.5457763671875\n",
      "step : 55.95 % , loss : 5.566127777099609\n",
      "step : 56.39 % , loss : 5.624099254608154\n",
      "step : 56.83 % , loss : 5.553496360778809\n",
      "step : 57.27 % , loss : 5.582494258880615\n",
      "step : 57.71 % , loss : 5.451413631439209\n",
      "step : 58.15 % , loss : 5.490139007568359\n",
      "step : 58.59 % , loss : 5.5616326332092285\n",
      "step : 59.03 % , loss : 5.621622562408447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 59.47 % , loss : 5.5650315284729\n",
      "step : 59.91 % , loss : 5.548281669616699\n",
      "step : 60.35 % , loss : 5.521976470947266\n",
      "step : 60.79 % , loss : 5.555530071258545\n",
      "step : 61.23 % , loss : 5.602028846740723\n",
      "step : 61.67 % , loss : 5.607512950897217\n",
      "step : 62.11 % , loss : 5.564645290374756\n",
      "step : 62.56 % , loss : 5.558887481689453\n",
      "step : 63.0 % , loss : 5.491245746612549\n",
      "step : 63.44 % , loss : 5.627364635467529\n",
      "step : 63.88 % , loss : 5.6169023513793945\n",
      "step : 64.32 % , loss : 5.6086344718933105\n",
      "step : 64.76 % , loss : 5.574284553527832\n",
      "step : 65.2 % , loss : 5.551684856414795\n",
      "step : 65.64 % , loss : 5.5131001472473145\n",
      "step : 66.08 % , loss : 5.648375034332275\n",
      "step : 66.52 % , loss : 5.430470943450928\n",
      "step : 66.96 % , loss : 5.574099063873291\n",
      "step : 67.4 % , loss : 5.472768306732178\n",
      "step : 67.84 % , loss : 5.503045082092285\n",
      "step : 68.28 % , loss : 5.492978096008301\n",
      "step : 68.72 % , loss : 5.451086044311523\n",
      "step : 69.16 % , loss : 5.544477939605713\n",
      "step : 69.6 % , loss : 5.473090171813965\n",
      "step : 70.04 % , loss : 5.586246013641357\n",
      "step : 70.48 % , loss : 5.505670070648193\n",
      "step : 70.93 % , loss : 5.514352321624756\n",
      "step : 71.37 % , loss : 5.481789588928223\n",
      "step : 71.81 % , loss : 5.520854473114014\n",
      "step : 72.25 % , loss : 5.546212673187256\n",
      "step : 72.69 % , loss : 5.560218334197998\n",
      "step : 73.13 % , loss : 5.578544616699219\n",
      "step : 73.57 % , loss : 5.551455497741699\n",
      "step : 74.01 % , loss : 5.634370803833008\n",
      "step : 74.45 % , loss : 5.490611553192139\n",
      "step : 74.89 % , loss : 5.568933486938477\n",
      "step : 75.33 % , loss : 5.543766021728516\n",
      "step : 75.77 % , loss : 5.55303955078125\n",
      "step : 76.21 % , loss : 5.413619041442871\n",
      "step : 76.65 % , loss : 5.506731033325195\n",
      "step : 77.09 % , loss : 5.512465476989746\n",
      "step : 77.53 % , loss : 5.6755828857421875\n",
      "step : 77.97 % , loss : 5.515838623046875\n",
      "step : 78.41 % , loss : 5.538083076477051\n",
      "step : 78.85 % , loss : 5.588677406311035\n",
      "step : 79.3 % , loss : 5.485344409942627\n",
      "step : 79.74 % , loss : 5.458725929260254\n",
      "step : 80.18 % , loss : 5.487885475158691\n",
      "step : 80.62 % , loss : 5.563037395477295\n",
      "step : 81.06 % , loss : 5.406726837158203\n",
      "step : 81.5 % , loss : 5.605465888977051\n",
      "step : 81.94 % , loss : 5.5656256675720215\n",
      "step : 82.38 % , loss : 5.587101459503174\n",
      "step : 82.82 % , loss : 5.5190606117248535\n",
      "step : 83.26 % , loss : 5.501584053039551\n",
      "step : 83.7 % , loss : 5.611091136932373\n",
      "step : 84.14 % , loss : 5.492557525634766\n",
      "step : 84.58 % , loss : 5.514037132263184\n",
      "step : 85.02 % , loss : 5.521410942077637\n",
      "step : 85.46 % , loss : 5.497422695159912\n",
      "step : 85.9 % , loss : 5.580073356628418\n",
      "step : 86.34 % , loss : 5.488166809082031\n",
      "step : 86.78 % , loss : 5.596333980560303\n",
      "step : 87.22 % , loss : 5.497504234313965\n",
      "step : 87.67 % , loss : 5.522335529327393\n",
      "step : 88.11 % , loss : 5.517399787902832\n",
      "step : 88.55 % , loss : 5.483953475952148\n",
      "step : 88.99 % , loss : 5.473330497741699\n",
      "step : 89.43 % , loss : 5.500983715057373\n",
      "step : 89.87 % , loss : 5.645870208740234\n",
      "step : 90.31 % , loss : 5.519855499267578\n",
      "step : 90.75 % , loss : 5.561500549316406\n",
      "step : 91.19 % , loss : 5.567627429962158\n",
      "step : 91.63 % , loss : 5.5698628425598145\n",
      "step : 92.07 % , loss : 5.541777610778809\n",
      "step : 92.51 % , loss : 5.566339492797852\n",
      "step : 92.95 % , loss : 5.487117767333984\n",
      "step : 93.39 % , loss : 5.537729740142822\n",
      "step : 93.83 % , loss : 5.589229106903076\n",
      "step : 94.27 % , loss : 5.584974765777588\n",
      "step : 94.71 % , loss : 5.429028034210205\n",
      "step : 95.15 % , loss : 5.5233473777771\n",
      "step : 95.59 % , loss : 5.582639694213867\n",
      "step : 96.04 % , loss : 5.526418685913086\n",
      "step : 96.48 % , loss : 5.4457902908325195\n",
      "step : 96.92 % , loss : 5.4235453605651855\n",
      "step : 97.36 % , loss : 5.4709954261779785\n",
      "step : 97.8 % , loss : 5.5533928871154785\n",
      "step : 98.24 % , loss : 5.4702467918396\n",
      "step : 98.68 % , loss : 5.585720062255859\n",
      "step : 99.12 % , loss : 5.549505710601807\n",
      "step : 99.56 % , loss : 5.4495768547058105\n",
      "\tTrain Loss: 5.592\n",
      "step : 0.0 % , loss : 5.468296051025391\n",
      "step : 0.44 % , loss : 5.455402374267578\n",
      "step : 0.88 % , loss : 5.517027378082275\n",
      "step : 1.32 % , loss : 5.451127052307129\n",
      "step : 1.76 % , loss : 5.43377161026001\n",
      "step : 2.2 % , loss : 5.478066921234131\n",
      "step : 2.64 % , loss : 5.352559566497803\n",
      "step : 3.08 % , loss : 5.491479396820068\n",
      "step : 3.52 % , loss : 5.469374179840088\n",
      "step : 3.96 % , loss : 5.523563385009766\n",
      "step : 4.41 % , loss : 5.507845878601074\n",
      "step : 4.85 % , loss : 5.429421901702881\n",
      "step : 5.29 % , loss : 5.476994514465332\n",
      "step : 5.73 % , loss : 5.475311756134033\n",
      "step : 6.17 % , loss : 5.468766689300537\n",
      "step : 6.61 % , loss : 5.602768898010254\n",
      "step : 7.05 % , loss : 5.429738998413086\n",
      "step : 7.49 % , loss : 5.496660232543945\n",
      "step : 7.93 % , loss : 5.391660690307617\n",
      "step : 8.37 % , loss : 5.508703708648682\n",
      "step : 8.81 % , loss : 5.5123677253723145\n",
      "step : 9.25 % , loss : 5.413638591766357\n",
      "step : 9.69 % , loss : 5.405989170074463\n",
      "step : 10.13 % , loss : 5.534017086029053\n",
      "step : 10.57 % , loss : 5.487733840942383\n",
      "step : 11.01 % , loss : 5.531035900115967\n",
      "step : 11.45 % , loss : 5.485514163970947\n",
      "step : 11.89 % , loss : 5.3762712478637695\n",
      "step : 12.33 % , loss : 5.441028594970703\n",
      "step : 12.78 % , loss : 5.40725564956665\n",
      "step : 13.22 % , loss : 5.442903518676758\n",
      "step : 13.66 % , loss : 5.333868980407715\n",
      "step : 14.1 % , loss : 5.382953643798828\n",
      "step : 14.54 % , loss : 5.395885944366455\n",
      "step : 14.98 % , loss : 5.5963568687438965\n",
      "step : 15.42 % , loss : 5.564777851104736\n",
      "step : 15.86 % , loss : 5.440891265869141\n",
      "step : 16.3 % , loss : 5.3570427894592285\n",
      "step : 16.74 % , loss : 5.482448577880859\n",
      "step : 17.18 % , loss : 5.35993766784668\n",
      "step : 17.62 % , loss : 5.413634777069092\n",
      "step : 18.06 % , loss : 5.409714698791504\n",
      "step : 18.5 % , loss : 5.447826862335205\n",
      "step : 18.94 % , loss : 5.514740943908691\n",
      "step : 19.38 % , loss : 5.512807369232178\n",
      "step : 19.82 % , loss : 5.401051044464111\n",
      "step : 20.26 % , loss : 5.5116753578186035\n",
      "step : 20.7 % , loss : 5.374161720275879\n",
      "step : 21.15 % , loss : 5.405741214752197\n",
      "step : 21.59 % , loss : 5.394845962524414\n",
      "step : 22.03 % , loss : 5.509289741516113\n",
      "step : 22.47 % , loss : 5.429152488708496\n",
      "step : 22.91 % , loss : 5.406962871551514\n",
      "step : 23.35 % , loss : 5.551540851593018\n",
      "step : 23.79 % , loss : 5.536559104919434\n",
      "step : 24.23 % , loss : 5.447474956512451\n",
      "step : 24.67 % , loss : 5.420037746429443\n",
      "step : 25.11 % , loss : 5.509448051452637\n",
      "step : 25.55 % , loss : 5.55333948135376\n",
      "step : 25.99 % , loss : 5.494792461395264\n",
      "step : 26.43 % , loss : 5.384571075439453\n",
      "step : 26.87 % , loss : 5.350468635559082\n",
      "step : 27.31 % , loss : 5.42748498916626\n",
      "step : 27.75 % , loss : 5.4642438888549805\n",
      "step : 28.19 % , loss : 5.4625349044799805\n",
      "step : 28.63 % , loss : 5.473119258880615\n",
      "step : 29.07 % , loss : 5.406013488769531\n",
      "step : 29.52 % , loss : 5.369107723236084\n",
      "step : 29.96 % , loss : 5.401182651519775\n",
      "step : 30.4 % , loss : 5.517703533172607\n",
      "step : 30.84 % , loss : 5.490336894989014\n",
      "step : 31.28 % , loss : 5.395315647125244\n",
      "step : 31.72 % , loss : 5.484453201293945\n",
      "step : 32.16 % , loss : 5.404181480407715\n",
      "step : 32.6 % , loss : 5.510740280151367\n",
      "step : 33.04 % , loss : 5.509469509124756\n",
      "step : 33.48 % , loss : 5.461043834686279\n",
      "step : 33.92 % , loss : 5.51333475112915\n",
      "step : 34.36 % , loss : 5.439556121826172\n",
      "step : 34.8 % , loss : 5.513893127441406\n",
      "step : 35.24 % , loss : 5.483736515045166\n",
      "step : 35.68 % , loss : 5.424371242523193\n",
      "step : 36.12 % , loss : 5.3607659339904785\n",
      "step : 36.56 % , loss : 5.42588996887207\n",
      "step : 37.0 % , loss : 5.411139011383057\n",
      "step : 37.44 % , loss : 5.323563098907471\n",
      "step : 37.89 % , loss : 5.493701934814453\n",
      "step : 38.33 % , loss : 5.4423112869262695\n",
      "step : 38.77 % , loss : 5.52797794342041\n",
      "step : 39.21 % , loss : 5.4723920822143555\n",
      "step : 39.65 % , loss : 5.366369247436523\n",
      "step : 40.09 % , loss : 5.435625076293945\n",
      "step : 40.53 % , loss : 5.4114813804626465\n",
      "step : 40.97 % , loss : 5.413455963134766\n",
      "step : 41.41 % , loss : 5.472373008728027\n",
      "step : 41.85 % , loss : 5.481300354003906\n",
      "step : 42.29 % , loss : 5.387972831726074\n",
      "step : 42.73 % , loss : 5.384481906890869\n",
      "step : 43.17 % , loss : 5.409651279449463\n",
      "step : 43.61 % , loss : 5.382261753082275\n",
      "step : 44.05 % , loss : 5.420563220977783\n",
      "step : 44.49 % , loss : 5.396578788757324\n",
      "step : 44.93 % , loss : 5.4884934425354\n",
      "step : 45.37 % , loss : 5.497663497924805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 45.81 % , loss : 5.370204925537109\n",
      "step : 46.26 % , loss : 5.399620056152344\n",
      "step : 46.7 % , loss : 5.444083213806152\n",
      "step : 47.14 % , loss : 5.390293598175049\n",
      "step : 47.58 % , loss : 5.404129505157471\n",
      "step : 48.02 % , loss : 5.392580986022949\n",
      "step : 48.46 % , loss : 5.400308609008789\n",
      "step : 48.9 % , loss : 5.419598579406738\n",
      "step : 49.34 % , loss : 5.4580512046813965\n",
      "step : 49.78 % , loss : 5.3514838218688965\n",
      "step : 50.22 % , loss : 5.391414165496826\n",
      "step : 50.66 % , loss : 5.415552139282227\n",
      "step : 51.1 % , loss : 5.4466962814331055\n",
      "step : 51.54 % , loss : 5.38783073425293\n",
      "step : 51.98 % , loss : 5.318424224853516\n",
      "step : 52.42 % , loss : 5.442814350128174\n",
      "step : 52.86 % , loss : 5.30525016784668\n",
      "step : 53.3 % , loss : 5.45377254486084\n",
      "step : 53.74 % , loss : 5.477793216705322\n",
      "step : 54.19 % , loss : 5.388012886047363\n",
      "step : 54.63 % , loss : 5.434401512145996\n",
      "step : 55.07 % , loss : 5.4332051277160645\n",
      "step : 55.51 % , loss : 5.309340476989746\n",
      "step : 55.95 % , loss : 5.283465385437012\n",
      "step : 56.39 % , loss : 5.399906635284424\n",
      "step : 56.83 % , loss : 5.379639148712158\n",
      "step : 57.27 % , loss : 5.485589504241943\n",
      "step : 57.71 % , loss : 5.370849132537842\n",
      "step : 58.15 % , loss : 5.390627861022949\n",
      "step : 58.59 % , loss : 5.432531356811523\n",
      "step : 59.03 % , loss : 5.434196949005127\n",
      "step : 59.47 % , loss : 5.388711452484131\n",
      "step : 59.91 % , loss : 5.324427127838135\n",
      "step : 60.35 % , loss : 5.365208148956299\n",
      "step : 60.79 % , loss : 5.424594402313232\n",
      "step : 61.23 % , loss : 5.336426258087158\n",
      "step : 61.67 % , loss : 5.385122299194336\n",
      "step : 62.11 % , loss : 5.410628795623779\n",
      "step : 62.56 % , loss : 5.432570457458496\n",
      "step : 63.0 % , loss : 5.370246887207031\n",
      "step : 63.44 % , loss : 5.466699123382568\n",
      "step : 63.88 % , loss : 5.415771961212158\n",
      "step : 64.32 % , loss : 5.271864891052246\n",
      "step : 64.76 % , loss : 5.455917835235596\n",
      "step : 65.2 % , loss : 5.3981194496154785\n",
      "step : 65.64 % , loss : 5.365816593170166\n",
      "step : 66.08 % , loss : 5.3318071365356445\n",
      "step : 66.52 % , loss : 5.462157726287842\n",
      "step : 66.96 % , loss : 5.4165449142456055\n",
      "step : 67.4 % , loss : 5.441816329956055\n",
      "step : 67.84 % , loss : 5.352766990661621\n",
      "step : 68.28 % , loss : 5.370691299438477\n",
      "step : 68.72 % , loss : 5.3646440505981445\n",
      "step : 69.16 % , loss : 5.454237937927246\n",
      "step : 69.6 % , loss : 5.33317756652832\n",
      "step : 70.04 % , loss : 5.352567672729492\n",
      "step : 70.48 % , loss : 5.35359001159668\n",
      "step : 70.93 % , loss : 5.445807933807373\n",
      "step : 71.37 % , loss : 5.362761974334717\n",
      "step : 71.81 % , loss : 5.336667060852051\n",
      "step : 72.25 % , loss : 5.369212627410889\n",
      "step : 72.69 % , loss : 5.392862319946289\n",
      "step : 73.13 % , loss : 5.333581447601318\n",
      "step : 73.57 % , loss : 5.346772193908691\n",
      "step : 74.01 % , loss : 5.341023921966553\n",
      "step : 74.45 % , loss : 5.299874782562256\n",
      "step : 74.89 % , loss : 5.2761921882629395\n",
      "step : 75.33 % , loss : 5.333204746246338\n",
      "step : 75.77 % , loss : 5.3617939949035645\n",
      "step : 76.21 % , loss : 5.392561435699463\n",
      "step : 76.65 % , loss : 5.411255836486816\n",
      "step : 77.09 % , loss : 5.4208292961120605\n",
      "step : 77.53 % , loss : 5.428659915924072\n",
      "step : 77.97 % , loss : 5.247621059417725\n",
      "step : 78.41 % , loss : 5.389391899108887\n",
      "step : 78.85 % , loss : 5.333465099334717\n",
      "step : 79.3 % , loss : 5.351239204406738\n",
      "step : 79.74 % , loss : 5.438128471374512\n",
      "step : 80.18 % , loss : 5.307469844818115\n",
      "step : 80.62 % , loss : 5.402968406677246\n",
      "step : 81.06 % , loss : 5.307363510131836\n",
      "step : 81.5 % , loss : 5.298864364624023\n",
      "step : 81.94 % , loss : 5.334388256072998\n",
      "step : 82.38 % , loss : 5.391509056091309\n",
      "step : 82.82 % , loss : 5.208346366882324\n",
      "step : 83.26 % , loss : 5.414275646209717\n",
      "step : 83.7 % , loss : 5.387580394744873\n",
      "step : 84.14 % , loss : 5.329479217529297\n",
      "step : 84.58 % , loss : 5.320270538330078\n",
      "step : 85.02 % , loss : 5.442368030548096\n",
      "step : 85.46 % , loss : 5.355144023895264\n",
      "step : 85.9 % , loss : 5.366863250732422\n",
      "step : 86.34 % , loss : 5.3973541259765625\n",
      "step : 86.78 % , loss : 5.2171759605407715\n",
      "step : 87.22 % , loss : 5.291154861450195\n",
      "step : 87.67 % , loss : 5.289698123931885\n",
      "step : 88.11 % , loss : 5.211170673370361\n",
      "step : 88.55 % , loss : 5.464705467224121\n",
      "step : 88.99 % , loss : 5.373774528503418\n",
      "step : 89.43 % , loss : 5.280994415283203\n",
      "step : 89.87 % , loss : 5.3086371421813965\n",
      "step : 90.31 % , loss : 5.358168601989746\n",
      "step : 90.75 % , loss : 5.286407947540283\n",
      "step : 91.19 % , loss : 5.347969055175781\n",
      "step : 91.63 % , loss : 5.30687952041626\n",
      "step : 92.07 % , loss : 5.247546195983887\n",
      "step : 92.51 % , loss : 5.389998912811279\n",
      "step : 92.95 % , loss : 5.390336036682129\n",
      "step : 93.39 % , loss : 5.3428826332092285\n",
      "step : 93.83 % , loss : 5.405713081359863\n",
      "step : 94.27 % , loss : 5.365172386169434\n",
      "step : 94.71 % , loss : 5.301510334014893\n",
      "step : 95.15 % , loss : 5.359968662261963\n",
      "step : 95.59 % , loss : 5.420607089996338\n",
      "step : 96.04 % , loss : 5.305458068847656\n",
      "step : 96.48 % , loss : 5.280500888824463\n",
      "step : 96.92 % , loss : 5.26059627532959\n",
      "step : 97.36 % , loss : 5.375076770782471\n",
      "step : 97.8 % , loss : 5.355841159820557\n",
      "step : 98.24 % , loss : 5.296332836151123\n",
      "step : 98.68 % , loss : 5.344281196594238\n",
      "step : 99.12 % , loss : 5.353736400604248\n",
      "step : 99.56 % , loss : 5.397848129272461\n",
      "\tTrain Loss: 5.406\n",
      "step : 0.0 % , loss : 5.421623706817627\n",
      "step : 0.44 % , loss : 5.286952972412109\n",
      "step : 0.88 % , loss : 5.435337543487549\n",
      "step : 1.32 % , loss : 5.364021301269531\n",
      "step : 1.76 % , loss : 5.319254398345947\n",
      "step : 2.2 % , loss : 5.309329986572266\n",
      "step : 2.64 % , loss : 5.305301189422607\n",
      "step : 3.08 % , loss : 5.344829082489014\n",
      "step : 3.52 % , loss : 5.223190784454346\n",
      "step : 3.96 % , loss : 5.202548027038574\n",
      "step : 4.41 % , loss : 5.36954402923584\n",
      "step : 4.85 % , loss : 5.325127601623535\n",
      "step : 5.29 % , loss : 5.368447780609131\n",
      "step : 5.73 % , loss : 5.302733898162842\n",
      "step : 6.17 % , loss : 5.318750381469727\n",
      "step : 6.61 % , loss : 5.2990593910217285\n",
      "step : 7.05 % , loss : 5.214999198913574\n",
      "step : 7.49 % , loss : 5.3155388832092285\n",
      "step : 7.93 % , loss : 5.289144992828369\n",
      "step : 8.37 % , loss : 5.334134578704834\n",
      "step : 8.81 % , loss : 5.275434494018555\n",
      "step : 9.25 % , loss : 5.312870502471924\n",
      "step : 9.69 % , loss : 5.368652820587158\n",
      "step : 10.13 % , loss : 5.415832996368408\n",
      "step : 10.57 % , loss : 5.358164310455322\n",
      "step : 11.01 % , loss : 5.256628036499023\n",
      "step : 11.45 % , loss : 5.299017906188965\n",
      "step : 11.89 % , loss : 5.307080268859863\n",
      "step : 12.33 % , loss : 5.329006195068359\n",
      "step : 12.78 % , loss : 5.3617095947265625\n",
      "step : 13.22 % , loss : 5.324800968170166\n",
      "step : 13.66 % , loss : 5.321672439575195\n",
      "step : 14.1 % , loss : 5.4370527267456055\n",
      "step : 14.54 % , loss : 5.206791877746582\n",
      "step : 14.98 % , loss : 5.345623016357422\n",
      "step : 15.42 % , loss : 5.3709282875061035\n",
      "step : 15.86 % , loss : 5.3318963050842285\n",
      "step : 16.3 % , loss : 5.251251697540283\n",
      "step : 16.74 % , loss : 5.289907932281494\n",
      "step : 17.18 % , loss : 5.314701080322266\n",
      "step : 17.62 % , loss : 5.300157070159912\n",
      "step : 18.06 % , loss : 5.339595794677734\n",
      "step : 18.5 % , loss : 5.342665195465088\n",
      "step : 18.94 % , loss : 5.229649543762207\n",
      "step : 19.38 % , loss : 5.34918737411499\n",
      "step : 19.82 % , loss : 5.258687496185303\n",
      "step : 20.26 % , loss : 5.2855353355407715\n",
      "step : 20.7 % , loss : 5.265035629272461\n",
      "step : 21.15 % , loss : 5.246838092803955\n",
      "step : 21.59 % , loss : 5.35732889175415\n",
      "step : 22.03 % , loss : 5.443420886993408\n",
      "step : 22.47 % , loss : 5.239969730377197\n",
      "step : 22.91 % , loss : 5.373768329620361\n",
      "step : 23.35 % , loss : 5.235572814941406\n",
      "step : 23.79 % , loss : 5.390132904052734\n",
      "step : 24.23 % , loss : 5.271246910095215\n",
      "step : 24.67 % , loss : 5.281437873840332\n",
      "step : 25.11 % , loss : 5.361983776092529\n",
      "step : 25.55 % , loss : 5.402320861816406\n",
      "step : 25.99 % , loss : 5.45405387878418\n",
      "step : 26.43 % , loss : 5.2711639404296875\n",
      "step : 26.87 % , loss : 5.320676803588867\n",
      "step : 27.31 % , loss : 5.31003999710083\n",
      "step : 27.75 % , loss : 5.271176338195801\n",
      "step : 28.19 % , loss : 5.230943202972412\n",
      "step : 28.63 % , loss : 5.270212650299072\n",
      "step : 29.07 % , loss : 5.356142044067383\n",
      "step : 29.52 % , loss : 5.292684555053711\n",
      "step : 29.96 % , loss : 5.29770565032959\n",
      "step : 30.4 % , loss : 5.341399669647217\n",
      "step : 30.84 % , loss : 5.183460712432861\n",
      "step : 31.28 % , loss : 5.4003095626831055\n",
      "step : 31.72 % , loss : 5.30907678604126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 32.16 % , loss : 5.415328502655029\n",
      "step : 32.6 % , loss : 5.378604888916016\n",
      "step : 33.04 % , loss : 5.376735210418701\n",
      "step : 33.48 % , loss : 5.323116302490234\n",
      "step : 33.92 % , loss : 5.315818786621094\n",
      "step : 34.36 % , loss : 5.331589698791504\n",
      "step : 34.8 % , loss : 5.214723110198975\n",
      "step : 35.24 % , loss : 5.356569290161133\n",
      "step : 35.68 % , loss : 5.386078357696533\n",
      "step : 36.12 % , loss : 5.289528846740723\n",
      "step : 36.56 % , loss : 5.348890781402588\n",
      "step : 37.0 % , loss : 5.210110187530518\n",
      "step : 37.44 % , loss : 5.326951503753662\n",
      "step : 37.89 % , loss : 5.291118144989014\n",
      "step : 38.33 % , loss : 5.343228816986084\n",
      "step : 38.77 % , loss : 5.370395183563232\n",
      "step : 39.21 % , loss : 5.284403324127197\n",
      "step : 39.65 % , loss : 5.312213897705078\n",
      "step : 40.09 % , loss : 5.2854766845703125\n",
      "step : 40.53 % , loss : 5.264047145843506\n",
      "step : 40.97 % , loss : 5.187804222106934\n",
      "step : 41.41 % , loss : 5.21824312210083\n",
      "step : 41.85 % , loss : 5.348559856414795\n",
      "step : 42.29 % , loss : 5.282342910766602\n",
      "step : 42.73 % , loss : 5.358249187469482\n",
      "step : 43.17 % , loss : 5.295808792114258\n",
      "step : 43.61 % , loss : 5.2356977462768555\n",
      "step : 44.05 % , loss : 5.321995735168457\n",
      "step : 44.49 % , loss : 5.324833393096924\n",
      "step : 44.93 % , loss : 5.3036932945251465\n",
      "step : 45.37 % , loss : 5.165045738220215\n",
      "step : 45.81 % , loss : 5.227633953094482\n",
      "step : 46.26 % , loss : 5.224482536315918\n",
      "step : 46.7 % , loss : 5.194366455078125\n",
      "step : 47.14 % , loss : 5.3121843338012695\n",
      "step : 47.58 % , loss : 5.196621894836426\n",
      "step : 48.02 % , loss : 5.28280782699585\n",
      "step : 48.46 % , loss : 5.326769828796387\n",
      "step : 48.9 % , loss : 5.290741920471191\n",
      "step : 49.34 % , loss : 5.27187442779541\n",
      "step : 49.78 % , loss : 5.1317853927612305\n",
      "step : 50.22 % , loss : 5.376605033874512\n",
      "step : 50.66 % , loss : 5.232181072235107\n",
      "step : 51.1 % , loss : 5.319745063781738\n",
      "step : 51.54 % , loss : 5.219334602355957\n",
      "step : 51.98 % , loss : 5.253615379333496\n",
      "step : 52.42 % , loss : 5.285453796386719\n",
      "step : 52.86 % , loss : 5.23148775100708\n",
      "step : 53.3 % , loss : 5.290109157562256\n",
      "step : 53.74 % , loss : 5.223546981811523\n",
      "step : 54.19 % , loss : 5.246402740478516\n",
      "step : 54.63 % , loss : 5.205793380737305\n",
      "step : 55.07 % , loss : 5.231637477874756\n",
      "step : 55.51 % , loss : 5.303426742553711\n",
      "step : 55.95 % , loss : 5.266722202301025\n",
      "step : 56.39 % , loss : 5.28333854675293\n",
      "step : 56.83 % , loss : 5.300389766693115\n",
      "step : 57.27 % , loss : 5.291018009185791\n",
      "step : 57.71 % , loss : 5.1805291175842285\n",
      "step : 58.15 % , loss : 5.234357833862305\n",
      "step : 58.59 % , loss : 5.260597229003906\n",
      "step : 59.03 % , loss : 5.404253005981445\n",
      "step : 59.47 % , loss : 5.306093692779541\n",
      "step : 59.91 % , loss : 5.2201433181762695\n",
      "step : 60.35 % , loss : 5.275482654571533\n",
      "step : 60.79 % , loss : 5.242674827575684\n",
      "step : 61.23 % , loss : 5.213344573974609\n",
      "step : 61.67 % , loss : 5.35770320892334\n",
      "step : 62.11 % , loss : 5.247518539428711\n",
      "step : 62.56 % , loss : 5.302011013031006\n",
      "step : 63.0 % , loss : 5.2650346755981445\n",
      "step : 63.44 % , loss : 5.167701244354248\n",
      "step : 63.88 % , loss : 5.273584365844727\n",
      "step : 64.32 % , loss : 5.272511959075928\n",
      "step : 64.76 % , loss : 5.291089057922363\n",
      "step : 65.2 % , loss : 5.335342884063721\n",
      "step : 65.64 % , loss : 5.125941276550293\n",
      "step : 66.08 % , loss : 5.338109016418457\n",
      "step : 66.52 % , loss : 5.295472621917725\n",
      "step : 66.96 % , loss : 5.311861515045166\n",
      "step : 67.4 % , loss : 5.15651273727417\n",
      "step : 67.84 % , loss : 5.362085342407227\n",
      "step : 68.28 % , loss : 5.35150671005249\n",
      "step : 68.72 % , loss : 5.377182483673096\n",
      "step : 69.16 % , loss : 5.367699146270752\n",
      "step : 69.6 % , loss : 5.291569709777832\n",
      "step : 70.04 % , loss : 5.27296257019043\n",
      "step : 70.48 % , loss : 5.298524856567383\n",
      "step : 70.93 % , loss : 5.25040864944458\n",
      "step : 71.37 % , loss : 5.197840690612793\n",
      "step : 71.81 % , loss : 5.18254280090332\n",
      "step : 72.25 % , loss : 5.155574798583984\n",
      "step : 72.69 % , loss : 5.24808931350708\n",
      "step : 73.13 % , loss : 5.298820495605469\n",
      "step : 73.57 % , loss : 5.299214839935303\n",
      "step : 74.01 % , loss : 5.246623992919922\n",
      "step : 74.45 % , loss : 5.281140327453613\n",
      "step : 74.89 % , loss : 5.263822078704834\n",
      "step : 75.33 % , loss : 5.254734992980957\n",
      "step : 75.77 % , loss : 5.2475266456604\n",
      "step : 76.21 % , loss : 5.238624572753906\n",
      "step : 76.65 % , loss : 5.203968048095703\n",
      "step : 77.09 % , loss : 5.18843936920166\n",
      "step : 77.53 % , loss : 5.226382255554199\n",
      "step : 77.97 % , loss : 5.230685234069824\n",
      "step : 78.41 % , loss : 5.15286111831665\n",
      "step : 78.85 % , loss : 5.354093551635742\n",
      "step : 79.3 % , loss : 5.289289951324463\n",
      "step : 79.74 % , loss : 5.245160102844238\n",
      "step : 80.18 % , loss : 5.392612934112549\n",
      "step : 80.62 % , loss : 5.249342918395996\n",
      "step : 81.06 % , loss : 5.397706985473633\n",
      "step : 81.5 % , loss : 5.23294734954834\n",
      "step : 81.94 % , loss : 5.195931434631348\n",
      "step : 82.38 % , loss : 5.321905136108398\n",
      "step : 82.82 % , loss : 5.230830192565918\n",
      "step : 83.26 % , loss : 5.250274181365967\n",
      "step : 83.7 % , loss : 5.1790242195129395\n",
      "step : 84.14 % , loss : 5.275691032409668\n",
      "step : 84.58 % , loss : 5.234799385070801\n",
      "step : 85.02 % , loss : 5.358203411102295\n",
      "step : 85.46 % , loss : 5.303534984588623\n",
      "step : 85.9 % , loss : 5.138571739196777\n",
      "step : 86.34 % , loss : 5.178451061248779\n",
      "step : 86.78 % , loss : 5.31506872177124\n",
      "step : 87.22 % , loss : 5.268322467803955\n",
      "step : 87.67 % , loss : 5.297308921813965\n",
      "step : 88.11 % , loss : 5.326904296875\n",
      "step : 88.55 % , loss : 5.281512260437012\n",
      "step : 88.99 % , loss : 5.224820137023926\n",
      "step : 89.43 % , loss : 5.227988243103027\n",
      "step : 89.87 % , loss : 5.211953163146973\n",
      "step : 90.31 % , loss : 5.265298366546631\n",
      "step : 90.75 % , loss : 5.219302654266357\n",
      "step : 91.19 % , loss : 5.278340816497803\n",
      "step : 91.63 % , loss : 5.286620140075684\n",
      "step : 92.07 % , loss : 5.271084308624268\n",
      "step : 92.51 % , loss : 5.204629898071289\n",
      "step : 92.95 % , loss : 5.24389123916626\n",
      "step : 93.39 % , loss : 5.309059143066406\n",
      "step : 93.83 % , loss : 5.31557559967041\n",
      "step : 94.27 % , loss : 5.2002854347229\n",
      "step : 94.71 % , loss : 5.166040897369385\n",
      "step : 95.15 % , loss : 5.291563034057617\n",
      "step : 95.59 % , loss : 5.35603666305542\n",
      "step : 96.04 % , loss : 5.298487663269043\n",
      "step : 96.48 % , loss : 5.306289196014404\n",
      "step : 96.92 % , loss : 5.235611438751221\n",
      "step : 97.36 % , loss : 5.269444942474365\n",
      "step : 97.8 % , loss : 5.203303337097168\n",
      "step : 98.24 % , loss : 5.2131829261779785\n",
      "step : 98.68 % , loss : 5.233202934265137\n",
      "step : 99.12 % , loss : 5.107549667358398\n",
      "step : 99.56 % , loss : 5.222304821014404\n",
      "\tTrain Loss: 5.285\n",
      "step : 0.0 % , loss : 5.1809492111206055\n",
      "step : 0.44 % , loss : 5.230145454406738\n",
      "step : 0.88 % , loss : 5.244041442871094\n",
      "step : 1.32 % , loss : 5.245668411254883\n",
      "step : 1.76 % , loss : 5.243428707122803\n",
      "step : 2.2 % , loss : 5.247682094573975\n",
      "step : 2.64 % , loss : 5.150761127471924\n",
      "step : 3.08 % , loss : 5.35181188583374\n",
      "step : 3.52 % , loss : 5.142363548278809\n",
      "step : 3.96 % , loss : 5.221892356872559\n",
      "step : 4.41 % , loss : 5.218285083770752\n",
      "step : 4.85 % , loss : 5.278637409210205\n",
      "step : 5.29 % , loss : 5.1995439529418945\n",
      "step : 5.73 % , loss : 5.305691242218018\n",
      "step : 6.17 % , loss : 5.214061260223389\n",
      "step : 6.61 % , loss : 5.280766487121582\n",
      "step : 7.05 % , loss : 5.056924343109131\n",
      "step : 7.49 % , loss : 5.2180705070495605\n",
      "step : 7.93 % , loss : 5.303492546081543\n",
      "step : 8.37 % , loss : 5.348248481750488\n",
      "step : 8.81 % , loss : 5.230476379394531\n",
      "step : 9.25 % , loss : 5.115630149841309\n",
      "step : 9.69 % , loss : 5.33059549331665\n",
      "step : 10.13 % , loss : 5.201843738555908\n",
      "step : 10.57 % , loss : 5.318671226501465\n",
      "step : 11.01 % , loss : 5.193970203399658\n",
      "step : 11.45 % , loss : 5.239983081817627\n",
      "step : 11.89 % , loss : 5.1386308670043945\n",
      "step : 12.33 % , loss : 5.270455360412598\n",
      "step : 12.78 % , loss : 5.211619853973389\n",
      "step : 13.22 % , loss : 5.302441596984863\n",
      "step : 13.66 % , loss : 5.213296890258789\n",
      "step : 14.1 % , loss : 5.24385929107666\n",
      "step : 14.54 % , loss : 5.164756774902344\n",
      "step : 14.98 % , loss : 5.162911415100098\n",
      "step : 15.42 % , loss : 5.12727689743042\n",
      "step : 15.86 % , loss : 5.197368144989014\n",
      "step : 16.3 % , loss : 5.352128505706787\n",
      "step : 16.74 % , loss : 5.244109153747559\n",
      "step : 17.18 % , loss : 5.17516565322876\n",
      "step : 17.62 % , loss : 5.163387775421143\n",
      "step : 18.06 % , loss : 5.28694486618042\n",
      "step : 18.5 % , loss : 5.335010051727295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 18.94 % , loss : 5.158939838409424\n",
      "step : 19.38 % , loss : 5.358432292938232\n",
      "step : 19.82 % , loss : 5.158692359924316\n",
      "step : 20.26 % , loss : 5.1907806396484375\n",
      "step : 20.7 % , loss : 5.244421005249023\n",
      "step : 21.15 % , loss : 5.231164455413818\n",
      "step : 21.59 % , loss : 5.142582893371582\n",
      "step : 22.03 % , loss : 5.263952732086182\n",
      "step : 22.47 % , loss : 5.188472747802734\n",
      "step : 22.91 % , loss : 5.266220569610596\n",
      "step : 23.35 % , loss : 5.218045711517334\n",
      "step : 23.79 % , loss : 5.252967834472656\n",
      "step : 24.23 % , loss : 5.2272772789001465\n",
      "step : 24.67 % , loss : 5.218534469604492\n",
      "step : 25.11 % , loss : 5.339821815490723\n",
      "step : 25.55 % , loss : 5.270227432250977\n",
      "step : 25.99 % , loss : 5.2432756423950195\n",
      "step : 26.43 % , loss : 5.200843334197998\n",
      "step : 26.87 % , loss : 5.1929850578308105\n",
      "step : 27.31 % , loss : 5.210150718688965\n",
      "step : 27.75 % , loss : 5.213120937347412\n",
      "step : 28.19 % , loss : 5.2657270431518555\n",
      "step : 28.63 % , loss : 5.281888484954834\n",
      "step : 29.07 % , loss : 5.195516109466553\n",
      "step : 29.52 % , loss : 5.262600421905518\n",
      "step : 29.96 % , loss : 5.1722731590271\n",
      "step : 30.4 % , loss : 5.188141822814941\n",
      "step : 30.84 % , loss : 5.218212127685547\n",
      "step : 31.28 % , loss : 5.113547325134277\n",
      "step : 31.72 % , loss : 5.239253520965576\n",
      "step : 32.16 % , loss : 5.2293195724487305\n",
      "step : 32.6 % , loss : 5.193579196929932\n",
      "step : 33.04 % , loss : 5.209489345550537\n",
      "step : 33.48 % , loss : 5.218611717224121\n",
      "step : 33.92 % , loss : 5.142188549041748\n",
      "step : 34.36 % , loss : 5.271514415740967\n",
      "step : 34.8 % , loss : 5.299996376037598\n",
      "step : 35.24 % , loss : 5.2128472328186035\n",
      "step : 35.68 % , loss : 5.201869964599609\n",
      "step : 36.12 % , loss : 5.235571384429932\n",
      "step : 36.56 % , loss : 5.182688236236572\n",
      "step : 37.0 % , loss : 5.130285739898682\n",
      "step : 37.44 % , loss : 5.250348091125488\n",
      "step : 37.89 % , loss : 5.198201656341553\n",
      "step : 38.33 % , loss : 5.233473300933838\n",
      "step : 38.77 % , loss : 5.1580810546875\n",
      "step : 39.21 % , loss : 5.278369426727295\n",
      "step : 39.65 % , loss : 5.232629299163818\n",
      "step : 40.09 % , loss : 5.230685234069824\n",
      "step : 40.53 % , loss : 5.153060436248779\n",
      "step : 40.97 % , loss : 5.1747965812683105\n",
      "step : 41.41 % , loss : 5.195696830749512\n",
      "step : 41.85 % , loss : 5.1810173988342285\n",
      "step : 42.29 % , loss : 5.2405195236206055\n",
      "step : 42.73 % , loss : 5.315855026245117\n",
      "step : 43.17 % , loss : 5.277645111083984\n",
      "step : 43.61 % , loss : 5.320461750030518\n",
      "step : 44.05 % , loss : 5.259169101715088\n",
      "step : 44.49 % , loss : 5.201010704040527\n",
      "step : 44.93 % , loss : 5.155916213989258\n",
      "step : 45.37 % , loss : 5.302184581756592\n",
      "step : 45.81 % , loss : 5.279104232788086\n",
      "step : 46.26 % , loss : 5.035737991333008\n",
      "step : 46.7 % , loss : 5.260564804077148\n",
      "step : 47.14 % , loss : 5.287357807159424\n",
      "step : 47.58 % , loss : 5.203628063201904\n",
      "step : 48.02 % , loss : 5.234544277191162\n",
      "step : 48.46 % , loss : 5.264566898345947\n",
      "step : 48.9 % , loss : 5.214152812957764\n",
      "step : 49.34 % , loss : 5.152396202087402\n",
      "step : 49.78 % , loss : 5.297413349151611\n",
      "step : 50.22 % , loss : 5.301534175872803\n",
      "step : 50.66 % , loss : 5.185009956359863\n",
      "step : 51.1 % , loss : 5.242746829986572\n",
      "step : 51.54 % , loss : 5.228027820587158\n",
      "step : 51.98 % , loss : 5.241612434387207\n",
      "step : 52.42 % , loss : 5.095829963684082\n",
      "step : 52.86 % , loss : 5.188595771789551\n",
      "step : 53.3 % , loss : 5.221050262451172\n",
      "step : 53.74 % , loss : 5.25177001953125\n",
      "step : 54.19 % , loss : 5.159800052642822\n",
      "step : 54.63 % , loss : 5.183513164520264\n",
      "step : 55.07 % , loss : 5.196953296661377\n",
      "step : 55.51 % , loss : 5.192665100097656\n",
      "step : 55.95 % , loss : 5.178719520568848\n",
      "step : 56.39 % , loss : 5.3171586990356445\n",
      "step : 56.83 % , loss : 5.123482704162598\n",
      "step : 57.27 % , loss : 5.2148847579956055\n",
      "step : 57.71 % , loss : 5.140068531036377\n",
      "step : 58.15 % , loss : 5.30165433883667\n",
      "step : 58.59 % , loss : 5.201821804046631\n",
      "step : 59.03 % , loss : 5.205467224121094\n",
      "step : 59.47 % , loss : 5.057252883911133\n",
      "step : 59.91 % , loss : 5.184353351593018\n",
      "step : 60.35 % , loss : 5.213838577270508\n",
      "step : 60.79 % , loss : 5.231139659881592\n",
      "step : 61.23 % , loss : 5.149587631225586\n",
      "step : 61.67 % , loss : 5.270781517028809\n",
      "step : 62.11 % , loss : 5.2473907470703125\n",
      "step : 62.56 % , loss : 5.266263484954834\n",
      "step : 63.0 % , loss : 5.179255962371826\n",
      "step : 63.44 % , loss : 5.140214443206787\n",
      "step : 63.88 % , loss : 5.170026779174805\n",
      "step : 64.32 % , loss : 5.12807559967041\n",
      "step : 64.76 % , loss : 5.1389875411987305\n",
      "step : 65.2 % , loss : 5.331025123596191\n",
      "step : 65.64 % , loss : 5.148191452026367\n",
      "step : 66.08 % , loss : 5.286467552185059\n",
      "step : 66.52 % , loss : 5.257650852203369\n",
      "step : 66.96 % , loss : 5.172311305999756\n",
      "step : 67.4 % , loss : 5.17830753326416\n",
      "step : 67.84 % , loss : 5.233915328979492\n",
      "step : 68.28 % , loss : 5.29008150100708\n",
      "step : 68.72 % , loss : 5.207545280456543\n",
      "step : 69.16 % , loss : 5.042362213134766\n",
      "step : 69.6 % , loss : 5.158910274505615\n",
      "step : 70.04 % , loss : 5.226119041442871\n",
      "step : 70.48 % , loss : 5.347768306732178\n",
      "step : 70.93 % , loss : 5.243130207061768\n",
      "step : 71.37 % , loss : 5.272927284240723\n",
      "step : 71.81 % , loss : 5.119714736938477\n",
      "step : 72.25 % , loss : 5.150993824005127\n",
      "step : 72.69 % , loss : 5.252321243286133\n",
      "step : 73.13 % , loss : 5.191447734832764\n",
      "step : 73.57 % , loss : 5.201239585876465\n",
      "step : 74.01 % , loss : 5.1287994384765625\n",
      "step : 74.45 % , loss : 5.163501739501953\n",
      "step : 74.89 % , loss : 5.2271728515625\n",
      "step : 75.33 % , loss : 5.276154518127441\n",
      "step : 75.77 % , loss : 5.281340599060059\n",
      "step : 76.21 % , loss : 5.215023040771484\n",
      "step : 76.65 % , loss : 5.1538472175598145\n",
      "step : 77.09 % , loss : 5.129998683929443\n",
      "step : 77.53 % , loss : 5.197473526000977\n",
      "step : 77.97 % , loss : 5.2056050300598145\n",
      "step : 78.41 % , loss : 5.1259918212890625\n",
      "step : 78.85 % , loss : 5.130277156829834\n",
      "step : 79.3 % , loss : 5.177855491638184\n",
      "step : 79.74 % , loss : 5.15676736831665\n",
      "step : 80.18 % , loss : 5.173059463500977\n",
      "step : 80.62 % , loss : 5.152719974517822\n",
      "step : 81.06 % , loss : 5.114174842834473\n",
      "step : 81.5 % , loss : 5.146410942077637\n",
      "step : 81.94 % , loss : 5.131909370422363\n",
      "step : 82.38 % , loss : 5.3024749755859375\n",
      "step : 82.82 % , loss : 5.151689529418945\n",
      "step : 83.26 % , loss : 5.259708404541016\n",
      "step : 83.7 % , loss : 5.237575054168701\n",
      "step : 84.14 % , loss : 5.109344482421875\n",
      "step : 84.58 % , loss : 5.245573997497559\n",
      "step : 85.02 % , loss : 5.1069560050964355\n",
      "step : 85.46 % , loss : 5.2466840744018555\n",
      "step : 85.9 % , loss : 5.236937999725342\n",
      "step : 86.34 % , loss : 5.2795939445495605\n",
      "step : 86.78 % , loss : 5.177576065063477\n",
      "step : 87.22 % , loss : 5.08965539932251\n",
      "step : 87.67 % , loss : 5.193865776062012\n",
      "step : 88.11 % , loss : 5.343402862548828\n",
      "step : 88.55 % , loss : 5.128193378448486\n",
      "step : 88.99 % , loss : 5.1055588722229\n",
      "step : 89.43 % , loss : 5.113331317901611\n",
      "step : 89.87 % , loss : 5.159321308135986\n",
      "step : 90.31 % , loss : 5.209136009216309\n",
      "step : 90.75 % , loss : 5.210395336151123\n",
      "step : 91.19 % , loss : 5.17110538482666\n",
      "step : 91.63 % , loss : 5.314505100250244\n",
      "step : 92.07 % , loss : 5.1700615882873535\n",
      "step : 92.51 % , loss : 5.225318908691406\n",
      "step : 92.95 % , loss : 5.125299453735352\n",
      "step : 93.39 % , loss : 5.119765758514404\n",
      "step : 93.83 % , loss : 5.084371089935303\n",
      "step : 94.27 % , loss : 5.168735504150391\n",
      "step : 94.71 % , loss : 5.155642509460449\n",
      "step : 95.15 % , loss : 5.183420181274414\n",
      "step : 95.59 % , loss : 5.154573917388916\n",
      "step : 96.04 % , loss : 5.200007915496826\n",
      "step : 96.48 % , loss : 5.213097095489502\n",
      "step : 96.92 % , loss : 5.139932632446289\n",
      "step : 97.36 % , loss : 4.9940595626831055\n",
      "step : 97.8 % , loss : 5.2182698249816895\n",
      "step : 98.24 % , loss : 5.219427585601807\n",
      "step : 98.68 % , loss : 5.193819999694824\n",
      "step : 99.12 % , loss : 5.156835556030273\n",
      "step : 99.56 % , loss : 5.115151405334473\n",
      "\tTrain Loss: 5.208\n",
      "step : 0.0 % , loss : 5.172461986541748\n",
      "step : 0.44 % , loss : 5.283830642700195\n",
      "step : 0.88 % , loss : 5.170579433441162\n",
      "step : 1.32 % , loss : 5.309689044952393\n",
      "step : 1.76 % , loss : 5.242309093475342\n",
      "step : 2.2 % , loss : 5.074153900146484\n",
      "step : 2.64 % , loss : 5.200972080230713\n",
      "step : 3.08 % , loss : 5.174725532531738\n",
      "step : 3.52 % , loss : 5.226597309112549\n",
      "step : 3.96 % , loss : 5.245709419250488\n",
      "step : 4.41 % , loss : 5.259316444396973\n",
      "step : 4.85 % , loss : 5.299508571624756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 5.29 % , loss : 5.187375545501709\n",
      "step : 5.73 % , loss : 5.162508010864258\n",
      "step : 6.17 % , loss : 5.3548970222473145\n",
      "step : 6.61 % , loss : 5.191980838775635\n",
      "step : 7.05 % , loss : 5.214736461639404\n",
      "step : 7.49 % , loss : 5.120456695556641\n",
      "step : 7.93 % , loss : 5.176262855529785\n",
      "step : 8.37 % , loss : 5.2109527587890625\n",
      "step : 8.81 % , loss : 5.306822776794434\n",
      "step : 9.25 % , loss : 5.236618518829346\n",
      "step : 9.69 % , loss : 5.183802127838135\n",
      "step : 10.13 % , loss : 5.094473838806152\n",
      "step : 10.57 % , loss : 5.247542381286621\n",
      "step : 11.01 % , loss : 5.151181697845459\n",
      "step : 11.45 % , loss : 5.140389442443848\n",
      "step : 11.89 % , loss : 5.096981048583984\n",
      "step : 12.33 % , loss : 5.1802077293396\n",
      "step : 12.78 % , loss : 5.061221122741699\n",
      "step : 13.22 % , loss : 5.192532062530518\n",
      "step : 13.66 % , loss : 5.150017738342285\n",
      "step : 14.1 % , loss : 5.155636787414551\n",
      "step : 14.54 % , loss : 5.288022518157959\n",
      "step : 14.98 % , loss : 5.17812967300415\n",
      "step : 15.42 % , loss : 5.2014007568359375\n",
      "step : 15.86 % , loss : 5.125970363616943\n",
      "step : 16.3 % , loss : 5.205753803253174\n",
      "step : 16.74 % , loss : 5.225728988647461\n",
      "step : 17.18 % , loss : 5.188635349273682\n",
      "step : 17.62 % , loss : 5.15896463394165\n",
      "step : 18.06 % , loss : 5.2140092849731445\n",
      "step : 18.5 % , loss : 5.127728462219238\n",
      "step : 18.94 % , loss : 5.075226783752441\n",
      "step : 19.38 % , loss : 5.235450744628906\n",
      "step : 19.82 % , loss : 5.142132759094238\n",
      "step : 20.26 % , loss : 5.1009721755981445\n",
      "step : 20.7 % , loss : 5.247519493103027\n",
      "step : 21.15 % , loss : 5.231040954589844\n",
      "step : 21.59 % , loss : 5.156188011169434\n",
      "step : 22.03 % , loss : 5.152946472167969\n",
      "step : 22.47 % , loss : 5.114063262939453\n",
      "step : 22.91 % , loss : 5.198204040527344\n",
      "step : 23.35 % , loss : 5.2428436279296875\n",
      "step : 23.79 % , loss : 5.175971508026123\n",
      "step : 24.23 % , loss : 5.169890403747559\n",
      "step : 24.67 % , loss : 5.098085403442383\n",
      "step : 25.11 % , loss : 5.122299671173096\n",
      "step : 25.55 % , loss : 5.158833980560303\n",
      "step : 25.99 % , loss : 5.111762523651123\n",
      "step : 26.43 % , loss : 5.171414375305176\n",
      "step : 26.87 % , loss : 5.172366142272949\n",
      "step : 27.31 % , loss : 5.302883148193359\n",
      "step : 27.75 % , loss : 5.163512706756592\n",
      "step : 28.19 % , loss : 5.186756134033203\n",
      "step : 28.63 % , loss : 5.212594985961914\n",
      "step : 29.07 % , loss : 5.210728645324707\n",
      "step : 29.52 % , loss : 5.0897369384765625\n",
      "step : 29.96 % , loss : 5.092670440673828\n",
      "step : 30.4 % , loss : 5.244474411010742\n",
      "step : 30.84 % , loss : 5.075138568878174\n",
      "step : 31.28 % , loss : 5.144711494445801\n",
      "step : 31.72 % , loss : 5.184197425842285\n",
      "step : 32.16 % , loss : 5.148825645446777\n",
      "step : 32.6 % , loss : 5.18499755859375\n",
      "step : 33.04 % , loss : 5.12959623336792\n",
      "step : 33.48 % , loss : 5.204039096832275\n",
      "step : 33.92 % , loss : 5.0757222175598145\n",
      "step : 34.36 % , loss : 5.174038410186768\n",
      "step : 34.8 % , loss : 5.218364715576172\n",
      "step : 35.24 % , loss : 5.071004390716553\n",
      "step : 35.68 % , loss : 5.145327568054199\n",
      "step : 36.12 % , loss : 5.122879505157471\n",
      "step : 36.56 % , loss : 5.124634742736816\n",
      "step : 37.0 % , loss : 5.0565900802612305\n",
      "step : 37.44 % , loss : 5.08695650100708\n",
      "step : 37.89 % , loss : 5.230541706085205\n",
      "step : 38.33 % , loss : 5.069483757019043\n",
      "step : 38.77 % , loss : 5.125471115112305\n",
      "step : 39.21 % , loss : 5.128271579742432\n",
      "step : 39.65 % , loss : 5.246469974517822\n",
      "step : 40.09 % , loss : 5.183003902435303\n",
      "step : 40.53 % , loss : 5.174501895904541\n",
      "step : 40.97 % , loss : 5.244411945343018\n",
      "step : 41.41 % , loss : 5.15572452545166\n",
      "step : 41.85 % , loss : 5.025761127471924\n",
      "step : 42.29 % , loss : 5.14261531829834\n",
      "step : 42.73 % , loss : 5.229191780090332\n",
      "step : 43.17 % , loss : 4.976160049438477\n",
      "step : 43.61 % , loss : 5.294588565826416\n",
      "step : 44.05 % , loss : 5.3135857582092285\n",
      "step : 44.49 % , loss : 5.225423336029053\n",
      "step : 44.93 % , loss : 5.111250400543213\n",
      "step : 45.37 % , loss : 5.023077487945557\n",
      "step : 45.81 % , loss : 5.151961803436279\n",
      "step : 46.26 % , loss : 5.129757404327393\n",
      "step : 46.7 % , loss : 5.213259696960449\n",
      "step : 47.14 % , loss : 5.145238399505615\n",
      "step : 47.58 % , loss : 5.235112190246582\n",
      "step : 48.02 % , loss : 5.193825721740723\n",
      "step : 48.46 % , loss : 5.128507137298584\n",
      "step : 48.9 % , loss : 5.208185195922852\n",
      "step : 49.34 % , loss : 5.233801364898682\n",
      "step : 49.78 % , loss : 5.0925703048706055\n",
      "step : 50.22 % , loss : 5.136706829071045\n",
      "step : 50.66 % , loss : 5.09152364730835\n",
      "step : 51.1 % , loss : 5.165794849395752\n",
      "step : 51.54 % , loss : 5.112926959991455\n",
      "step : 51.98 % , loss : 5.2283935546875\n",
      "step : 52.42 % , loss : 5.188050746917725\n",
      "step : 52.86 % , loss : 5.106558799743652\n",
      "step : 53.3 % , loss : 5.141892910003662\n",
      "step : 53.74 % , loss : 5.022521018981934\n",
      "step : 54.19 % , loss : 5.1244306564331055\n",
      "step : 54.63 % , loss : 5.1500115394592285\n",
      "step : 55.07 % , loss : 5.155988693237305\n",
      "step : 55.51 % , loss : 5.308873176574707\n",
      "step : 55.95 % , loss : 5.098053932189941\n",
      "step : 56.39 % , loss : 5.194139003753662\n",
      "step : 56.83 % , loss : 5.171772003173828\n",
      "step : 57.27 % , loss : 5.168460369110107\n",
      "step : 57.71 % , loss : 5.089447975158691\n",
      "step : 58.15 % , loss : 5.160158634185791\n",
      "step : 58.59 % , loss : 5.149538993835449\n",
      "step : 59.03 % , loss : 5.05613899230957\n",
      "step : 59.47 % , loss : 5.128178596496582\n",
      "step : 59.91 % , loss : 5.1352219581604\n",
      "step : 60.35 % , loss : 5.224209785461426\n",
      "step : 60.79 % , loss : 5.164778709411621\n",
      "step : 61.23 % , loss : 5.151934623718262\n",
      "step : 61.67 % , loss : 5.249436855316162\n",
      "step : 62.11 % , loss : 5.152594566345215\n",
      "step : 62.56 % , loss : 5.158985614776611\n",
      "step : 63.0 % , loss : 5.019818305969238\n",
      "step : 63.44 % , loss : 5.231858730316162\n",
      "step : 63.88 % , loss : 5.159843921661377\n",
      "step : 64.32 % , loss : 5.177962303161621\n",
      "step : 64.76 % , loss : 5.15238094329834\n",
      "step : 65.2 % , loss : 5.118875503540039\n",
      "step : 65.64 % , loss : 5.0927886962890625\n",
      "step : 66.08 % , loss : 5.203160285949707\n",
      "step : 66.52 % , loss : 5.211645603179932\n",
      "step : 66.96 % , loss : 5.04405403137207\n",
      "step : 67.4 % , loss : 5.207596302032471\n",
      "step : 67.84 % , loss : 5.157664775848389\n",
      "step : 68.28 % , loss : 5.076386451721191\n",
      "step : 68.72 % , loss : 5.168598175048828\n",
      "step : 69.16 % , loss : 5.20574426651001\n",
      "step : 69.6 % , loss : 5.077023983001709\n",
      "step : 70.04 % , loss : 5.1300177574157715\n",
      "step : 70.48 % , loss : 5.182762145996094\n",
      "step : 70.93 % , loss : 5.14718770980835\n",
      "step : 71.37 % , loss : 5.075619697570801\n",
      "step : 71.81 % , loss : 5.125607967376709\n",
      "step : 72.25 % , loss : 5.1129231452941895\n",
      "step : 72.69 % , loss : 5.2423295974731445\n",
      "step : 73.13 % , loss : 5.129801273345947\n",
      "step : 73.57 % , loss : 5.336822986602783\n",
      "step : 74.01 % , loss : 5.139930248260498\n",
      "step : 74.45 % , loss : 5.2064924240112305\n",
      "step : 74.89 % , loss : 4.965155601501465\n",
      "step : 75.33 % , loss : 5.17588472366333\n",
      "step : 75.77 % , loss : 5.116367340087891\n",
      "step : 76.21 % , loss : 5.235317707061768\n",
      "step : 76.65 % , loss : 5.084589004516602\n",
      "step : 77.09 % , loss : 5.138710021972656\n",
      "step : 77.53 % , loss : 5.246376037597656\n",
      "step : 77.97 % , loss : 5.156999111175537\n",
      "step : 78.41 % , loss : 5.156906604766846\n",
      "step : 78.85 % , loss : 5.278534412384033\n",
      "step : 79.3 % , loss : 5.089393615722656\n",
      "step : 79.74 % , loss : 5.10792875289917\n",
      "step : 80.18 % , loss : 5.16725492477417\n",
      "step : 80.62 % , loss : 5.111750602722168\n",
      "step : 81.06 % , loss : 5.131537914276123\n",
      "step : 81.5 % , loss : 5.162932395935059\n",
      "step : 81.94 % , loss : 5.186540603637695\n",
      "step : 82.38 % , loss : 5.081581115722656\n",
      "step : 82.82 % , loss : 5.135844707489014\n",
      "step : 83.26 % , loss : 5.242560386657715\n",
      "step : 83.7 % , loss : 5.189353942871094\n",
      "step : 84.14 % , loss : 5.053579807281494\n",
      "step : 84.58 % , loss : 5.089413642883301\n",
      "step : 85.02 % , loss : 5.230100631713867\n",
      "step : 85.46 % , loss : 5.132830619812012\n",
      "step : 85.9 % , loss : 5.123198986053467\n",
      "step : 86.34 % , loss : 5.222769260406494\n",
      "step : 86.78 % , loss : 5.186132907867432\n",
      "step : 87.22 % , loss : 5.161012649536133\n",
      "step : 87.67 % , loss : 5.1619391441345215\n",
      "step : 88.11 % , loss : 5.173498153686523\n",
      "step : 88.55 % , loss : 5.112693786621094\n",
      "step : 88.99 % , loss : 5.2036919593811035\n",
      "step : 89.43 % , loss : 5.116779804229736\n",
      "step : 89.87 % , loss : 5.241250514984131\n",
      "step : 90.31 % , loss : 5.155076026916504\n",
      "step : 90.75 % , loss : 5.146004676818848\n",
      "step : 91.19 % , loss : 5.2436652183532715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 91.63 % , loss : 5.239532947540283\n",
      "step : 92.07 % , loss : 5.145934104919434\n",
      "step : 92.51 % , loss : 5.020941257476807\n",
      "step : 92.95 % , loss : 5.181087493896484\n",
      "step : 93.39 % , loss : 5.068437099456787\n",
      "step : 93.83 % , loss : 5.110363960266113\n",
      "step : 94.27 % , loss : 5.117898464202881\n",
      "step : 94.71 % , loss : 5.110725402832031\n",
      "step : 95.15 % , loss : 5.181407451629639\n",
      "step : 95.59 % , loss : 5.207895755767822\n",
      "step : 96.04 % , loss : 5.08170223236084\n",
      "step : 96.48 % , loss : 5.046642303466797\n",
      "step : 96.92 % , loss : 5.182994842529297\n",
      "step : 97.36 % , loss : 5.002692222595215\n",
      "step : 97.8 % , loss : 5.206475734710693\n",
      "step : 98.24 % , loss : 5.192604064941406\n",
      "step : 98.68 % , loss : 5.045716762542725\n",
      "step : 99.12 % , loss : 5.13408088684082\n",
      "step : 99.56 % , loss : 5.199545383453369\n",
      "\tTrain Loss: 5.162\n",
      "step : 0.0 % , loss : 5.093535423278809\n",
      "step : 0.44 % , loss : 5.103537559509277\n",
      "step : 0.88 % , loss : 5.135608673095703\n",
      "step : 1.32 % , loss : 5.095526218414307\n",
      "step : 1.76 % , loss : 5.191866397857666\n",
      "step : 2.2 % , loss : 5.135372161865234\n",
      "step : 2.64 % , loss : 5.142705917358398\n",
      "step : 3.08 % , loss : 5.124095439910889\n",
      "step : 3.52 % , loss : 5.3146185874938965\n",
      "step : 3.96 % , loss : 5.120635032653809\n",
      "step : 4.41 % , loss : 5.128622531890869\n",
      "step : 4.85 % , loss : 5.184118747711182\n",
      "step : 5.29 % , loss : 5.099843978881836\n",
      "step : 5.73 % , loss : 5.100653171539307\n",
      "step : 6.17 % , loss : 5.204407691955566\n",
      "step : 6.61 % , loss : 5.15164852142334\n",
      "step : 7.05 % , loss : 5.0098795890808105\n",
      "step : 7.49 % , loss : 5.181144714355469\n",
      "step : 7.93 % , loss : 5.153154373168945\n",
      "step : 8.37 % , loss : 5.14763879776001\n",
      "step : 8.81 % , loss : 5.059229373931885\n",
      "step : 9.25 % , loss : 5.148881912231445\n",
      "step : 9.69 % , loss : 5.174117088317871\n",
      "step : 10.13 % , loss : 5.148855209350586\n",
      "step : 10.57 % , loss : 5.068999767303467\n",
      "step : 11.01 % , loss : 5.154860019683838\n",
      "step : 11.45 % , loss : 5.083181858062744\n",
      "step : 11.89 % , loss : 5.084040641784668\n",
      "step : 12.33 % , loss : 5.174038887023926\n",
      "step : 12.78 % , loss : 5.251092910766602\n",
      "step : 13.22 % , loss : 5.164462566375732\n",
      "step : 13.66 % , loss : 5.1814398765563965\n",
      "step : 14.1 % , loss : 5.175525188446045\n",
      "step : 14.54 % , loss : 5.114190101623535\n",
      "step : 14.98 % , loss : 5.1482672691345215\n",
      "step : 15.42 % , loss : 5.163217544555664\n",
      "step : 15.86 % , loss : 5.059617519378662\n",
      "step : 16.3 % , loss : 5.142410755157471\n",
      "step : 16.74 % , loss : 5.170794486999512\n",
      "step : 17.18 % , loss : 5.248239517211914\n",
      "step : 17.62 % , loss : 5.106451034545898\n",
      "step : 18.06 % , loss : 5.146742820739746\n",
      "step : 18.5 % , loss : 5.096606731414795\n",
      "step : 18.94 % , loss : 5.173604965209961\n",
      "step : 19.38 % , loss : 5.235894680023193\n",
      "step : 19.82 % , loss : 5.129676342010498\n",
      "step : 20.26 % , loss : 5.081935405731201\n",
      "step : 20.7 % , loss : 5.181270122528076\n",
      "step : 21.15 % , loss : 5.071145534515381\n",
      "step : 21.59 % , loss : 5.043483734130859\n",
      "step : 22.03 % , loss : 5.160906791687012\n",
      "step : 22.47 % , loss : 5.128317832946777\n",
      "step : 22.91 % , loss : 5.114871978759766\n",
      "step : 23.35 % , loss : 5.079998016357422\n",
      "step : 23.79 % , loss : 5.094032287597656\n",
      "step : 24.23 % , loss : 5.079524040222168\n",
      "step : 24.67 % , loss : 5.138266086578369\n",
      "step : 25.11 % , loss : 5.200562953948975\n",
      "step : 25.55 % , loss : 5.138731479644775\n",
      "step : 25.99 % , loss : 5.14421272277832\n",
      "step : 26.43 % , loss : 5.16502571105957\n",
      "step : 26.87 % , loss : 5.205068588256836\n",
      "step : 27.31 % , loss : 5.186002731323242\n",
      "step : 27.75 % , loss : 5.145056247711182\n",
      "step : 28.19 % , loss : 5.1109137535095215\n",
      "step : 28.63 % , loss : 5.177276134490967\n",
      "step : 29.07 % , loss : 5.150934219360352\n",
      "step : 29.52 % , loss : 5.065483093261719\n",
      "step : 29.96 % , loss : 5.132083415985107\n",
      "step : 30.4 % , loss : 5.1578850746154785\n",
      "step : 30.84 % , loss : 5.115818977355957\n",
      "step : 31.28 % , loss : 5.04218053817749\n",
      "step : 31.72 % , loss : 5.214318752288818\n",
      "step : 32.16 % , loss : 5.1848554611206055\n",
      "step : 32.6 % , loss : 5.149168491363525\n",
      "step : 33.04 % , loss : 5.145501136779785\n",
      "step : 33.48 % , loss : 5.19953727722168\n",
      "step : 33.92 % , loss : 5.16731071472168\n",
      "step : 34.36 % , loss : 5.171498775482178\n",
      "step : 34.8 % , loss : 5.216115951538086\n",
      "step : 35.24 % , loss : 5.120710849761963\n",
      "step : 35.68 % , loss : 5.106612205505371\n",
      "step : 36.12 % , loss : 5.110918045043945\n",
      "step : 36.56 % , loss : 5.129845142364502\n",
      "step : 37.0 % , loss : 5.203629970550537\n",
      "step : 37.44 % , loss : 5.1413726806640625\n",
      "step : 37.89 % , loss : 5.122034549713135\n",
      "step : 38.33 % , loss : 4.98974084854126\n",
      "step : 38.77 % , loss : 5.16868257522583\n",
      "step : 39.21 % , loss : 5.160190582275391\n",
      "step : 39.65 % , loss : 5.074493885040283\n",
      "step : 40.09 % , loss : 5.1022491455078125\n",
      "step : 40.53 % , loss : 5.0640130043029785\n",
      "step : 40.97 % , loss : 5.196007251739502\n",
      "step : 41.41 % , loss : 5.060926914215088\n",
      "step : 41.85 % , loss : 5.116072654724121\n",
      "step : 42.29 % , loss : 5.131157398223877\n",
      "step : 42.73 % , loss : 5.112074375152588\n",
      "step : 43.17 % , loss : 5.059514999389648\n",
      "step : 43.61 % , loss : 5.0889692306518555\n",
      "step : 44.05 % , loss : 5.174520969390869\n",
      "step : 44.49 % , loss : 5.1626787185668945\n",
      "step : 44.93 % , loss : 5.150357246398926\n",
      "step : 45.37 % , loss : 5.039914608001709\n",
      "step : 45.81 % , loss : 5.122819423675537\n",
      "step : 46.26 % , loss : 5.095536708831787\n",
      "step : 46.7 % , loss : 5.074813365936279\n",
      "step : 47.14 % , loss : 5.0862579345703125\n",
      "step : 47.58 % , loss : 5.061093330383301\n",
      "step : 48.02 % , loss : 5.133424282073975\n",
      "step : 48.46 % , loss : 5.2020673751831055\n",
      "step : 48.9 % , loss : 5.159045696258545\n",
      "step : 49.34 % , loss : 5.145462989807129\n",
      "step : 49.78 % , loss : 5.191018581390381\n",
      "step : 50.22 % , loss : 5.106851100921631\n",
      "step : 50.66 % , loss : 5.028594970703125\n",
      "step : 51.1 % , loss : 5.152288436889648\n",
      "step : 51.54 % , loss : 5.1144256591796875\n",
      "step : 51.98 % , loss : 5.162441730499268\n",
      "step : 52.42 % , loss : 5.1409711837768555\n",
      "step : 52.86 % , loss : 5.143469333648682\n",
      "step : 53.3 % , loss : 5.066858768463135\n",
      "step : 53.74 % , loss : 5.031078815460205\n",
      "step : 54.19 % , loss : 4.998877048492432\n",
      "step : 54.63 % , loss : 5.115987777709961\n",
      "step : 55.07 % , loss : 5.156876564025879\n",
      "step : 55.51 % , loss : 5.038849353790283\n",
      "step : 55.95 % , loss : 5.190539360046387\n",
      "step : 56.39 % , loss : 5.203204154968262\n",
      "step : 56.83 % , loss : 5.029141902923584\n",
      "step : 57.27 % , loss : 5.220376491546631\n",
      "step : 57.71 % , loss : 5.131292819976807\n",
      "step : 58.15 % , loss : 5.108928203582764\n",
      "step : 58.59 % , loss : 5.206732749938965\n",
      "step : 59.03 % , loss : 5.1596574783325195\n",
      "step : 59.47 % , loss : 5.133632659912109\n",
      "step : 59.91 % , loss : 5.142019271850586\n",
      "step : 60.35 % , loss : 5.172752857208252\n",
      "step : 60.79 % , loss : 5.095272064208984\n",
      "step : 61.23 % , loss : 4.982171535491943\n",
      "step : 61.67 % , loss : 5.035562992095947\n",
      "step : 62.11 % , loss : 5.164812088012695\n",
      "step : 62.56 % , loss : 5.120272636413574\n",
      "step : 63.0 % , loss : 5.100132942199707\n",
      "step : 63.44 % , loss : 5.094311714172363\n",
      "step : 63.88 % , loss : 5.101020336151123\n",
      "step : 64.32 % , loss : 5.191462993621826\n",
      "step : 64.76 % , loss : 5.051459789276123\n",
      "step : 65.2 % , loss : 5.225147724151611\n",
      "step : 65.64 % , loss : 5.056304931640625\n",
      "step : 66.08 % , loss : 5.070041179656982\n",
      "step : 66.52 % , loss : 5.145747184753418\n",
      "step : 66.96 % , loss : 5.123553276062012\n",
      "step : 67.4 % , loss : 5.141692161560059\n",
      "step : 67.84 % , loss : 5.1193742752075195\n",
      "step : 68.28 % , loss : 5.137662410736084\n",
      "step : 68.72 % , loss : 4.991693019866943\n",
      "step : 69.16 % , loss : 5.13580846786499\n",
      "step : 69.6 % , loss : 5.060438632965088\n",
      "step : 70.04 % , loss : 5.13939905166626\n",
      "step : 70.48 % , loss : 5.090296268463135\n",
      "step : 70.93 % , loss : 5.155773162841797\n",
      "step : 71.37 % , loss : 5.03215217590332\n",
      "step : 71.81 % , loss : 5.049652576446533\n",
      "step : 72.25 % , loss : 5.097546577453613\n",
      "step : 72.69 % , loss : 5.042670249938965\n",
      "step : 73.13 % , loss : 5.107300758361816\n",
      "step : 73.57 % , loss : 5.126360893249512\n",
      "step : 74.01 % , loss : 5.070888042449951\n",
      "step : 74.45 % , loss : 5.039069652557373\n",
      "step : 74.89 % , loss : 5.1811323165893555\n",
      "step : 75.33 % , loss : 5.0953369140625\n",
      "step : 75.77 % , loss : 5.188100814819336\n",
      "step : 76.21 % , loss : 5.047241687774658\n",
      "step : 76.65 % , loss : 5.114565372467041\n",
      "step : 77.09 % , loss : 5.134521961212158\n",
      "step : 77.53 % , loss : 5.132933616638184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 77.97 % , loss : 5.079910755157471\n",
      "step : 78.41 % , loss : 5.087642669677734\n",
      "step : 78.85 % , loss : 5.166558742523193\n",
      "step : 79.3 % , loss : 5.1417131423950195\n",
      "step : 79.74 % , loss : 5.158013343811035\n",
      "step : 80.18 % , loss : 5.211662292480469\n",
      "step : 80.62 % , loss : 5.063708305358887\n",
      "step : 81.06 % , loss : 5.118456840515137\n",
      "step : 81.5 % , loss : 5.041408061981201\n",
      "step : 81.94 % , loss : 5.118243217468262\n",
      "step : 82.38 % , loss : 5.153997421264648\n",
      "step : 82.82 % , loss : 5.101075649261475\n",
      "step : 83.26 % , loss : 5.054316997528076\n",
      "step : 83.7 % , loss : 5.075475692749023\n",
      "step : 84.14 % , loss : 5.0831379890441895\n",
      "step : 84.58 % , loss : 5.110898494720459\n",
      "step : 85.02 % , loss : 5.143378734588623\n",
      "step : 85.46 % , loss : 5.077622413635254\n",
      "step : 85.9 % , loss : 5.021984100341797\n",
      "step : 86.34 % , loss : 5.0796074867248535\n",
      "step : 86.78 % , loss : 5.27147912979126\n",
      "step : 87.22 % , loss : 5.009756088256836\n",
      "step : 87.67 % , loss : 5.12315034866333\n",
      "step : 88.11 % , loss : 5.094592571258545\n",
      "step : 88.55 % , loss : 5.079758167266846\n",
      "step : 88.99 % , loss : 5.125948905944824\n",
      "step : 89.43 % , loss : 5.09162712097168\n",
      "step : 89.87 % , loss : 5.094118118286133\n",
      "step : 90.31 % , loss : 5.080230236053467\n",
      "step : 90.75 % , loss : 5.046141147613525\n",
      "step : 91.19 % , loss : 5.056395530700684\n",
      "step : 91.63 % , loss : 5.049505710601807\n",
      "step : 92.07 % , loss : 5.1835126876831055\n",
      "step : 92.51 % , loss : 5.1199750900268555\n",
      "step : 92.95 % , loss : 5.113389015197754\n",
      "step : 93.39 % , loss : 5.191827774047852\n",
      "step : 93.83 % , loss : 5.123724460601807\n",
      "step : 94.27 % , loss : 5.1147780418396\n",
      "step : 94.71 % , loss : 5.141574382781982\n",
      "step : 95.15 % , loss : 5.110187530517578\n",
      "step : 95.59 % , loss : 5.07777214050293\n",
      "step : 96.04 % , loss : 5.222055912017822\n",
      "step : 96.48 % , loss : 5.095934867858887\n",
      "step : 96.92 % , loss : 5.206323623657227\n",
      "step : 97.36 % , loss : 5.0345683097839355\n",
      "step : 97.8 % , loss : 5.113818645477295\n",
      "step : 98.24 % , loss : 5.003719806671143\n",
      "step : 98.68 % , loss : 5.162744045257568\n",
      "step : 99.12 % , loss : 5.174790382385254\n",
      "step : 99.56 % , loss : 5.234536647796631\n",
      "\tTrain Loss: 5.123\n",
      "step : 0.0 % , loss : 5.121084690093994\n",
      "step : 0.44 % , loss : 5.116342067718506\n",
      "step : 0.88 % , loss : 5.173223972320557\n",
      "step : 1.32 % , loss : 5.226517677307129\n",
      "step : 1.76 % , loss : 5.078863143920898\n",
      "step : 2.2 % , loss : 5.035643577575684\n",
      "step : 2.64 % , loss : 5.069269180297852\n",
      "step : 3.08 % , loss : 5.067410469055176\n",
      "step : 3.52 % , loss : 4.923215866088867\n",
      "step : 3.96 % , loss : 5.128934860229492\n",
      "step : 4.41 % , loss : 5.005648136138916\n",
      "step : 4.85 % , loss : 5.102042198181152\n",
      "step : 5.29 % , loss : 5.190845489501953\n",
      "step : 5.73 % , loss : 5.078014373779297\n",
      "step : 6.17 % , loss : 5.204661846160889\n",
      "step : 6.61 % , loss : 5.059895038604736\n",
      "step : 7.05 % , loss : 5.091240882873535\n",
      "step : 7.49 % , loss : 5.258559226989746\n",
      "step : 7.93 % , loss : 5.149115562438965\n",
      "step : 8.37 % , loss : 5.103527545928955\n",
      "step : 8.81 % , loss : 5.064237117767334\n",
      "step : 9.25 % , loss : 5.091083526611328\n",
      "step : 9.69 % , loss : 5.179976463317871\n",
      "step : 10.13 % , loss : 5.160479545593262\n",
      "step : 10.57 % , loss : 5.09255313873291\n",
      "step : 11.01 % , loss : 5.151471138000488\n",
      "step : 11.45 % , loss : 5.011188983917236\n",
      "step : 11.89 % , loss : 5.169000148773193\n",
      "step : 12.33 % , loss : 5.050854682922363\n",
      "step : 12.78 % , loss : 5.127455711364746\n",
      "step : 13.22 % , loss : 5.080104351043701\n",
      "step : 13.66 % , loss : 5.114570617675781\n",
      "step : 14.1 % , loss : 5.138481616973877\n",
      "step : 14.54 % , loss : 4.943155288696289\n",
      "step : 14.98 % , loss : 5.165369033813477\n",
      "step : 15.42 % , loss : 5.087325572967529\n",
      "step : 15.86 % , loss : 5.130577087402344\n",
      "step : 16.3 % , loss : 5.145603656768799\n",
      "step : 16.74 % , loss : 5.125463008880615\n",
      "step : 17.18 % , loss : 5.065915584564209\n",
      "step : 17.62 % , loss : 5.214354515075684\n",
      "step : 18.06 % , loss : 5.090452671051025\n",
      "step : 18.5 % , loss : 5.0947160720825195\n",
      "step : 18.94 % , loss : 5.103165626525879\n",
      "step : 19.38 % , loss : 4.999631881713867\n",
      "step : 19.82 % , loss : 5.142252445220947\n",
      "step : 20.26 % , loss : 5.113516807556152\n",
      "step : 20.7 % , loss : 4.992771625518799\n",
      "step : 21.15 % , loss : 5.093542575836182\n",
      "step : 21.59 % , loss : 5.143532752990723\n",
      "step : 22.03 % , loss : 5.053309440612793\n",
      "step : 22.47 % , loss : 5.144535541534424\n",
      "step : 22.91 % , loss : 4.999727249145508\n",
      "step : 23.35 % , loss : 5.0575480461120605\n",
      "step : 23.79 % , loss : 5.162677764892578\n",
      "step : 24.23 % , loss : 5.11909818649292\n",
      "step : 24.67 % , loss : 5.077571868896484\n",
      "step : 25.11 % , loss : 5.134596824645996\n",
      "step : 25.55 % , loss : 5.15803337097168\n",
      "step : 25.99 % , loss : 5.105411052703857\n",
      "step : 26.43 % , loss : 5.112185478210449\n",
      "step : 26.87 % , loss : 5.024733543395996\n",
      "step : 27.31 % , loss : 5.044699192047119\n",
      "step : 27.75 % , loss : 5.071990489959717\n",
      "step : 28.19 % , loss : 5.034730434417725\n",
      "step : 28.63 % , loss : 5.10751485824585\n",
      "step : 29.07 % , loss : 5.142319679260254\n",
      "step : 29.52 % , loss : 5.076258182525635\n",
      "step : 29.96 % , loss : 5.125183582305908\n",
      "step : 30.4 % , loss : 5.103067874908447\n",
      "step : 30.84 % , loss : 5.007605075836182\n",
      "step : 31.28 % , loss : 5.045968532562256\n",
      "step : 31.72 % , loss : 5.038454055786133\n",
      "step : 32.16 % , loss : 5.037517070770264\n",
      "step : 32.6 % , loss : 5.055141448974609\n",
      "step : 33.04 % , loss : 5.036535263061523\n",
      "step : 33.48 % , loss : 5.104208946228027\n",
      "step : 33.92 % , loss : 5.000772476196289\n",
      "step : 34.36 % , loss : 5.0696539878845215\n",
      "step : 34.8 % , loss : 5.215256214141846\n",
      "step : 35.24 % , loss : 5.069240093231201\n",
      "step : 35.68 % , loss : 5.0315961837768555\n",
      "step : 36.12 % , loss : 5.049561023712158\n",
      "step : 36.56 % , loss : 5.120709419250488\n",
      "step : 37.0 % , loss : 5.101820468902588\n",
      "step : 37.44 % , loss : 5.079707145690918\n",
      "step : 37.89 % , loss : 5.076712131500244\n",
      "step : 38.33 % , loss : 5.055887222290039\n",
      "step : 38.77 % , loss : 5.0081329345703125\n",
      "step : 39.21 % , loss : 5.014890193939209\n",
      "step : 39.65 % , loss : 5.106513977050781\n",
      "step : 40.09 % , loss : 5.045161247253418\n",
      "step : 40.53 % , loss : 5.076473712921143\n",
      "step : 40.97 % , loss : 5.012004852294922\n",
      "step : 41.41 % , loss : 4.978072643280029\n",
      "step : 41.85 % , loss : 5.112604141235352\n",
      "step : 42.29 % , loss : 5.078805446624756\n",
      "step : 42.73 % , loss : 5.039792537689209\n",
      "step : 43.17 % , loss : 5.140070915222168\n",
      "step : 43.61 % , loss : 4.994028091430664\n",
      "step : 44.05 % , loss : 5.054806709289551\n",
      "step : 44.49 % , loss : 5.1039228439331055\n",
      "step : 44.93 % , loss : 4.98186731338501\n",
      "step : 45.37 % , loss : 4.972081184387207\n",
      "step : 45.81 % , loss : 4.96730899810791\n",
      "step : 46.26 % , loss : 5.065323829650879\n",
      "step : 46.7 % , loss : 5.11558198928833\n",
      "step : 47.14 % , loss : 5.11144495010376\n",
      "step : 47.58 % , loss : 5.035195350646973\n",
      "step : 48.02 % , loss : 4.990654945373535\n",
      "step : 48.46 % , loss : 5.156412601470947\n",
      "step : 48.9 % , loss : 5.060798168182373\n",
      "step : 49.34 % , loss : 5.01197624206543\n",
      "step : 49.78 % , loss : 5.0299224853515625\n",
      "step : 50.22 % , loss : 5.1212310791015625\n",
      "step : 50.66 % , loss : 5.086850643157959\n",
      "step : 51.1 % , loss : 5.031887531280518\n",
      "step : 51.54 % , loss : 5.153801441192627\n",
      "step : 51.98 % , loss : 5.035637378692627\n",
      "step : 52.42 % , loss : 5.0151166915893555\n",
      "step : 52.86 % , loss : 5.093243598937988\n",
      "step : 53.3 % , loss : 4.92673397064209\n",
      "step : 53.74 % , loss : 4.949509143829346\n",
      "step : 54.19 % , loss : 5.090431213378906\n",
      "step : 54.63 % , loss : 5.003559112548828\n",
      "step : 55.07 % , loss : 4.908684253692627\n",
      "step : 55.51 % , loss : 4.882281303405762\n",
      "step : 55.95 % , loss : 5.0178446769714355\n",
      "step : 56.39 % , loss : 5.0904059410095215\n",
      "step : 56.83 % , loss : 5.087973594665527\n",
      "step : 57.27 % , loss : 5.007234573364258\n",
      "step : 57.71 % , loss : 5.154439449310303\n",
      "step : 58.15 % , loss : 5.018401622772217\n",
      "step : 58.59 % , loss : 5.012871742248535\n",
      "step : 59.03 % , loss : 5.11069917678833\n",
      "step : 59.47 % , loss : 5.092952728271484\n",
      "step : 59.91 % , loss : 5.020514965057373\n",
      "step : 60.35 % , loss : 4.992506980895996\n",
      "step : 60.79 % , loss : 5.042545318603516\n",
      "step : 61.23 % , loss : 5.025240421295166\n",
      "step : 61.67 % , loss : 5.044354438781738\n",
      "step : 62.11 % , loss : 5.045677185058594\n",
      "step : 62.56 % , loss : 5.171744346618652\n",
      "step : 63.0 % , loss : 5.13125467300415\n",
      "step : 63.44 % , loss : 5.045877933502197\n",
      "step : 63.88 % , loss : 4.954546928405762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 64.32 % , loss : 4.974672794342041\n",
      "step : 64.76 % , loss : 5.028350353240967\n",
      "step : 65.2 % , loss : 5.015574932098389\n",
      "step : 65.64 % , loss : 5.08001184463501\n",
      "step : 66.08 % , loss : 4.9387102127075195\n",
      "step : 66.52 % , loss : 4.975991725921631\n",
      "step : 66.96 % , loss : 5.15720272064209\n",
      "step : 67.4 % , loss : 5.064297199249268\n",
      "step : 67.84 % , loss : 5.1693034172058105\n",
      "step : 68.28 % , loss : 5.108489990234375\n",
      "step : 68.72 % , loss : 4.9478020668029785\n",
      "step : 69.16 % , loss : 5.063982963562012\n",
      "step : 69.6 % , loss : 4.943664073944092\n",
      "step : 70.04 % , loss : 5.058370113372803\n",
      "step : 70.48 % , loss : 4.9064249992370605\n",
      "step : 70.93 % , loss : 5.085102081298828\n",
      "step : 71.37 % , loss : 5.121302127838135\n",
      "step : 71.81 % , loss : 4.976963520050049\n",
      "step : 72.25 % , loss : 4.988828659057617\n",
      "step : 72.69 % , loss : 5.050327777862549\n",
      "step : 73.13 % , loss : 4.91131591796875\n",
      "step : 73.57 % , loss : 5.002174377441406\n",
      "step : 74.01 % , loss : 5.008303642272949\n",
      "step : 74.45 % , loss : 5.052617073059082\n",
      "step : 74.89 % , loss : 4.9654221534729\n",
      "step : 75.33 % , loss : 4.958103656768799\n",
      "step : 75.77 % , loss : 5.035222053527832\n",
      "step : 76.21 % , loss : 5.00013542175293\n",
      "step : 76.65 % , loss : 5.0564141273498535\n",
      "step : 77.09 % , loss : 5.116424560546875\n",
      "step : 77.53 % , loss : 4.934134006500244\n",
      "step : 77.97 % , loss : 4.972883701324463\n",
      "step : 78.41 % , loss : 4.965198516845703\n",
      "step : 78.85 % , loss : 5.012880802154541\n",
      "step : 79.3 % , loss : 5.0127129554748535\n",
      "step : 79.74 % , loss : 5.084556579589844\n",
      "step : 80.18 % , loss : 5.036648750305176\n",
      "step : 80.62 % , loss : 5.131324768066406\n",
      "step : 81.06 % , loss : 5.007205963134766\n",
      "step : 81.5 % , loss : 5.09099817276001\n",
      "step : 81.94 % , loss : 5.142656326293945\n",
      "step : 82.38 % , loss : 4.988129615783691\n",
      "step : 82.82 % , loss : 5.082703590393066\n",
      "step : 83.26 % , loss : 5.048470973968506\n",
      "step : 83.7 % , loss : 4.9557085037231445\n",
      "step : 84.14 % , loss : 5.028481960296631\n",
      "step : 84.58 % , loss : 5.0598039627075195\n",
      "step : 85.02 % , loss : 4.973223686218262\n",
      "step : 85.46 % , loss : 5.080084323883057\n",
      "step : 85.9 % , loss : 5.03381872177124\n",
      "step : 86.34 % , loss : 5.140104293823242\n",
      "step : 86.78 % , loss : 4.880706310272217\n",
      "step : 87.22 % , loss : 5.039849758148193\n",
      "step : 87.67 % , loss : 5.050684452056885\n",
      "step : 88.11 % , loss : 5.069761276245117\n",
      "step : 88.55 % , loss : 5.048642635345459\n",
      "step : 88.99 % , loss : 4.989997386932373\n",
      "step : 89.43 % , loss : 5.048330783843994\n",
      "step : 89.87 % , loss : 4.991914749145508\n",
      "step : 90.31 % , loss : 5.05309534072876\n",
      "step : 90.75 % , loss : 5.004288196563721\n",
      "step : 91.19 % , loss : 5.033350467681885\n",
      "step : 91.63 % , loss : 4.8740434646606445\n",
      "step : 92.07 % , loss : 5.086002826690674\n",
      "step : 92.51 % , loss : 5.009406089782715\n",
      "step : 92.95 % , loss : 5.024465560913086\n",
      "step : 93.39 % , loss : 5.037092208862305\n",
      "step : 93.83 % , loss : 5.002867698669434\n",
      "step : 94.27 % , loss : 5.11305046081543\n",
      "step : 94.71 % , loss : 5.017828941345215\n",
      "step : 95.15 % , loss : 4.994905471801758\n",
      "step : 95.59 % , loss : 4.91792631149292\n",
      "step : 96.04 % , loss : 5.074067115783691\n",
      "step : 96.48 % , loss : 5.099418640136719\n",
      "step : 96.92 % , loss : 5.14402437210083\n",
      "step : 97.36 % , loss : 5.082128524780273\n",
      "step : 97.8 % , loss : 5.122152805328369\n",
      "step : 98.24 % , loss : 5.014292240142822\n",
      "step : 98.68 % , loss : 4.901796340942383\n",
      "step : 99.12 % , loss : 4.995712757110596\n",
      "step : 99.56 % , loss : 5.023749828338623\n",
      "\tTrain Loss: 5.058\n",
      "step : 0.0 % , loss : 4.948060512542725\n",
      "step : 0.44 % , loss : 5.107045650482178\n",
      "step : 0.88 % , loss : 4.958726406097412\n",
      "step : 1.32 % , loss : 5.035268783569336\n",
      "step : 1.76 % , loss : 5.0247416496276855\n",
      "step : 2.2 % , loss : 4.922863483428955\n",
      "step : 2.64 % , loss : 4.969480037689209\n",
      "step : 3.08 % , loss : 5.0877556800842285\n",
      "step : 3.52 % , loss : 5.036415100097656\n",
      "step : 3.96 % , loss : 4.909856796264648\n",
      "step : 4.41 % , loss : 5.092188358306885\n",
      "step : 4.85 % , loss : 5.022866249084473\n",
      "step : 5.29 % , loss : 5.015739440917969\n",
      "step : 5.73 % , loss : 5.076242446899414\n",
      "step : 6.17 % , loss : 5.0399017333984375\n",
      "step : 6.61 % , loss : 5.009785175323486\n",
      "step : 7.05 % , loss : 5.113224983215332\n",
      "step : 7.49 % , loss : 5.09634256362915\n",
      "step : 7.93 % , loss : 5.032718658447266\n",
      "step : 8.37 % , loss : 5.083545207977295\n",
      "step : 8.81 % , loss : 4.929790019989014\n",
      "step : 9.25 % , loss : 4.943863391876221\n",
      "step : 9.69 % , loss : 4.9388651847839355\n",
      "step : 10.13 % , loss : 4.953306674957275\n",
      "step : 10.57 % , loss : 4.96746301651001\n",
      "step : 11.01 % , loss : 5.017810821533203\n",
      "step : 11.45 % , loss : 4.9177069664001465\n",
      "step : 11.89 % , loss : 5.004693031311035\n",
      "step : 12.33 % , loss : 5.070038318634033\n",
      "step : 12.78 % , loss : 4.944855213165283\n",
      "step : 13.22 % , loss : 5.041231155395508\n",
      "step : 13.66 % , loss : 4.9241790771484375\n",
      "step : 14.1 % , loss : 5.069501876831055\n",
      "step : 14.54 % , loss : 5.022761344909668\n",
      "step : 14.98 % , loss : 4.964430332183838\n",
      "step : 15.42 % , loss : 4.998451232910156\n",
      "step : 15.86 % , loss : 5.063754081726074\n",
      "step : 16.3 % , loss : 5.052183151245117\n",
      "step : 16.74 % , loss : 5.067532539367676\n",
      "step : 17.18 % , loss : 4.921139240264893\n",
      "step : 17.62 % , loss : 4.968679428100586\n",
      "step : 18.06 % , loss : 5.0142083168029785\n",
      "step : 18.5 % , loss : 5.037600040435791\n",
      "step : 18.94 % , loss : 5.019400119781494\n",
      "step : 19.38 % , loss : 4.963895320892334\n",
      "step : 19.82 % , loss : 4.970793724060059\n",
      "step : 20.26 % , loss : 4.930983543395996\n",
      "step : 20.7 % , loss : 4.9126152992248535\n",
      "step : 21.15 % , loss : 4.9394025802612305\n",
      "step : 21.59 % , loss : 5.035599708557129\n",
      "step : 22.03 % , loss : 4.975812911987305\n",
      "step : 22.47 % , loss : 4.960648059844971\n",
      "step : 22.91 % , loss : 5.05760383605957\n",
      "step : 23.35 % , loss : 4.994322299957275\n",
      "step : 23.79 % , loss : 5.050170421600342\n",
      "step : 24.23 % , loss : 4.926348686218262\n",
      "step : 24.67 % , loss : 4.987342834472656\n",
      "step : 25.11 % , loss : 5.062664985656738\n",
      "step : 25.55 % , loss : 4.988640785217285\n",
      "step : 25.99 % , loss : 5.002874851226807\n",
      "step : 26.43 % , loss : 4.951286315917969\n",
      "step : 26.87 % , loss : 5.008650302886963\n",
      "step : 27.31 % , loss : 4.988193511962891\n",
      "step : 27.75 % , loss : 5.059659957885742\n",
      "step : 28.19 % , loss : 4.895905017852783\n",
      "step : 28.63 % , loss : 4.876949787139893\n",
      "step : 29.07 % , loss : 5.009260654449463\n",
      "step : 29.52 % , loss : 4.963554382324219\n",
      "step : 29.96 % , loss : 4.961910724639893\n",
      "step : 30.4 % , loss : 4.881293296813965\n",
      "step : 30.84 % , loss : 4.961386203765869\n",
      "step : 31.28 % , loss : 5.017261028289795\n",
      "step : 31.72 % , loss : 4.986452579498291\n",
      "step : 32.16 % , loss : 5.0114593505859375\n",
      "step : 32.6 % , loss : 4.994277477264404\n",
      "step : 33.04 % , loss : 4.94388484954834\n",
      "step : 33.48 % , loss : 4.889945983886719\n",
      "step : 33.92 % , loss : 4.939216136932373\n",
      "step : 34.36 % , loss : 4.900853633880615\n",
      "step : 34.8 % , loss : 5.040988445281982\n",
      "step : 35.24 % , loss : 4.865364074707031\n",
      "step : 35.68 % , loss : 4.897873878479004\n",
      "step : 36.12 % , loss : 5.024118900299072\n",
      "step : 36.56 % , loss : 4.971503734588623\n",
      "step : 37.0 % , loss : 4.990201950073242\n",
      "step : 37.44 % , loss : 4.863259315490723\n",
      "step : 37.89 % , loss : 4.85971212387085\n",
      "step : 38.33 % , loss : 4.983534336090088\n",
      "step : 38.77 % , loss : 4.9910383224487305\n",
      "step : 39.21 % , loss : 4.899524211883545\n",
      "step : 39.65 % , loss : 4.9464335441589355\n",
      "step : 40.09 % , loss : 4.954188346862793\n",
      "step : 40.53 % , loss : 4.994441986083984\n",
      "step : 40.97 % , loss : 5.076356887817383\n",
      "step : 41.41 % , loss : 4.983709335327148\n",
      "step : 41.85 % , loss : 4.924849033355713\n",
      "step : 42.29 % , loss : 4.869802951812744\n",
      "step : 42.73 % , loss : 5.020476818084717\n",
      "step : 43.17 % , loss : 5.070098400115967\n",
      "step : 43.61 % , loss : 4.966599464416504\n",
      "step : 44.05 % , loss : 5.009059429168701\n",
      "step : 44.49 % , loss : 5.020712852478027\n",
      "step : 44.93 % , loss : 4.841164588928223\n",
      "step : 45.37 % , loss : 5.060731887817383\n",
      "step : 45.81 % , loss : 5.120701789855957\n",
      "step : 46.26 % , loss : 4.867083549499512\n",
      "step : 46.7 % , loss : 5.017006874084473\n",
      "step : 47.14 % , loss : 4.967730522155762\n",
      "step : 47.58 % , loss : 4.992013931274414\n",
      "step : 48.02 % , loss : 5.025691032409668\n",
      "step : 48.46 % , loss : 4.922649383544922\n",
      "step : 48.9 % , loss : 5.005870819091797\n",
      "step : 49.34 % , loss : 5.05406379699707\n",
      "step : 49.78 % , loss : 5.076171398162842\n",
      "step : 50.22 % , loss : 4.991511821746826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 50.66 % , loss : 5.0617289543151855\n",
      "step : 51.1 % , loss : 4.9276123046875\n",
      "step : 51.54 % , loss : 5.0104660987854\n",
      "step : 51.98 % , loss : 4.952930927276611\n",
      "step : 52.42 % , loss : 4.978856086730957\n",
      "step : 52.86 % , loss : 4.892258167266846\n",
      "step : 53.3 % , loss : 4.9480133056640625\n",
      "step : 53.74 % , loss : 4.982456684112549\n",
      "step : 54.19 % , loss : 4.858301639556885\n",
      "step : 54.63 % , loss : 4.919564723968506\n",
      "step : 55.07 % , loss : 5.004458427429199\n",
      "step : 55.51 % , loss : 5.019655227661133\n",
      "step : 55.95 % , loss : 5.042981147766113\n",
      "step : 56.39 % , loss : 4.9275641441345215\n",
      "step : 56.83 % , loss : 4.974697113037109\n",
      "step : 57.27 % , loss : 4.82843017578125\n",
      "step : 57.71 % , loss : 4.924426555633545\n",
      "step : 58.15 % , loss : 4.9868950843811035\n",
      "step : 58.59 % , loss : 4.911970138549805\n",
      "step : 59.03 % , loss : 5.014366626739502\n",
      "step : 59.47 % , loss : 4.950563430786133\n",
      "step : 59.91 % , loss : 4.898519515991211\n",
      "step : 60.35 % , loss : 4.931960582733154\n",
      "step : 60.79 % , loss : 4.893294334411621\n",
      "step : 61.23 % , loss : 4.976302623748779\n",
      "step : 61.67 % , loss : 5.004118919372559\n",
      "step : 62.11 % , loss : 5.0003767013549805\n",
      "step : 62.56 % , loss : 4.876576900482178\n",
      "step : 63.0 % , loss : 5.022141456604004\n",
      "step : 63.44 % , loss : 4.948373317718506\n",
      "step : 63.88 % , loss : 5.120168685913086\n",
      "step : 64.32 % , loss : 5.025355339050293\n",
      "step : 64.76 % , loss : 5.020702362060547\n",
      "step : 65.2 % , loss : 5.056303977966309\n",
      "step : 65.64 % , loss : 4.860500335693359\n",
      "step : 66.08 % , loss : 5.077311992645264\n",
      "step : 66.52 % , loss : 4.929548263549805\n",
      "step : 66.96 % , loss : 4.910052299499512\n",
      "step : 67.4 % , loss : 4.847898960113525\n",
      "step : 67.84 % , loss : 4.973301887512207\n",
      "step : 68.28 % , loss : 5.017767429351807\n",
      "step : 68.72 % , loss : 4.968688011169434\n",
      "step : 69.16 % , loss : 4.952967166900635\n",
      "step : 69.6 % , loss : 4.9776458740234375\n",
      "step : 70.04 % , loss : 4.960670471191406\n",
      "step : 70.48 % , loss : 4.887953281402588\n",
      "step : 70.93 % , loss : 4.945995807647705\n",
      "step : 71.37 % , loss : 5.041457176208496\n",
      "step : 71.81 % , loss : 4.954346179962158\n",
      "step : 72.25 % , loss : 4.956483840942383\n",
      "step : 72.69 % , loss : 4.938625335693359\n",
      "step : 73.13 % , loss : 4.9417524337768555\n",
      "step : 73.57 % , loss : 4.885807514190674\n",
      "step : 74.01 % , loss : 4.996623516082764\n",
      "step : 74.45 % , loss : 4.866457939147949\n",
      "step : 74.89 % , loss : 5.056980133056641\n",
      "step : 75.33 % , loss : 4.945779800415039\n",
      "step : 75.77 % , loss : 4.999942302703857\n",
      "step : 76.21 % , loss : 4.982676029205322\n",
      "step : 76.65 % , loss : 4.9168195724487305\n",
      "step : 77.09 % , loss : 4.981014251708984\n",
      "step : 77.53 % , loss : 4.9671711921691895\n",
      "step : 77.97 % , loss : 4.891721725463867\n",
      "step : 78.41 % , loss : 5.08614444732666\n",
      "step : 78.85 % , loss : 5.007107734680176\n",
      "step : 79.3 % , loss : 4.884140968322754\n",
      "step : 79.74 % , loss : 4.994699001312256\n",
      "step : 80.18 % , loss : 4.853231906890869\n",
      "step : 80.62 % , loss : 5.0317254066467285\n",
      "step : 81.06 % , loss : 4.853481769561768\n",
      "step : 81.5 % , loss : 4.973072052001953\n",
      "step : 81.94 % , loss : 4.877142906188965\n",
      "step : 82.38 % , loss : 4.855243682861328\n",
      "step : 82.82 % , loss : 4.933411121368408\n",
      "step : 83.26 % , loss : 4.939455509185791\n",
      "step : 83.7 % , loss : 4.995739936828613\n",
      "step : 84.14 % , loss : 4.820765972137451\n",
      "step : 84.58 % , loss : 4.958991527557373\n",
      "step : 85.02 % , loss : 4.932062149047852\n",
      "step : 85.46 % , loss : 5.007373332977295\n",
      "step : 85.9 % , loss : 5.039257049560547\n",
      "step : 86.34 % , loss : 4.986332416534424\n",
      "step : 86.78 % , loss : 4.908731460571289\n",
      "step : 87.22 % , loss : 5.06093168258667\n",
      "step : 87.67 % , loss : 4.9310808181762695\n",
      "step : 88.11 % , loss : 5.003413200378418\n",
      "step : 88.55 % , loss : 4.845423221588135\n",
      "step : 88.99 % , loss : 4.912831783294678\n",
      "step : 89.43 % , loss : 5.024270057678223\n",
      "step : 89.87 % , loss : 4.8482136726379395\n",
      "step : 90.31 % , loss : 4.854085445404053\n",
      "step : 90.75 % , loss : 4.887053489685059\n",
      "step : 91.19 % , loss : 4.897990703582764\n",
      "step : 91.63 % , loss : 4.994233131408691\n",
      "step : 92.07 % , loss : 4.909467697143555\n",
      "step : 92.51 % , loss : 4.977583885192871\n",
      "step : 92.95 % , loss : 4.945684432983398\n",
      "step : 93.39 % , loss : 4.857948303222656\n",
      "step : 93.83 % , loss : 4.987804889678955\n",
      "step : 94.27 % , loss : 4.9766526222229\n",
      "step : 94.71 % , loss : 4.842322826385498\n",
      "step : 95.15 % , loss : 4.964547634124756\n",
      "step : 95.59 % , loss : 4.980478286743164\n",
      "step : 96.04 % , loss : 4.965674877166748\n",
      "step : 96.48 % , loss : 4.965396404266357\n",
      "step : 96.92 % , loss : 4.839240550994873\n",
      "step : 97.36 % , loss : 5.035184860229492\n",
      "step : 97.8 % , loss : 4.927341461181641\n",
      "step : 98.24 % , loss : 4.979553699493408\n",
      "step : 98.68 % , loss : 4.982907295227051\n",
      "step : 99.12 % , loss : 4.975015640258789\n",
      "step : 99.56 % , loss : 4.869716644287109\n",
      "\tTrain Loss: 4.971\n",
      "step : 0.0 % , loss : 4.859134197235107\n",
      "step : 0.44 % , loss : 4.956916809082031\n",
      "step : 0.88 % , loss : 4.908686637878418\n",
      "step : 1.32 % , loss : 4.914742946624756\n",
      "step : 1.76 % , loss : 4.971902370452881\n",
      "step : 2.2 % , loss : 5.005825996398926\n",
      "step : 2.64 % , loss : 4.911770820617676\n",
      "step : 3.08 % , loss : 4.994612216949463\n",
      "step : 3.52 % , loss : 4.880227565765381\n",
      "step : 3.96 % , loss : 4.98539924621582\n",
      "step : 4.41 % , loss : 4.939502716064453\n",
      "step : 4.85 % , loss : 5.121990203857422\n",
      "step : 5.29 % , loss : 4.831376552581787\n",
      "step : 5.73 % , loss : 4.964527130126953\n",
      "step : 6.17 % , loss : 5.0054216384887695\n",
      "step : 6.61 % , loss : 4.891115665435791\n",
      "step : 7.05 % , loss : 5.002841472625732\n",
      "step : 7.49 % , loss : 4.8701019287109375\n",
      "step : 7.93 % , loss : 4.91348123550415\n",
      "step : 8.37 % , loss : 4.815941333770752\n",
      "step : 8.81 % , loss : 4.8824143409729\n",
      "step : 9.25 % , loss : 4.989798545837402\n",
      "step : 9.69 % , loss : 4.956266403198242\n",
      "step : 10.13 % , loss : 4.988841533660889\n",
      "step : 10.57 % , loss : 4.960550785064697\n",
      "step : 11.01 % , loss : 4.957932472229004\n",
      "step : 11.45 % , loss : 4.962369918823242\n",
      "step : 11.89 % , loss : 4.889894485473633\n",
      "step : 12.33 % , loss : 4.95949649810791\n",
      "step : 12.78 % , loss : 5.019856929779053\n",
      "step : 13.22 % , loss : 4.8504438400268555\n",
      "step : 13.66 % , loss : 4.9654412269592285\n",
      "step : 14.1 % , loss : 5.0212931632995605\n",
      "step : 14.54 % , loss : 4.904173374176025\n",
      "step : 14.98 % , loss : 4.910595893859863\n",
      "step : 15.42 % , loss : 4.951526165008545\n",
      "step : 15.86 % , loss : 4.986804008483887\n",
      "step : 16.3 % , loss : 5.000543117523193\n",
      "step : 16.74 % , loss : 4.8738884925842285\n",
      "step : 17.18 % , loss : 4.958539962768555\n",
      "step : 17.62 % , loss : 4.943731784820557\n",
      "step : 18.06 % , loss : 4.999011516571045\n",
      "step : 18.5 % , loss : 4.850412368774414\n",
      "step : 18.94 % , loss : 5.102837085723877\n",
      "step : 19.38 % , loss : 4.849851608276367\n",
      "step : 19.82 % , loss : 4.920039176940918\n",
      "step : 20.26 % , loss : 5.057948589324951\n",
      "step : 20.7 % , loss : 4.842683792114258\n",
      "step : 21.15 % , loss : 4.945313453674316\n",
      "step : 21.59 % , loss : 4.965848445892334\n",
      "step : 22.03 % , loss : 4.906861782073975\n",
      "step : 22.47 % , loss : 4.935924053192139\n",
      "step : 22.91 % , loss : 5.00990629196167\n",
      "step : 23.35 % , loss : 4.969869136810303\n",
      "step : 23.79 % , loss : 5.065060615539551\n",
      "step : 24.23 % , loss : 4.935070037841797\n",
      "step : 24.67 % , loss : 5.009774684906006\n",
      "step : 25.11 % , loss : 5.050649642944336\n",
      "step : 25.55 % , loss : 4.9249372482299805\n",
      "step : 25.99 % , loss : 4.866659641265869\n",
      "step : 26.43 % , loss : 4.86216926574707\n",
      "step : 26.87 % , loss : 4.8172760009765625\n",
      "step : 27.31 % , loss : 4.938658237457275\n",
      "step : 27.75 % , loss : 4.968346118927002\n",
      "step : 28.19 % , loss : 4.798320293426514\n",
      "step : 28.63 % , loss : 4.940354347229004\n",
      "step : 29.07 % , loss : 4.949610233306885\n",
      "step : 29.52 % , loss : 4.971539497375488\n",
      "step : 29.96 % , loss : 4.913804054260254\n",
      "step : 30.4 % , loss : 4.886402606964111\n",
      "step : 30.84 % , loss : 4.965953350067139\n",
      "step : 31.28 % , loss : 4.954412937164307\n",
      "step : 31.72 % , loss : 5.00816535949707\n",
      "step : 32.16 % , loss : 4.9449462890625\n",
      "step : 32.6 % , loss : 4.9777116775512695\n",
      "step : 33.04 % , loss : 4.8099212646484375\n",
      "step : 33.48 % , loss : 5.033291816711426\n",
      "step : 33.92 % , loss : 4.949370384216309\n",
      "step : 34.36 % , loss : 5.081239700317383\n",
      "step : 34.8 % , loss : 4.992001056671143\n",
      "step : 35.24 % , loss : 4.902398109436035\n",
      "step : 35.68 % , loss : 4.9517436027526855\n",
      "step : 36.12 % , loss : 4.850869655609131\n",
      "step : 36.56 % , loss : 4.9996562004089355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 37.0 % , loss : 5.002415657043457\n",
      "step : 37.44 % , loss : 4.958298683166504\n",
      "step : 37.89 % , loss : 4.933481216430664\n",
      "step : 38.33 % , loss : 4.9006547927856445\n",
      "step : 38.77 % , loss : 5.022242069244385\n",
      "step : 39.21 % , loss : 4.896275520324707\n",
      "step : 39.65 % , loss : 4.951450347900391\n",
      "step : 40.09 % , loss : 4.956408500671387\n",
      "step : 40.53 % , loss : 4.85603141784668\n",
      "step : 40.97 % , loss : 4.9147138595581055\n",
      "step : 41.41 % , loss : 5.013486385345459\n",
      "step : 41.85 % , loss : 4.981802463531494\n",
      "step : 42.29 % , loss : 4.891615867614746\n",
      "step : 42.73 % , loss : 4.923184394836426\n",
      "step : 43.17 % , loss : 4.904707908630371\n",
      "step : 43.61 % , loss : 4.9551920890808105\n",
      "step : 44.05 % , loss : 4.860176086425781\n",
      "step : 44.49 % , loss : 5.005152702331543\n",
      "step : 44.93 % , loss : 4.871753692626953\n",
      "step : 45.37 % , loss : 4.903177738189697\n",
      "step : 45.81 % , loss : 4.965453624725342\n",
      "step : 46.26 % , loss : 4.869676113128662\n",
      "step : 46.7 % , loss : 4.91786003112793\n",
      "step : 47.14 % , loss : 4.950780868530273\n",
      "step : 47.58 % , loss : 4.9272613525390625\n",
      "step : 48.02 % , loss : 4.842690467834473\n",
      "step : 48.46 % , loss : 4.971030235290527\n",
      "step : 48.9 % , loss : 4.862934589385986\n",
      "step : 49.34 % , loss : 4.957114219665527\n",
      "step : 49.78 % , loss : 4.887800693511963\n",
      "step : 50.22 % , loss : 4.95651912689209\n",
      "step : 50.66 % , loss : 4.945019245147705\n",
      "step : 51.1 % , loss : 4.899983882904053\n",
      "step : 51.54 % , loss : 5.048545837402344\n",
      "step : 51.98 % , loss : 5.03289270401001\n",
      "step : 52.42 % , loss : 4.938632965087891\n",
      "step : 52.86 % , loss : 4.943098545074463\n",
      "step : 53.3 % , loss : 4.936320781707764\n",
      "step : 53.74 % , loss : 4.943512916564941\n",
      "step : 54.19 % , loss : 4.972324848175049\n",
      "step : 54.63 % , loss : 4.875444412231445\n",
      "step : 55.07 % , loss : 4.993587970733643\n",
      "step : 55.51 % , loss : 4.950322151184082\n",
      "step : 55.95 % , loss : 4.907844543457031\n",
      "step : 56.39 % , loss : 4.928321838378906\n",
      "step : 56.83 % , loss : 4.876548767089844\n",
      "step : 57.27 % , loss : 4.934971332550049\n",
      "step : 57.71 % , loss : 4.95163631439209\n",
      "step : 58.15 % , loss : 4.8594560623168945\n",
      "step : 58.59 % , loss : 4.935863971710205\n",
      "step : 59.03 % , loss : 5.030736923217773\n",
      "step : 59.47 % , loss : 4.864459991455078\n",
      "step : 59.91 % , loss : 4.978211402893066\n",
      "step : 60.35 % , loss : 4.952108383178711\n",
      "step : 60.79 % , loss : 4.938385963439941\n",
      "step : 61.23 % , loss : 4.921173095703125\n",
      "step : 61.67 % , loss : 4.764519214630127\n",
      "step : 62.11 % , loss : 4.8693952560424805\n",
      "step : 62.56 % , loss : 4.9192705154418945\n",
      "step : 63.0 % , loss : 4.962501049041748\n",
      "step : 63.44 % , loss : 4.992210388183594\n",
      "step : 63.88 % , loss : 4.960737705230713\n",
      "step : 64.32 % , loss : 4.829404830932617\n",
      "step : 64.76 % , loss : 5.0469865798950195\n",
      "step : 65.2 % , loss : 4.986125469207764\n",
      "step : 65.64 % , loss : 4.974547386169434\n",
      "step : 66.08 % , loss : 4.974542140960693\n",
      "step : 66.52 % , loss : 4.960710525512695\n",
      "step : 66.96 % , loss : 4.915960788726807\n",
      "step : 67.4 % , loss : 4.894321918487549\n",
      "step : 67.84 % , loss : 4.881210803985596\n",
      "step : 68.28 % , loss : 4.863648891448975\n",
      "step : 68.72 % , loss : 4.919042110443115\n",
      "step : 69.16 % , loss : 5.030124187469482\n",
      "step : 69.6 % , loss : 4.990850448608398\n",
      "step : 70.04 % , loss : 5.060919761657715\n",
      "step : 70.48 % , loss : 4.907081127166748\n",
      "step : 70.93 % , loss : 4.948892593383789\n",
      "step : 71.37 % , loss : 4.952793598175049\n",
      "step : 71.81 % , loss : 5.003807544708252\n",
      "step : 72.25 % , loss : 4.973694801330566\n",
      "step : 72.69 % , loss : 5.016061782836914\n",
      "step : 73.13 % , loss : 4.961562156677246\n",
      "step : 73.57 % , loss : 4.890772342681885\n",
      "step : 74.01 % , loss : 4.849225997924805\n",
      "step : 74.45 % , loss : 5.061127662658691\n",
      "step : 74.89 % , loss : 4.9522385597229\n",
      "step : 75.33 % , loss : 4.839034557342529\n",
      "step : 75.77 % , loss : 4.963305950164795\n",
      "step : 76.21 % , loss : 4.9183807373046875\n",
      "step : 76.65 % , loss : 4.950230598449707\n",
      "step : 77.09 % , loss : 4.884632110595703\n",
      "step : 77.53 % , loss : 4.920270919799805\n",
      "step : 77.97 % , loss : 4.942291259765625\n",
      "step : 78.41 % , loss : 4.928626537322998\n",
      "step : 78.85 % , loss : 4.937255382537842\n",
      "step : 79.3 % , loss : 4.935523509979248\n",
      "step : 79.74 % , loss : 4.884608268737793\n",
      "step : 80.18 % , loss : 5.155399322509766\n",
      "step : 80.62 % , loss : 5.0348286628723145\n",
      "step : 81.06 % , loss : 4.861451148986816\n",
      "step : 81.5 % , loss : 4.872617244720459\n",
      "step : 81.94 % , loss : 4.973336696624756\n",
      "step : 82.38 % , loss : 4.904006481170654\n",
      "step : 82.82 % , loss : 4.932075500488281\n",
      "step : 83.26 % , loss : 4.871189594268799\n",
      "step : 83.7 % , loss : 4.88721227645874\n",
      "step : 84.14 % , loss : 4.937541961669922\n",
      "step : 84.58 % , loss : 4.905641078948975\n",
      "step : 85.02 % , loss : 5.036709308624268\n",
      "step : 85.46 % , loss : 4.838084697723389\n",
      "step : 85.9 % , loss : 4.975484371185303\n",
      "step : 86.34 % , loss : 4.909803867340088\n",
      "step : 86.78 % , loss : 4.933845520019531\n",
      "step : 87.22 % , loss : 4.902118682861328\n",
      "step : 87.67 % , loss : 5.005810260772705\n",
      "step : 88.11 % , loss : 4.8363118171691895\n",
      "step : 88.55 % , loss : 4.934017658233643\n",
      "step : 88.99 % , loss : 4.836607933044434\n",
      "step : 89.43 % , loss : 4.976066589355469\n",
      "step : 89.87 % , loss : 4.946169376373291\n",
      "step : 90.31 % , loss : 4.853908538818359\n",
      "step : 90.75 % , loss : 4.930140018463135\n",
      "step : 91.19 % , loss : 5.0080718994140625\n",
      "step : 91.63 % , loss : 4.973413944244385\n",
      "step : 92.07 % , loss : 4.819379806518555\n",
      "step : 92.51 % , loss : 4.879319667816162\n",
      "step : 92.95 % , loss : 4.9021806716918945\n",
      "step : 93.39 % , loss : 4.934489727020264\n",
      "step : 93.83 % , loss : 4.875791072845459\n",
      "step : 94.27 % , loss : 4.8795695304870605\n",
      "step : 94.71 % , loss : 4.7333292961120605\n",
      "step : 95.15 % , loss : 4.904260158538818\n",
      "step : 95.59 % , loss : 4.910195827484131\n",
      "step : 96.04 % , loss : 4.902492046356201\n",
      "step : 96.48 % , loss : 4.961513519287109\n",
      "step : 96.92 % , loss : 4.9746413230896\n",
      "step : 97.36 % , loss : 5.112546443939209\n",
      "step : 97.8 % , loss : 4.870532989501953\n",
      "step : 98.24 % , loss : 4.9118781089782715\n",
      "step : 98.68 % , loss : 4.808810710906982\n",
      "step : 99.12 % , loss : 4.839083194732666\n",
      "step : 99.56 % , loss : 4.809534549713135\n",
      "\tTrain Loss: 4.936\n",
      "step : 0.0 % , loss : 4.965724468231201\n",
      "step : 0.44 % , loss : 4.90975284576416\n",
      "step : 0.88 % , loss : 5.031487941741943\n",
      "step : 1.32 % , loss : 4.944315433502197\n",
      "step : 1.76 % , loss : 4.841310501098633\n",
      "step : 2.2 % , loss : 4.860316276550293\n",
      "step : 2.64 % , loss : 4.905616760253906\n",
      "step : 3.08 % , loss : 4.975252151489258\n",
      "step : 3.52 % , loss : 4.899189472198486\n",
      "step : 3.96 % , loss : 4.938958644866943\n",
      "step : 4.41 % , loss : 4.872577667236328\n",
      "step : 4.85 % , loss : 4.943619728088379\n",
      "step : 5.29 % , loss : 5.040280818939209\n",
      "step : 5.73 % , loss : 4.940384387969971\n",
      "step : 6.17 % , loss : 4.935583591461182\n",
      "step : 6.61 % , loss : 4.956243991851807\n",
      "step : 7.05 % , loss : 4.884942054748535\n",
      "step : 7.49 % , loss : 4.944331169128418\n",
      "step : 7.93 % , loss : 4.965444087982178\n",
      "step : 8.37 % , loss : 5.009748935699463\n",
      "step : 8.81 % , loss : 4.933771133422852\n",
      "step : 9.25 % , loss : 4.938969612121582\n",
      "step : 9.69 % , loss : 4.8798370361328125\n",
      "step : 10.13 % , loss : 5.013878345489502\n",
      "step : 10.57 % , loss : 4.963772773742676\n",
      "step : 11.01 % , loss : 4.896392822265625\n",
      "step : 11.45 % , loss : 4.902541637420654\n",
      "step : 11.89 % , loss : 4.890117168426514\n",
      "step : 12.33 % , loss : 4.94713830947876\n",
      "step : 12.78 % , loss : 4.845919609069824\n",
      "step : 13.22 % , loss : 4.93978214263916\n",
      "step : 13.66 % , loss : 4.963707447052002\n",
      "step : 14.1 % , loss : 4.908863544464111\n",
      "step : 14.54 % , loss : 4.900975704193115\n",
      "step : 14.98 % , loss : 4.953178882598877\n",
      "step : 15.42 % , loss : 4.928715229034424\n",
      "step : 15.86 % , loss : 4.798346042633057\n",
      "step : 16.3 % , loss : 5.002371788024902\n",
      "step : 16.74 % , loss : 4.94441032409668\n",
      "step : 17.18 % , loss : 5.047313213348389\n",
      "step : 17.62 % , loss : 4.989403247833252\n",
      "step : 18.06 % , loss : 4.908463478088379\n",
      "step : 18.5 % , loss : 4.958278656005859\n",
      "step : 18.94 % , loss : 4.9390363693237305\n",
      "step : 19.38 % , loss : 4.978019714355469\n",
      "step : 19.82 % , loss : 4.905319690704346\n",
      "step : 20.26 % , loss : 4.889385223388672\n",
      "step : 20.7 % , loss : 4.910012722015381\n",
      "step : 21.15 % , loss : 4.909255504608154\n",
      "step : 21.59 % , loss : 4.935422420501709\n",
      "step : 22.03 % , loss : 4.982316970825195\n",
      "step : 22.47 % , loss : 4.873075485229492\n",
      "step : 22.91 % , loss : 4.773159027099609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 23.35 % , loss : 4.869688510894775\n",
      "step : 23.79 % , loss : 4.977694988250732\n",
      "step : 24.23 % , loss : 4.922088623046875\n",
      "step : 24.67 % , loss : 4.921957015991211\n",
      "step : 25.11 % , loss : 5.024097442626953\n",
      "step : 25.55 % , loss : 4.903772830963135\n",
      "step : 25.99 % , loss : 4.864085674285889\n",
      "step : 26.43 % , loss : 4.960143566131592\n",
      "step : 26.87 % , loss : 4.98155403137207\n",
      "step : 27.31 % , loss : 4.88946008682251\n",
      "step : 27.75 % , loss : 4.940820217132568\n",
      "step : 28.19 % , loss : 4.852221965789795\n",
      "step : 28.63 % , loss : 4.834404945373535\n",
      "step : 29.07 % , loss : 4.925986289978027\n",
      "step : 29.52 % , loss : 4.84989595413208\n",
      "step : 29.96 % , loss : 4.872293472290039\n",
      "step : 30.4 % , loss : 4.881842136383057\n",
      "step : 30.84 % , loss : 5.010127067565918\n",
      "step : 31.28 % , loss : 4.962536334991455\n",
      "step : 31.72 % , loss : 4.8400797843933105\n",
      "step : 32.16 % , loss : 4.934924125671387\n",
      "step : 32.6 % , loss : 5.038213729858398\n",
      "step : 33.04 % , loss : 4.88314151763916\n",
      "step : 33.48 % , loss : 4.9603095054626465\n",
      "step : 33.92 % , loss : 4.950066089630127\n",
      "step : 34.36 % , loss : 4.934175968170166\n",
      "step : 34.8 % , loss : 4.888720512390137\n",
      "step : 35.24 % , loss : 5.011552810668945\n",
      "step : 35.68 % , loss : 4.978829383850098\n",
      "step : 36.12 % , loss : 5.021674156188965\n",
      "step : 36.56 % , loss : 4.970010280609131\n",
      "step : 37.0 % , loss : 4.954541206359863\n",
      "step : 37.44 % , loss : 4.916482448577881\n",
      "step : 37.89 % , loss : 4.920090675354004\n",
      "step : 38.33 % , loss : 4.963690757751465\n",
      "step : 38.77 % , loss : 4.89927339553833\n",
      "step : 39.21 % , loss : 4.93032693862915\n",
      "step : 39.65 % , loss : 4.853339195251465\n",
      "step : 40.09 % , loss : 4.928210258483887\n",
      "step : 40.53 % , loss : 4.887321472167969\n",
      "step : 40.97 % , loss : 4.960468769073486\n",
      "step : 41.41 % , loss : 4.819358825683594\n",
      "step : 41.85 % , loss : 4.985586643218994\n",
      "step : 42.29 % , loss : 4.849649906158447\n",
      "step : 42.73 % , loss : 4.832918643951416\n",
      "step : 43.17 % , loss : 5.065462589263916\n",
      "step : 43.61 % , loss : 4.973595142364502\n",
      "step : 44.05 % , loss : 4.972546577453613\n",
      "step : 44.49 % , loss : 4.966843605041504\n",
      "step : 44.93 % , loss : 4.9028120040893555\n",
      "step : 45.37 % , loss : 4.898209095001221\n",
      "step : 45.81 % , loss : 4.893357753753662\n",
      "step : 46.26 % , loss : 5.007298946380615\n",
      "step : 46.7 % , loss : 5.056685924530029\n",
      "step : 47.14 % , loss : 4.801445007324219\n",
      "step : 47.58 % , loss : 4.838409423828125\n",
      "step : 48.02 % , loss : 4.945856094360352\n",
      "step : 48.46 % , loss : 4.935047626495361\n",
      "step : 48.9 % , loss : 4.7926411628723145\n",
      "step : 49.34 % , loss : 4.937015056610107\n",
      "step : 49.78 % , loss : 4.981736183166504\n",
      "step : 50.22 % , loss : 5.012202739715576\n",
      "step : 50.66 % , loss : 4.912601470947266\n",
      "step : 51.1 % , loss : 4.931967258453369\n",
      "step : 51.54 % , loss : 4.919523239135742\n",
      "step : 51.98 % , loss : 4.911335468292236\n",
      "step : 52.42 % , loss : 4.871986389160156\n",
      "step : 52.86 % , loss : 4.8995041847229\n",
      "step : 53.3 % , loss : 4.961295127868652\n",
      "step : 53.74 % , loss : 4.855752468109131\n",
      "step : 54.19 % , loss : 4.990237712860107\n",
      "step : 54.63 % , loss : 4.919559478759766\n",
      "step : 55.07 % , loss : 4.900868892669678\n",
      "step : 55.51 % , loss : 4.876562595367432\n",
      "step : 55.95 % , loss : 4.900094032287598\n",
      "step : 56.39 % , loss : 5.065566062927246\n",
      "step : 56.83 % , loss : 4.882285118103027\n",
      "step : 57.27 % , loss : 4.947938442230225\n",
      "step : 57.71 % , loss : 4.849307537078857\n",
      "step : 58.15 % , loss : 4.8329997062683105\n",
      "step : 58.59 % , loss : 4.868838787078857\n",
      "step : 59.03 % , loss : 4.863887786865234\n",
      "step : 59.47 % , loss : 4.907229900360107\n",
      "step : 59.91 % , loss : 4.960424900054932\n",
      "step : 60.35 % , loss : 4.9059157371521\n",
      "step : 60.79 % , loss : 4.937203884124756\n",
      "step : 61.23 % , loss : 4.988179683685303\n",
      "step : 61.67 % , loss : 4.791225433349609\n",
      "step : 62.11 % , loss : 4.958451747894287\n",
      "step : 62.56 % , loss : 4.820261478424072\n",
      "step : 63.0 % , loss : 4.787959575653076\n",
      "step : 63.44 % , loss : 4.901252746582031\n",
      "step : 63.88 % , loss : 4.970713138580322\n",
      "step : 64.32 % , loss : 4.941286563873291\n",
      "step : 64.76 % , loss : 4.8721418380737305\n",
      "step : 65.2 % , loss : 4.892020225524902\n",
      "step : 65.64 % , loss : 4.90962028503418\n",
      "step : 66.08 % , loss : 4.951761722564697\n",
      "step : 66.52 % , loss : 4.893741607666016\n",
      "step : 66.96 % , loss : 4.7728142738342285\n",
      "step : 67.4 % , loss : 4.990256309509277\n",
      "step : 67.84 % , loss : 4.927659511566162\n",
      "step : 68.28 % , loss : 4.772143363952637\n",
      "step : 68.72 % , loss : 4.962382793426514\n",
      "step : 69.16 % , loss : 4.836926460266113\n",
      "step : 69.6 % , loss : 4.935486316680908\n",
      "step : 70.04 % , loss : 5.094346046447754\n",
      "step : 70.48 % , loss : 4.9814982414245605\n",
      "step : 70.93 % , loss : 4.8403425216674805\n",
      "step : 71.37 % , loss : 4.770569324493408\n",
      "step : 71.81 % , loss : 4.768267631530762\n",
      "step : 72.25 % , loss : 4.856866359710693\n",
      "step : 72.69 % , loss : 4.861778736114502\n",
      "step : 73.13 % , loss : 4.932431697845459\n",
      "step : 73.57 % , loss : 4.86020040512085\n",
      "step : 74.01 % , loss : 4.802684307098389\n",
      "step : 74.45 % , loss : 4.947854518890381\n",
      "step : 74.89 % , loss : 4.902182579040527\n",
      "step : 75.33 % , loss : 4.778433799743652\n",
      "step : 75.77 % , loss : 5.013534069061279\n",
      "step : 76.21 % , loss : 4.891774654388428\n",
      "step : 76.65 % , loss : 4.903046131134033\n",
      "step : 77.09 % , loss : 4.9786176681518555\n",
      "step : 77.53 % , loss : 4.807933807373047\n",
      "step : 77.97 % , loss : 4.808199405670166\n",
      "step : 78.41 % , loss : 4.974510669708252\n",
      "step : 78.85 % , loss : 4.9573845863342285\n",
      "step : 79.3 % , loss : 4.98881721496582\n",
      "step : 79.74 % , loss : 4.962869167327881\n",
      "step : 80.18 % , loss : 4.8495192527771\n",
      "step : 80.62 % , loss : 4.812649250030518\n",
      "step : 81.06 % , loss : 4.822871685028076\n",
      "step : 81.5 % , loss : 4.913452625274658\n",
      "step : 81.94 % , loss : 4.898656845092773\n",
      "step : 82.38 % , loss : 4.944653034210205\n",
      "step : 82.82 % , loss : 4.924694538116455\n",
      "step : 83.26 % , loss : 4.982358455657959\n",
      "step : 83.7 % , loss : 4.896687984466553\n",
      "step : 84.14 % , loss : 5.041686058044434\n",
      "step : 84.58 % , loss : 4.894988536834717\n",
      "step : 85.02 % , loss : 4.912717819213867\n",
      "step : 85.46 % , loss : 4.920809745788574\n",
      "step : 85.9 % , loss : 4.863231658935547\n",
      "step : 86.34 % , loss : 4.901597023010254\n",
      "step : 86.78 % , loss : 4.9252800941467285\n",
      "step : 87.22 % , loss : 4.77370023727417\n",
      "step : 87.67 % , loss : 4.871248245239258\n",
      "step : 88.11 % , loss : 4.898813724517822\n",
      "step : 88.55 % , loss : 4.9495744705200195\n",
      "step : 88.99 % , loss : 4.792198181152344\n",
      "step : 89.43 % , loss : 4.988867282867432\n",
      "step : 89.87 % , loss : 4.92842960357666\n",
      "step : 90.31 % , loss : 4.997704029083252\n",
      "step : 90.75 % , loss : 4.849754810333252\n",
      "step : 91.19 % , loss : 4.90950870513916\n",
      "step : 91.63 % , loss : 4.850802898406982\n",
      "step : 92.07 % , loss : 4.81048059463501\n",
      "step : 92.51 % , loss : 4.935920238494873\n",
      "step : 92.95 % , loss : 4.923413276672363\n",
      "step : 93.39 % , loss : 4.945927619934082\n",
      "step : 93.83 % , loss : 4.960737705230713\n",
      "step : 94.27 % , loss : 4.840836048126221\n",
      "step : 94.71 % , loss : 4.94757080078125\n",
      "step : 95.15 % , loss : 4.889732360839844\n",
      "step : 95.59 % , loss : 4.8395490646362305\n",
      "step : 96.04 % , loss : 4.908741474151611\n",
      "step : 96.48 % , loss : 4.861071586608887\n",
      "step : 96.92 % , loss : 4.998290538787842\n",
      "step : 97.36 % , loss : 4.981533527374268\n",
      "step : 97.8 % , loss : 4.912210464477539\n",
      "step : 98.24 % , loss : 4.9229841232299805\n",
      "step : 98.68 % , loss : 4.891446590423584\n",
      "step : 99.12 % , loss : 4.883610725402832\n",
      "step : 99.56 % , loss : 4.808473110198975\n",
      "\tTrain Loss: 4.916\n",
      "step : 0.0 % , loss : 4.927073001861572\n",
      "step : 0.44 % , loss : 4.89444637298584\n",
      "step : 0.88 % , loss : 4.808014869689941\n",
      "step : 1.32 % , loss : 4.969082355499268\n",
      "step : 1.76 % , loss : 4.9311394691467285\n",
      "step : 2.2 % , loss : 4.970982074737549\n",
      "step : 2.64 % , loss : 4.9793500900268555\n",
      "step : 3.08 % , loss : 4.970231056213379\n",
      "step : 3.52 % , loss : 4.901344299316406\n",
      "step : 3.96 % , loss : 4.989712715148926\n",
      "step : 4.41 % , loss : 4.882352352142334\n",
      "step : 4.85 % , loss : 4.8563127517700195\n",
      "step : 5.29 % , loss : 4.914673328399658\n",
      "step : 5.73 % , loss : 4.879752159118652\n",
      "step : 6.17 % , loss : 4.949612617492676\n",
      "step : 6.61 % , loss : 4.934180736541748\n",
      "step : 7.05 % , loss : 4.935054302215576\n",
      "step : 7.49 % , loss : 5.045497417449951\n",
      "step : 7.93 % , loss : 4.958876609802246\n",
      "step : 8.37 % , loss : 4.935332775115967\n",
      "step : 8.81 % , loss : 4.866641044616699\n",
      "step : 9.25 % , loss : 4.834939479827881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 9.69 % , loss : 5.025793075561523\n",
      "step : 10.13 % , loss : 4.8974103927612305\n",
      "step : 10.57 % , loss : 4.93107795715332\n",
      "step : 11.01 % , loss : 4.926447868347168\n",
      "step : 11.45 % , loss : 4.9900360107421875\n",
      "step : 11.89 % , loss : 4.918724536895752\n",
      "step : 12.33 % , loss : 4.87777853012085\n",
      "step : 12.78 % , loss : 4.883840560913086\n",
      "step : 13.22 % , loss : 4.91553258895874\n",
      "step : 13.66 % , loss : 4.88218879699707\n",
      "step : 14.1 % , loss : 4.864728927612305\n",
      "step : 14.54 % , loss : 4.925866603851318\n",
      "step : 14.98 % , loss : 4.917864799499512\n",
      "step : 15.42 % , loss : 4.829474449157715\n",
      "step : 15.86 % , loss : 4.906494140625\n",
      "step : 16.3 % , loss : 4.967778205871582\n",
      "step : 16.74 % , loss : 4.952324390411377\n",
      "step : 17.18 % , loss : 4.92543363571167\n",
      "step : 17.62 % , loss : 4.882214546203613\n",
      "step : 18.06 % , loss : 4.9500579833984375\n",
      "step : 18.5 % , loss : 4.86131477355957\n",
      "step : 18.94 % , loss : 4.895547389984131\n",
      "step : 19.38 % , loss : 4.913297176361084\n",
      "step : 19.82 % , loss : 4.892246723175049\n",
      "step : 20.26 % , loss : 4.898660182952881\n",
      "step : 20.7 % , loss : 4.90752649307251\n",
      "step : 21.15 % , loss : 4.864117622375488\n",
      "step : 21.59 % , loss : 4.940591335296631\n",
      "step : 22.03 % , loss : 4.939163684844971\n",
      "step : 22.47 % , loss : 4.841126918792725\n",
      "step : 22.91 % , loss : 4.9294023513793945\n",
      "step : 23.35 % , loss : 4.91981315612793\n",
      "step : 23.79 % , loss : 4.940927505493164\n",
      "step : 24.23 % , loss : 4.955328941345215\n",
      "step : 24.67 % , loss : 4.972762107849121\n",
      "step : 25.11 % , loss : 4.707543849945068\n",
      "step : 25.55 % , loss : 4.845746040344238\n",
      "step : 25.99 % , loss : 4.888643741607666\n",
      "step : 26.43 % , loss : 4.765563488006592\n",
      "step : 26.87 % , loss : 4.861556529998779\n",
      "step : 27.31 % , loss : 4.782686233520508\n",
      "step : 27.75 % , loss : 4.828371524810791\n",
      "step : 28.19 % , loss : 4.971333980560303\n",
      "step : 28.63 % , loss : 4.907800197601318\n",
      "step : 29.07 % , loss : 4.880620002746582\n",
      "step : 29.52 % , loss : 4.820775985717773\n",
      "step : 29.96 % , loss : 4.828935146331787\n",
      "step : 30.4 % , loss : 4.836970329284668\n",
      "step : 30.84 % , loss : 4.89661979675293\n",
      "step : 31.28 % , loss : 4.926610469818115\n",
      "step : 31.72 % , loss : 4.87660551071167\n",
      "step : 32.16 % , loss : 4.8274383544921875\n",
      "step : 32.6 % , loss : 4.994202613830566\n",
      "step : 33.04 % , loss : 4.772200107574463\n",
      "step : 33.48 % , loss : 5.006946563720703\n",
      "step : 33.92 % , loss : 4.869335651397705\n",
      "step : 34.36 % , loss : 4.8566083908081055\n",
      "step : 34.8 % , loss : 4.931995391845703\n",
      "step : 35.24 % , loss : 4.936967849731445\n",
      "step : 35.68 % , loss : 4.865665435791016\n",
      "step : 36.12 % , loss : 4.892277717590332\n",
      "step : 36.56 % , loss : 4.855198383331299\n",
      "step : 37.0 % , loss : 4.9121928215026855\n",
      "step : 37.44 % , loss : 4.933887958526611\n",
      "step : 37.89 % , loss : 5.005417346954346\n",
      "step : 38.33 % , loss : 4.899547100067139\n",
      "step : 38.77 % , loss : 4.870946407318115\n",
      "step : 39.21 % , loss : 4.8816704750061035\n",
      "step : 39.65 % , loss : 4.95650053024292\n",
      "step : 40.09 % , loss : 4.79622220993042\n",
      "step : 40.53 % , loss : 5.017797470092773\n",
      "step : 40.97 % , loss : 4.98642635345459\n",
      "step : 41.41 % , loss : 4.985650539398193\n",
      "step : 41.85 % , loss : 4.833019256591797\n",
      "step : 42.29 % , loss : 5.015514850616455\n",
      "step : 42.73 % , loss : 4.879854679107666\n",
      "step : 43.17 % , loss : 4.799834251403809\n",
      "step : 43.61 % , loss : 4.935451507568359\n",
      "step : 44.05 % , loss : 4.868721008300781\n",
      "step : 44.49 % , loss : 4.864341735839844\n",
      "step : 44.93 % , loss : 4.960700988769531\n",
      "step : 45.37 % , loss : 4.781223297119141\n",
      "step : 45.81 % , loss : 4.854315757751465\n",
      "step : 46.26 % , loss : 4.866298675537109\n",
      "step : 46.7 % , loss : 4.973949909210205\n",
      "step : 47.14 % , loss : 4.938658237457275\n",
      "step : 47.58 % , loss : 4.863648414611816\n",
      "step : 48.02 % , loss : 4.85403299331665\n",
      "step : 48.46 % , loss : 4.821304798126221\n",
      "step : 48.9 % , loss : 4.8287739753723145\n",
      "step : 49.34 % , loss : 4.765501976013184\n",
      "step : 49.78 % , loss : 4.850361347198486\n",
      "step : 50.22 % , loss : 4.9311323165893555\n",
      "step : 50.66 % , loss : 4.939574241638184\n",
      "step : 51.1 % , loss : 4.907635688781738\n",
      "step : 51.54 % , loss : 4.868784427642822\n",
      "step : 51.98 % , loss : 4.785892486572266\n",
      "step : 52.42 % , loss : 4.955923557281494\n",
      "step : 52.86 % , loss : 4.825498580932617\n",
      "step : 53.3 % , loss : 4.987428188323975\n",
      "step : 53.74 % , loss : 4.872697830200195\n",
      "step : 54.19 % , loss : 4.95869255065918\n",
      "step : 54.63 % , loss : 4.93820333480835\n",
      "step : 55.07 % , loss : 4.84250545501709\n",
      "step : 55.51 % , loss : 4.792533874511719\n",
      "step : 55.95 % , loss : 4.942929744720459\n",
      "step : 56.39 % , loss : 4.7869038581848145\n",
      "step : 56.83 % , loss : 4.877418518066406\n",
      "step : 57.27 % , loss : 4.7328996658325195\n",
      "step : 57.71 % , loss : 4.989646911621094\n",
      "step : 58.15 % , loss : 4.914987087249756\n",
      "step : 58.59 % , loss : 4.865575790405273\n",
      "step : 59.03 % , loss : 4.888028144836426\n",
      "step : 59.47 % , loss : 4.836756706237793\n",
      "step : 59.91 % , loss : 4.982600688934326\n",
      "step : 60.35 % , loss : 4.908331394195557\n",
      "step : 60.79 % , loss : 4.967720031738281\n",
      "step : 61.23 % , loss : 4.789806365966797\n",
      "step : 61.67 % , loss : 4.877685070037842\n",
      "step : 62.11 % , loss : 4.8077239990234375\n",
      "step : 62.56 % , loss : 4.8913984298706055\n",
      "step : 63.0 % , loss : 4.871212005615234\n",
      "step : 63.44 % , loss : 5.092308044433594\n",
      "step : 63.88 % , loss : 4.8728227615356445\n",
      "step : 64.32 % , loss : 4.909371852874756\n",
      "step : 64.76 % , loss : 4.894202709197998\n",
      "step : 65.2 % , loss : 4.807168960571289\n",
      "step : 65.64 % , loss : 4.8298187255859375\n",
      "step : 66.08 % , loss : 4.960010051727295\n",
      "step : 66.52 % , loss : 4.882271766662598\n",
      "step : 66.96 % , loss : 4.833773136138916\n",
      "step : 67.4 % , loss : 4.951794624328613\n",
      "step : 67.84 % , loss : 4.807258129119873\n",
      "step : 68.28 % , loss : 4.9405951499938965\n",
      "step : 68.72 % , loss : 4.8691558837890625\n",
      "step : 69.16 % , loss : 5.00377893447876\n",
      "step : 69.6 % , loss : 4.971964359283447\n",
      "step : 70.04 % , loss : 4.829690456390381\n",
      "step : 70.48 % , loss : 4.982429504394531\n",
      "step : 70.93 % , loss : 4.9302167892456055\n",
      "step : 71.37 % , loss : 4.902699947357178\n",
      "step : 71.81 % , loss : 4.821741580963135\n",
      "step : 72.25 % , loss : 4.877287864685059\n",
      "step : 72.69 % , loss : 4.981707572937012\n",
      "step : 73.13 % , loss : 4.775088310241699\n",
      "step : 73.57 % , loss : 4.8819403648376465\n",
      "step : 74.01 % , loss : 4.937341690063477\n",
      "step : 74.45 % , loss : 4.865617275238037\n",
      "step : 74.89 % , loss : 4.905497074127197\n",
      "step : 75.33 % , loss : 5.010735034942627\n",
      "step : 75.77 % , loss : 4.9156365394592285\n",
      "step : 76.21 % , loss : 4.85450553894043\n",
      "step : 76.65 % , loss : 4.976680278778076\n",
      "step : 77.09 % , loss : 5.072510242462158\n",
      "step : 77.53 % , loss : 4.753266334533691\n",
      "step : 77.97 % , loss : 4.899447441101074\n",
      "step : 78.41 % , loss : 4.885171413421631\n",
      "step : 78.85 % , loss : 4.8297553062438965\n",
      "step : 79.3 % , loss : 4.821716785430908\n",
      "step : 79.74 % , loss : 4.783265590667725\n",
      "step : 80.18 % , loss : 4.859645366668701\n",
      "step : 80.62 % , loss : 4.93520450592041\n",
      "step : 81.06 % , loss : 4.918278694152832\n",
      "step : 81.5 % , loss : 4.857914924621582\n",
      "step : 81.94 % , loss : 4.943734169006348\n",
      "step : 82.38 % , loss : 4.824912071228027\n",
      "step : 82.82 % , loss : 4.902698040008545\n",
      "step : 83.26 % , loss : 4.881004333496094\n",
      "step : 83.7 % , loss : 4.961127281188965\n",
      "step : 84.14 % , loss : 4.8451104164123535\n",
      "step : 84.58 % , loss : 4.910927772521973\n",
      "step : 85.02 % , loss : 4.874966144561768\n",
      "step : 85.46 % , loss : 4.932246208190918\n",
      "step : 85.9 % , loss : 4.885430335998535\n",
      "step : 86.34 % , loss : 4.79409122467041\n",
      "step : 86.78 % , loss : 4.804446220397949\n",
      "step : 87.22 % , loss : 5.003307819366455\n",
      "step : 87.67 % , loss : 4.8995680809021\n",
      "step : 88.11 % , loss : 4.846810817718506\n",
      "step : 88.55 % , loss : 4.910895347595215\n",
      "step : 88.99 % , loss : 4.905559062957764\n",
      "step : 89.43 % , loss : 4.8879499435424805\n",
      "step : 89.87 % , loss : 4.856355667114258\n",
      "step : 90.31 % , loss : 4.888769626617432\n",
      "step : 90.75 % , loss : 4.8789591789245605\n",
      "step : 91.19 % , loss : 4.830608367919922\n",
      "step : 91.63 % , loss : 4.823399543762207\n",
      "step : 92.07 % , loss : 4.965944766998291\n",
      "step : 92.51 % , loss : 4.932999610900879\n",
      "step : 92.95 % , loss : 4.825793266296387\n",
      "step : 93.39 % , loss : 4.906005382537842\n",
      "step : 93.83 % , loss : 4.98849630355835\n",
      "step : 94.27 % , loss : 5.046799659729004\n",
      "step : 94.71 % , loss : 4.835543632507324\n",
      "step : 95.15 % , loss : 4.874955177307129\n",
      "step : 95.59 % , loss : 4.707640171051025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 96.04 % , loss : 4.8435869216918945\n",
      "step : 96.48 % , loss : 4.853485584259033\n",
      "step : 96.92 % , loss : 4.9236159324646\n",
      "step : 97.36 % , loss : 4.81754732131958\n",
      "step : 97.8 % , loss : 4.988285064697266\n",
      "step : 98.24 % , loss : 4.846158027648926\n",
      "step : 98.68 % , loss : 4.9607367515563965\n",
      "step : 99.12 % , loss : 4.941893577575684\n",
      "step : 99.56 % , loss : 4.752861976623535\n",
      "\tTrain Loss: 4.895\n",
      "step : 0.0 % , loss : 4.840977668762207\n",
      "step : 0.44 % , loss : 4.950919151306152\n",
      "step : 0.88 % , loss : 4.928229331970215\n",
      "step : 1.32 % , loss : 4.857216835021973\n",
      "step : 1.76 % , loss : 4.928187370300293\n",
      "step : 2.2 % , loss : 4.85783052444458\n",
      "step : 2.64 % , loss : 4.730561256408691\n",
      "step : 3.08 % , loss : 4.931110858917236\n",
      "step : 3.52 % , loss : 4.856010913848877\n",
      "step : 3.96 % , loss : 4.847874641418457\n",
      "step : 4.41 % , loss : 4.856167316436768\n",
      "step : 4.85 % , loss : 4.774860382080078\n",
      "step : 5.29 % , loss : 4.862550258636475\n",
      "step : 5.73 % , loss : 4.896691799163818\n",
      "step : 6.17 % , loss : 4.921788215637207\n",
      "step : 6.61 % , loss : 4.960079669952393\n",
      "step : 7.05 % , loss : 4.87628698348999\n",
      "step : 7.49 % , loss : 4.872267246246338\n",
      "step : 7.93 % , loss : 4.905923843383789\n",
      "step : 8.37 % , loss : 4.9059672355651855\n",
      "step : 8.81 % , loss : 4.814259052276611\n",
      "step : 9.25 % , loss : 4.8511834144592285\n",
      "step : 9.69 % , loss : 4.901991844177246\n",
      "step : 10.13 % , loss : 4.982057094573975\n",
      "step : 10.57 % , loss : 4.837111473083496\n",
      "step : 11.01 % , loss : 4.78190279006958\n",
      "step : 11.45 % , loss : 4.81283712387085\n",
      "step : 11.89 % , loss : 4.834661960601807\n",
      "step : 12.33 % , loss : 5.010700702667236\n",
      "step : 12.78 % , loss : 4.932492733001709\n",
      "step : 13.22 % , loss : 4.919663906097412\n",
      "step : 13.66 % , loss : 4.827810764312744\n",
      "step : 14.1 % , loss : 4.857336521148682\n",
      "step : 14.54 % , loss : 4.831286430358887\n",
      "step : 14.98 % , loss : 4.843815803527832\n",
      "step : 15.42 % , loss : 4.818115234375\n",
      "step : 15.86 % , loss : 4.878517150878906\n",
      "step : 16.3 % , loss : 4.832190990447998\n",
      "step : 16.74 % , loss : 4.907491207122803\n",
      "step : 17.18 % , loss : 4.936599254608154\n",
      "step : 17.62 % , loss : 4.856055736541748\n",
      "step : 18.06 % , loss : 4.896962642669678\n",
      "step : 18.5 % , loss : 4.916387557983398\n",
      "step : 18.94 % , loss : 4.893082618713379\n",
      "step : 19.38 % , loss : 4.8368000984191895\n",
      "step : 19.82 % , loss : 4.854240894317627\n",
      "step : 20.26 % , loss : 4.835222244262695\n",
      "step : 20.7 % , loss : 4.886595249176025\n",
      "step : 21.15 % , loss : 4.8155622482299805\n",
      "step : 21.59 % , loss : 4.8470458984375\n",
      "step : 22.03 % , loss : 4.9044013023376465\n",
      "step : 22.47 % , loss : 4.813304901123047\n",
      "step : 22.91 % , loss : 4.754203796386719\n",
      "step : 23.35 % , loss : 4.902646541595459\n",
      "step : 23.79 % , loss : 4.961461544036865\n",
      "step : 24.23 % , loss : 4.903197765350342\n",
      "step : 24.67 % , loss : 4.907296180725098\n",
      "step : 25.11 % , loss : 4.856228828430176\n",
      "step : 25.55 % , loss : 4.899230003356934\n",
      "step : 25.99 % , loss : 4.960072994232178\n",
      "step : 26.43 % , loss : 4.929232597351074\n",
      "step : 26.87 % , loss : 4.852219104766846\n",
      "step : 27.31 % , loss : 4.912866115570068\n",
      "step : 27.75 % , loss : 4.843994617462158\n",
      "step : 28.19 % , loss : 4.883833408355713\n",
      "step : 28.63 % , loss : 4.855051040649414\n",
      "step : 29.07 % , loss : 4.9341607093811035\n",
      "step : 29.52 % , loss : 4.85115385055542\n",
      "step : 29.96 % , loss : 4.900370121002197\n",
      "step : 30.4 % , loss : 4.9212212562561035\n",
      "step : 30.84 % , loss : 4.928920269012451\n",
      "step : 31.28 % , loss : 4.891242504119873\n",
      "step : 31.72 % , loss : 4.846817493438721\n",
      "step : 32.16 % , loss : 4.870381832122803\n",
      "step : 32.6 % , loss : 4.736963748931885\n",
      "step : 33.04 % , loss : 4.994387149810791\n",
      "step : 33.48 % , loss : 4.924248218536377\n",
      "step : 33.92 % , loss : 4.927907943725586\n",
      "step : 34.36 % , loss : 4.940068244934082\n",
      "step : 34.8 % , loss : 4.898612976074219\n",
      "step : 35.24 % , loss : 4.911179065704346\n",
      "step : 35.68 % , loss : 4.9753594398498535\n",
      "step : 36.12 % , loss : 4.841838359832764\n",
      "step : 36.56 % , loss : 4.943629264831543\n",
      "step : 37.0 % , loss : 5.014993667602539\n",
      "step : 37.44 % , loss : 4.947795391082764\n",
      "step : 37.89 % , loss : 4.8693928718566895\n",
      "step : 38.33 % , loss : 4.901484966278076\n",
      "step : 38.77 % , loss : 4.838220596313477\n",
      "step : 39.21 % , loss : 4.873636245727539\n",
      "step : 39.65 % , loss : 4.964545249938965\n",
      "step : 40.09 % , loss : 4.80280876159668\n",
      "step : 40.53 % , loss : 4.961755752563477\n",
      "step : 40.97 % , loss : 4.752978324890137\n",
      "step : 41.41 % , loss : 4.878642559051514\n",
      "step : 41.85 % , loss : 4.891529560089111\n",
      "step : 42.29 % , loss : 4.872920036315918\n",
      "step : 42.73 % , loss : 4.867444038391113\n",
      "step : 43.17 % , loss : 4.9407501220703125\n",
      "step : 43.61 % , loss : 4.830933570861816\n",
      "step : 44.05 % , loss : 4.824625492095947\n",
      "step : 44.49 % , loss : 4.927135944366455\n",
      "step : 44.93 % , loss : 4.900110721588135\n",
      "step : 45.37 % , loss : 4.769899368286133\n",
      "step : 45.81 % , loss : 4.8990159034729\n",
      "step : 46.26 % , loss : 4.875890731811523\n",
      "step : 46.7 % , loss : 4.95399284362793\n",
      "step : 47.14 % , loss : 4.791159629821777\n",
      "step : 47.58 % , loss : 4.882422924041748\n",
      "step : 48.02 % , loss : 4.9112396240234375\n",
      "step : 48.46 % , loss : 4.879325866699219\n",
      "step : 48.9 % , loss : 4.871194362640381\n",
      "step : 49.34 % , loss : 4.86469030380249\n",
      "step : 49.78 % , loss : 4.861207008361816\n",
      "step : 50.22 % , loss : 4.842404842376709\n",
      "step : 50.66 % , loss : 4.9281158447265625\n",
      "step : 51.1 % , loss : 4.89745569229126\n",
      "step : 51.54 % , loss : 4.927567481994629\n",
      "step : 51.98 % , loss : 4.810286521911621\n",
      "step : 52.42 % , loss : 4.939975738525391\n",
      "step : 52.86 % , loss : 4.877426624298096\n",
      "step : 53.3 % , loss : 4.84380578994751\n",
      "step : 53.74 % , loss : 4.7871270179748535\n",
      "step : 54.19 % , loss : 4.868659496307373\n",
      "step : 54.63 % , loss : 4.909261703491211\n",
      "step : 55.07 % , loss : 4.943343639373779\n",
      "step : 55.51 % , loss : 4.808932781219482\n",
      "step : 55.95 % , loss : 4.738662242889404\n",
      "step : 56.39 % , loss : 4.828451633453369\n",
      "step : 56.83 % , loss : 4.82778263092041\n",
      "step : 57.27 % , loss : 4.921968936920166\n",
      "step : 57.71 % , loss : 4.867386817932129\n",
      "step : 58.15 % , loss : 4.8436102867126465\n",
      "step : 58.59 % , loss : 4.790126323699951\n",
      "step : 59.03 % , loss : 4.903384208679199\n",
      "step : 59.47 % , loss : 5.0064215660095215\n",
      "step : 59.91 % , loss : 4.835268020629883\n",
      "step : 60.35 % , loss : 4.910728454589844\n",
      "step : 60.79 % , loss : 4.886503219604492\n",
      "step : 61.23 % , loss : 4.926677227020264\n",
      "step : 61.67 % , loss : 4.758296012878418\n",
      "step : 62.11 % , loss : 4.960293769836426\n",
      "step : 62.56 % , loss : 4.938268184661865\n",
      "step : 63.0 % , loss : 4.809591293334961\n",
      "step : 63.44 % , loss : 4.876828193664551\n",
      "step : 63.88 % , loss : 4.877678394317627\n",
      "step : 64.32 % , loss : 4.773809909820557\n",
      "step : 64.76 % , loss : 4.978906154632568\n",
      "step : 65.2 % , loss : 4.938886642456055\n",
      "step : 65.64 % , loss : 4.9058146476745605\n",
      "step : 66.08 % , loss : 4.901355743408203\n",
      "step : 66.52 % , loss : 4.892975807189941\n",
      "step : 66.96 % , loss : 4.949970245361328\n",
      "step : 67.4 % , loss : 4.899031162261963\n",
      "step : 67.84 % , loss : 4.86481237411499\n",
      "step : 68.28 % , loss : 4.821178913116455\n",
      "step : 68.72 % , loss : 4.831855297088623\n",
      "step : 69.16 % , loss : 4.802927494049072\n",
      "step : 69.6 % , loss : 4.836511611938477\n",
      "step : 70.04 % , loss : 4.940349578857422\n",
      "step : 70.48 % , loss : 4.8856072425842285\n",
      "step : 70.93 % , loss : 4.869570732116699\n",
      "step : 71.37 % , loss : 4.930546760559082\n",
      "step : 71.81 % , loss : 4.7642998695373535\n",
      "step : 72.25 % , loss : 4.8941731452941895\n",
      "step : 72.69 % , loss : 4.801805019378662\n",
      "step : 73.13 % , loss : 4.921009063720703\n",
      "step : 73.57 % , loss : 4.933001518249512\n",
      "step : 74.01 % , loss : 4.94195032119751\n",
      "step : 74.45 % , loss : 4.912612438201904\n",
      "step : 74.89 % , loss : 4.818222522735596\n",
      "step : 75.33 % , loss : 4.8530449867248535\n",
      "step : 75.77 % , loss : 4.977363109588623\n",
      "step : 76.21 % , loss : 4.873459339141846\n",
      "step : 76.65 % , loss : 4.801334381103516\n",
      "step : 77.09 % , loss : 4.879125595092773\n",
      "step : 77.53 % , loss : 4.865577220916748\n",
      "step : 77.97 % , loss : 4.91370964050293\n",
      "step : 78.41 % , loss : 4.848866939544678\n",
      "step : 78.85 % , loss : 4.806763172149658\n",
      "step : 79.3 % , loss : 4.809475898742676\n",
      "step : 79.74 % , loss : 4.887441635131836\n",
      "step : 80.18 % , loss : 4.809790134429932\n",
      "step : 80.62 % , loss : 4.822542667388916\n",
      "step : 81.06 % , loss : 4.828044414520264\n",
      "step : 81.5 % , loss : 4.898105144500732\n",
      "step : 81.94 % , loss : 4.87665319442749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 82.38 % , loss : 4.828501224517822\n",
      "step : 82.82 % , loss : 4.844749927520752\n",
      "step : 83.26 % , loss : 4.75948429107666\n",
      "step : 83.7 % , loss : 4.931096076965332\n",
      "step : 84.14 % , loss : 4.762701988220215\n",
      "step : 84.58 % , loss : 4.910794258117676\n",
      "step : 85.02 % , loss : 4.85793399810791\n",
      "step : 85.46 % , loss : 4.9486308097839355\n",
      "step : 85.9 % , loss : 4.872363090515137\n",
      "step : 86.34 % , loss : 4.925178527832031\n",
      "step : 86.78 % , loss : 4.857644081115723\n",
      "step : 87.22 % , loss : 4.943312644958496\n",
      "step : 87.67 % , loss : 4.8772430419921875\n",
      "step : 88.11 % , loss : 4.948180198669434\n",
      "step : 88.55 % , loss : 4.941559791564941\n",
      "step : 88.99 % , loss : 4.8845601081848145\n",
      "step : 89.43 % , loss : 4.95602560043335\n",
      "step : 89.87 % , loss : 4.909693717956543\n",
      "step : 90.31 % , loss : 4.772228240966797\n",
      "step : 90.75 % , loss : 4.803237438201904\n",
      "step : 91.19 % , loss : 4.829371452331543\n",
      "step : 91.63 % , loss : 4.957765579223633\n",
      "step : 92.07 % , loss : 4.8414177894592285\n",
      "step : 92.51 % , loss : 4.9057416915893555\n",
      "step : 92.95 % , loss : 4.8124518394470215\n",
      "step : 93.39 % , loss : 4.970413684844971\n",
      "step : 93.83 % , loss : 4.815654754638672\n",
      "step : 94.27 % , loss : 4.821120738983154\n",
      "step : 94.71 % , loss : 4.823136329650879\n",
      "step : 95.15 % , loss : 4.849189281463623\n",
      "step : 95.59 % , loss : 4.839456558227539\n",
      "step : 96.04 % , loss : 4.814498424530029\n",
      "step : 96.48 % , loss : 4.899372577667236\n",
      "step : 96.92 % , loss : 4.96338415145874\n",
      "step : 97.36 % , loss : 4.87965726852417\n",
      "step : 97.8 % , loss : 4.737438201904297\n",
      "step : 98.24 % , loss : 4.913497447967529\n",
      "step : 98.68 % , loss : 4.818115234375\n",
      "step : 99.12 % , loss : 4.8316192626953125\n",
      "step : 99.56 % , loss : 4.7571892738342285\n",
      "\tTrain Loss: 4.875\n",
      "step : 0.0 % , loss : 4.871016502380371\n",
      "step : 0.44 % , loss : 4.8173747062683105\n",
      "step : 0.88 % , loss : 4.847903251647949\n",
      "step : 1.32 % , loss : 4.822799205780029\n",
      "step : 1.76 % , loss : 4.926053047180176\n",
      "step : 2.2 % , loss : 4.823791980743408\n",
      "step : 2.64 % , loss : 4.909093379974365\n",
      "step : 3.08 % , loss : 4.9160895347595215\n",
      "step : 3.52 % , loss : 4.9723801612854\n",
      "step : 3.96 % , loss : 4.976210117340088\n",
      "step : 4.41 % , loss : 5.003814697265625\n",
      "step : 4.85 % , loss : 4.811953067779541\n",
      "step : 5.29 % , loss : 4.798742771148682\n",
      "step : 5.73 % , loss : 4.915172576904297\n",
      "step : 6.17 % , loss : 4.829487323760986\n",
      "step : 6.61 % , loss : 4.891794204711914\n",
      "step : 7.05 % , loss : 4.936807155609131\n",
      "step : 7.49 % , loss : 4.849790096282959\n",
      "step : 7.93 % , loss : 4.827525615692139\n",
      "step : 8.37 % , loss : 4.850342750549316\n",
      "step : 8.81 % , loss : 4.802219867706299\n",
      "step : 9.25 % , loss : 4.797686576843262\n",
      "step : 9.69 % , loss : 4.935466766357422\n",
      "step : 10.13 % , loss : 4.8321685791015625\n",
      "step : 10.57 % , loss : 4.84417724609375\n",
      "step : 11.01 % , loss : 4.866862773895264\n",
      "step : 11.45 % , loss : 4.843083381652832\n",
      "step : 11.89 % , loss : 4.86397647857666\n",
      "step : 12.33 % , loss : 4.872585296630859\n",
      "step : 12.78 % , loss : 4.846802234649658\n",
      "step : 13.22 % , loss : 4.920759201049805\n",
      "step : 13.66 % , loss : 4.889016628265381\n",
      "step : 14.1 % , loss : 4.802439212799072\n",
      "step : 14.54 % , loss : 4.834469318389893\n",
      "step : 14.98 % , loss : 4.851785659790039\n",
      "step : 15.42 % , loss : 4.8132710456848145\n",
      "step : 15.86 % , loss : 4.777174472808838\n",
      "step : 16.3 % , loss : 4.900952339172363\n",
      "step : 16.74 % , loss : 4.851812362670898\n",
      "step : 17.18 % , loss : 4.882376194000244\n",
      "step : 17.62 % , loss : 4.918047904968262\n",
      "step : 18.06 % , loss : 4.854191780090332\n",
      "step : 18.5 % , loss : 4.942119121551514\n",
      "step : 18.94 % , loss : 4.972751617431641\n",
      "step : 19.38 % , loss : 4.806946277618408\n",
      "step : 19.82 % , loss : 4.864022254943848\n",
      "step : 20.26 % , loss : 4.7894368171691895\n",
      "step : 20.7 % , loss : 4.883160591125488\n",
      "step : 21.15 % , loss : 4.801774501800537\n",
      "step : 21.59 % , loss : 4.814869403839111\n",
      "step : 22.03 % , loss : 4.796756744384766\n",
      "step : 22.47 % , loss : 4.713235855102539\n",
      "step : 22.91 % , loss : 4.752470970153809\n",
      "step : 23.35 % , loss : 4.779362678527832\n",
      "step : 23.79 % , loss : 4.7115864753723145\n",
      "step : 24.23 % , loss : 4.980020046234131\n",
      "step : 24.67 % , loss : 4.770381927490234\n",
      "step : 25.11 % , loss : 4.7310943603515625\n",
      "step : 25.55 % , loss : 4.886776447296143\n",
      "step : 25.99 % , loss : 4.894831657409668\n",
      "step : 26.43 % , loss : 4.920833587646484\n",
      "step : 26.87 % , loss : 5.006489276885986\n",
      "step : 27.31 % , loss : 4.895782470703125\n",
      "step : 27.75 % , loss : 4.916700839996338\n",
      "step : 28.19 % , loss : 4.847864627838135\n",
      "step : 28.63 % , loss : 4.805014133453369\n",
      "step : 29.07 % , loss : 4.692884922027588\n",
      "step : 29.52 % , loss : 4.928547382354736\n",
      "step : 29.96 % , loss : 4.882379531860352\n",
      "step : 30.4 % , loss : 4.823493957519531\n",
      "step : 30.84 % , loss : 4.818930625915527\n",
      "step : 31.28 % , loss : 4.794322490692139\n",
      "step : 31.72 % , loss : 4.769476413726807\n",
      "step : 32.16 % , loss : 4.8788676261901855\n",
      "step : 32.6 % , loss : 4.765340328216553\n",
      "step : 33.04 % , loss : 4.9320878982543945\n",
      "step : 33.48 % , loss : 4.888933181762695\n",
      "step : 33.92 % , loss : 4.905709743499756\n",
      "step : 34.36 % , loss : 4.827417850494385\n",
      "step : 34.8 % , loss : 5.007628917694092\n",
      "step : 35.24 % , loss : 4.6969428062438965\n",
      "step : 35.68 % , loss : 4.89549446105957\n",
      "step : 36.12 % , loss : 4.942279815673828\n",
      "step : 36.56 % , loss : 4.841361999511719\n",
      "step : 37.0 % , loss : 4.913846969604492\n",
      "step : 37.44 % , loss : 4.924646854400635\n",
      "step : 37.89 % , loss : 4.851926803588867\n",
      "step : 38.33 % , loss : 4.873942852020264\n",
      "step : 38.77 % , loss : 4.84501838684082\n",
      "step : 39.21 % , loss : 4.868484020233154\n",
      "step : 39.65 % , loss : 4.873823165893555\n",
      "step : 40.09 % , loss : 4.917342662811279\n",
      "step : 40.53 % , loss : 4.763505458831787\n",
      "step : 40.97 % , loss : 4.85730504989624\n",
      "step : 41.41 % , loss : 4.801203727722168\n",
      "step : 41.85 % , loss : 4.883527755737305\n",
      "step : 42.29 % , loss : 4.842463970184326\n",
      "step : 42.73 % , loss : 4.844265937805176\n",
      "step : 43.17 % , loss : 4.889902591705322\n",
      "step : 43.61 % , loss : 4.981613636016846\n",
      "step : 44.05 % , loss : 4.768802165985107\n",
      "step : 44.49 % , loss : 4.839318752288818\n",
      "step : 44.93 % , loss : 4.879793167114258\n",
      "step : 45.37 % , loss : 4.99233341217041\n",
      "step : 45.81 % , loss : 4.85500955581665\n",
      "step : 46.26 % , loss : 4.876936435699463\n",
      "step : 46.7 % , loss : 4.8275017738342285\n",
      "step : 47.14 % , loss : 4.856066703796387\n",
      "step : 47.58 % , loss : 4.893901824951172\n",
      "step : 48.02 % , loss : 4.799131870269775\n",
      "step : 48.46 % , loss : 4.804044723510742\n",
      "step : 48.9 % , loss : 4.749145984649658\n",
      "step : 49.34 % , loss : 4.9445037841796875\n",
      "step : 49.78 % , loss : 4.850465774536133\n",
      "step : 50.22 % , loss : 4.940252304077148\n",
      "step : 50.66 % , loss : 4.970885753631592\n",
      "step : 51.1 % , loss : 4.847372055053711\n",
      "step : 51.54 % , loss : 4.722843170166016\n",
      "step : 51.98 % , loss : 4.819820880889893\n",
      "step : 52.42 % , loss : 4.774090766906738\n",
      "step : 52.86 % , loss : 4.932497501373291\n",
      "step : 53.3 % , loss : 4.8822503089904785\n",
      "step : 53.74 % , loss : 4.696187973022461\n",
      "step : 54.19 % , loss : 4.948234558105469\n",
      "step : 54.63 % , loss : 4.942939281463623\n",
      "step : 55.07 % , loss : 4.934474945068359\n",
      "step : 55.51 % , loss : 4.859504222869873\n",
      "step : 55.95 % , loss : 4.892965793609619\n",
      "step : 56.39 % , loss : 4.819622039794922\n",
      "step : 56.83 % , loss : 4.841794013977051\n",
      "step : 57.27 % , loss : 4.861348628997803\n",
      "step : 57.71 % , loss : 4.778817653656006\n",
      "step : 58.15 % , loss : 4.808290958404541\n",
      "step : 58.59 % , loss : 4.82313346862793\n",
      "step : 59.03 % , loss : 4.864373683929443\n",
      "step : 59.47 % , loss : 4.957671165466309\n",
      "step : 59.91 % , loss : 4.816746711730957\n",
      "step : 60.35 % , loss : 4.818106651306152\n",
      "step : 60.79 % , loss : 4.8172407150268555\n",
      "step : 61.23 % , loss : 4.847053527832031\n",
      "step : 61.67 % , loss : 4.903186798095703\n",
      "step : 62.11 % , loss : 4.849454402923584\n",
      "step : 62.56 % , loss : 4.763895511627197\n",
      "step : 63.0 % , loss : 4.792200565338135\n",
      "step : 63.44 % , loss : 4.790808200836182\n",
      "step : 63.88 % , loss : 4.915820598602295\n",
      "step : 64.32 % , loss : 4.789670944213867\n",
      "step : 64.76 % , loss : 4.761660099029541\n",
      "step : 65.2 % , loss : 4.830353736877441\n",
      "step : 65.64 % , loss : 4.870753288269043\n",
      "step : 66.08 % , loss : 4.782321929931641\n",
      "step : 66.52 % , loss : 4.732938289642334\n",
      "step : 66.96 % , loss : 4.936243057250977\n",
      "step : 67.4 % , loss : 4.9589619636535645\n",
      "step : 67.84 % , loss : 4.909266471862793\n",
      "step : 68.28 % , loss : 5.003734111785889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 68.72 % , loss : 4.824273109436035\n",
      "step : 69.16 % , loss : 4.879471778869629\n",
      "step : 69.6 % , loss : 4.892272472381592\n",
      "step : 70.04 % , loss : 4.903055191040039\n",
      "step : 70.48 % , loss : 4.825155735015869\n",
      "step : 70.93 % , loss : 4.836333751678467\n",
      "step : 71.37 % , loss : 4.722871780395508\n",
      "step : 71.81 % , loss : 4.868553161621094\n",
      "step : 72.25 % , loss : 4.828563213348389\n",
      "step : 72.69 % , loss : 4.851286888122559\n",
      "step : 73.13 % , loss : 4.937962055206299\n",
      "step : 73.57 % , loss : 4.777480602264404\n",
      "step : 74.01 % , loss : 4.905638217926025\n",
      "step : 74.45 % , loss : 4.81693172454834\n",
      "step : 74.89 % , loss : 4.789332389831543\n",
      "step : 75.33 % , loss : 4.794468402862549\n",
      "step : 75.77 % , loss : 4.8556342124938965\n",
      "step : 76.21 % , loss : 4.904207229614258\n",
      "step : 76.65 % , loss : 4.870615482330322\n",
      "step : 77.09 % , loss : 4.936858654022217\n",
      "step : 77.53 % , loss : 4.900258541107178\n",
      "step : 77.97 % , loss : 4.76442289352417\n",
      "step : 78.41 % , loss : 4.802104473114014\n",
      "step : 78.85 % , loss : 4.777596473693848\n",
      "step : 79.3 % , loss : 4.845755577087402\n",
      "step : 79.74 % , loss : 4.826168537139893\n",
      "step : 80.18 % , loss : 4.860617637634277\n",
      "step : 80.62 % , loss : 4.778848171234131\n",
      "step : 81.06 % , loss : 4.889028072357178\n",
      "step : 81.5 % , loss : 4.892580032348633\n",
      "step : 81.94 % , loss : 5.0391845703125\n",
      "step : 82.38 % , loss : 4.906065464019775\n",
      "step : 82.82 % , loss : 4.917798042297363\n",
      "step : 83.26 % , loss : 4.821750640869141\n",
      "step : 83.7 % , loss : 5.001372337341309\n",
      "step : 84.14 % , loss : 4.751081943511963\n",
      "step : 84.58 % , loss : 4.742997646331787\n",
      "step : 85.02 % , loss : 4.958679676055908\n",
      "step : 85.46 % , loss : 4.775310039520264\n",
      "step : 85.9 % , loss : 4.905238151550293\n",
      "step : 86.34 % , loss : 4.9602952003479\n",
      "step : 86.78 % , loss : 4.804417610168457\n",
      "step : 87.22 % , loss : 4.860572338104248\n",
      "step : 87.67 % , loss : 4.793676376342773\n",
      "step : 88.11 % , loss : 4.810861110687256\n",
      "step : 88.55 % , loss : 4.782500743865967\n",
      "step : 88.99 % , loss : 4.818374156951904\n",
      "step : 89.43 % , loss : 4.824655532836914\n",
      "step : 89.87 % , loss : 4.820481777191162\n",
      "step : 90.31 % , loss : 4.9146623611450195\n",
      "step : 90.75 % , loss : 4.7200212478637695\n",
      "step : 91.19 % , loss : 4.891970157623291\n",
      "step : 91.63 % , loss : 4.821915626525879\n",
      "step : 92.07 % , loss : 4.828428745269775\n",
      "step : 92.51 % , loss : 4.940345287322998\n",
      "step : 92.95 % , loss : 4.730220794677734\n",
      "step : 93.39 % , loss : 4.889424800872803\n",
      "step : 93.83 % , loss : 4.852442741394043\n",
      "step : 94.27 % , loss : 4.903558254241943\n",
      "step : 94.71 % , loss : 4.843838691711426\n",
      "step : 95.15 % , loss : 4.784902572631836\n",
      "step : 95.59 % , loss : 4.894791603088379\n",
      "step : 96.04 % , loss : 4.802975177764893\n",
      "step : 96.48 % , loss : 5.030571937561035\n",
      "step : 96.92 % , loss : 4.975480079650879\n",
      "step : 97.36 % , loss : 4.7777419090271\n",
      "step : 97.8 % , loss : 4.793948173522949\n",
      "step : 98.24 % , loss : 4.874233722686768\n",
      "step : 98.68 % , loss : 4.873745918273926\n",
      "step : 99.12 % , loss : 4.835297584533691\n",
      "step : 99.56 % , loss : 4.865257740020752\n",
      "\tTrain Loss: 4.855\n",
      "step : 0.0 % , loss : 4.953901290893555\n",
      "step : 0.44 % , loss : 4.936608791351318\n",
      "step : 0.88 % , loss : 4.815524101257324\n",
      "step : 1.32 % , loss : 4.906864166259766\n",
      "step : 1.76 % , loss : 4.837407112121582\n",
      "step : 2.2 % , loss : 4.829185962677002\n",
      "step : 2.64 % , loss : 4.934568881988525\n",
      "step : 3.08 % , loss : 4.909457683563232\n",
      "step : 3.52 % , loss : 4.82943058013916\n",
      "step : 3.96 % , loss : 4.823551177978516\n",
      "step : 4.41 % , loss : 4.85198450088501\n",
      "step : 4.85 % , loss : 4.84998893737793\n",
      "step : 5.29 % , loss : 4.939352989196777\n",
      "step : 5.73 % , loss : 4.820538520812988\n",
      "step : 6.17 % , loss : 4.786983966827393\n",
      "step : 6.61 % , loss : 4.804823875427246\n",
      "step : 7.05 % , loss : 4.878421783447266\n",
      "step : 7.49 % , loss : 4.839687824249268\n",
      "step : 7.93 % , loss : 4.721914291381836\n",
      "step : 8.37 % , loss : 4.81441068649292\n",
      "step : 8.81 % , loss : 4.780161380767822\n",
      "step : 9.25 % , loss : 4.881211280822754\n",
      "step : 9.69 % , loss : 4.896979331970215\n",
      "step : 10.13 % , loss : 4.8553996086120605\n",
      "step : 10.57 % , loss : 4.768180847167969\n",
      "step : 11.01 % , loss : 4.7263898849487305\n",
      "step : 11.45 % , loss : 4.85047721862793\n",
      "step : 11.89 % , loss : 4.8218207359313965\n",
      "step : 12.33 % , loss : 4.873016834259033\n",
      "step : 12.78 % , loss : 4.947827339172363\n",
      "step : 13.22 % , loss : 4.755687236785889\n",
      "step : 13.66 % , loss : 4.914764404296875\n",
      "step : 14.1 % , loss : 4.885044097900391\n",
      "step : 14.54 % , loss : 4.797783374786377\n",
      "step : 14.98 % , loss : 4.776142597198486\n",
      "step : 15.42 % , loss : 4.783742427825928\n",
      "step : 15.86 % , loss : 4.846631050109863\n",
      "step : 16.3 % , loss : 4.873537540435791\n",
      "step : 16.74 % , loss : 4.839026927947998\n",
      "step : 17.18 % , loss : 4.779110431671143\n",
      "step : 17.62 % , loss : 4.865653991699219\n",
      "step : 18.06 % , loss : 4.867335319519043\n",
      "step : 18.5 % , loss : 4.7860260009765625\n",
      "step : 18.94 % , loss : 4.72960901260376\n",
      "step : 19.38 % , loss : 4.86898136138916\n",
      "step : 19.82 % , loss : 4.8400421142578125\n",
      "step : 20.26 % , loss : 4.794610023498535\n",
      "step : 20.7 % , loss : 4.949350357055664\n",
      "step : 21.15 % , loss : 4.8643412590026855\n",
      "step : 21.59 % , loss : 4.976598739624023\n",
      "step : 22.03 % , loss : 4.8790130615234375\n",
      "step : 22.47 % , loss : 4.772005081176758\n",
      "step : 22.91 % , loss : 4.81008768081665\n",
      "step : 23.35 % , loss : 4.856423377990723\n",
      "step : 23.79 % , loss : 4.801614284515381\n",
      "step : 24.23 % , loss : 4.8570051193237305\n",
      "step : 24.67 % , loss : 4.8339619636535645\n",
      "step : 25.11 % , loss : 4.841961860656738\n",
      "step : 25.55 % , loss : 4.861791610717773\n",
      "step : 25.99 % , loss : 4.745726108551025\n",
      "step : 26.43 % , loss : 4.8380560874938965\n",
      "step : 26.87 % , loss : 4.740167617797852\n",
      "step : 27.31 % , loss : 4.896217346191406\n",
      "step : 27.75 % , loss : 4.767941474914551\n",
      "step : 28.19 % , loss : 4.840126991271973\n",
      "step : 28.63 % , loss : 4.784658432006836\n",
      "step : 29.07 % , loss : 4.8803534507751465\n",
      "step : 29.52 % , loss : 4.709479808807373\n",
      "step : 29.96 % , loss : 4.8882951736450195\n",
      "step : 30.4 % , loss : 4.870116233825684\n",
      "step : 30.84 % , loss : 4.843776702880859\n",
      "step : 31.28 % , loss : 4.779748439788818\n",
      "step : 31.72 % , loss : 4.779750823974609\n",
      "step : 32.16 % , loss : 4.785817623138428\n",
      "step : 32.6 % , loss : 4.889293193817139\n",
      "step : 33.04 % , loss : 4.788887023925781\n",
      "step : 33.48 % , loss : 4.832428455352783\n",
      "step : 33.92 % , loss : 4.889340877532959\n",
      "step : 34.36 % , loss : 4.874189376831055\n",
      "step : 34.8 % , loss : 4.822403907775879\n",
      "step : 35.24 % , loss : 4.834291458129883\n",
      "step : 35.68 % , loss : 4.741247177124023\n",
      "step : 36.12 % , loss : 4.802112579345703\n",
      "step : 36.56 % , loss : 5.024597644805908\n",
      "step : 37.0 % , loss : 4.856722354888916\n",
      "step : 37.44 % , loss : 4.7822771072387695\n",
      "step : 37.89 % , loss : 4.789465427398682\n",
      "step : 38.33 % , loss : 4.902419090270996\n",
      "step : 38.77 % , loss : 4.903469562530518\n",
      "step : 39.21 % , loss : 4.746349334716797\n",
      "step : 39.65 % , loss : 4.826727390289307\n",
      "step : 40.09 % , loss : 4.774960994720459\n",
      "step : 40.53 % , loss : 4.862939834594727\n",
      "step : 40.97 % , loss : 4.851541996002197\n",
      "step : 41.41 % , loss : 4.791004657745361\n",
      "step : 41.85 % , loss : 4.763515949249268\n",
      "step : 42.29 % , loss : 4.926223278045654\n",
      "step : 42.73 % , loss : 4.889581203460693\n",
      "step : 43.17 % , loss : 4.931690692901611\n",
      "step : 43.61 % , loss : 4.897161483764648\n",
      "step : 44.05 % , loss : 4.840397834777832\n",
      "step : 44.49 % , loss : 4.765523910522461\n",
      "step : 44.93 % , loss : 4.776277542114258\n",
      "step : 45.37 % , loss : 4.827383518218994\n",
      "step : 45.81 % , loss : 4.8258256912231445\n",
      "step : 46.26 % , loss : 4.9044976234436035\n",
      "step : 46.7 % , loss : 4.757048606872559\n",
      "step : 47.14 % , loss : 4.837630748748779\n",
      "step : 47.58 % , loss : 4.8123297691345215\n",
      "step : 48.02 % , loss : 4.705093860626221\n",
      "step : 48.46 % , loss : 4.842930793762207\n",
      "step : 48.9 % , loss : 4.774503231048584\n",
      "step : 49.34 % , loss : 4.777843952178955\n",
      "step : 49.78 % , loss : 4.769159317016602\n",
      "step : 50.22 % , loss : 4.7870378494262695\n",
      "step : 50.66 % , loss : 4.854861736297607\n",
      "step : 51.1 % , loss : 4.863430500030518\n",
      "step : 51.54 % , loss : 4.717508316040039\n",
      "step : 51.98 % , loss : 4.834948539733887\n",
      "step : 52.42 % , loss : 4.930832862854004\n",
      "step : 52.86 % , loss : 4.8724822998046875\n",
      "step : 53.3 % , loss : 4.8782219886779785\n",
      "step : 53.74 % , loss : 4.8236775398254395\n",
      "step : 54.19 % , loss : 4.903325080871582\n",
      "step : 54.63 % , loss : 4.894931793212891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 55.07 % , loss : 4.769710063934326\n",
      "step : 55.51 % , loss : 4.792829990386963\n",
      "step : 55.95 % , loss : 4.877965450286865\n",
      "step : 56.39 % , loss : 4.8150434494018555\n",
      "step : 56.83 % , loss : 4.811454772949219\n",
      "step : 57.27 % , loss : 4.750597953796387\n",
      "step : 57.71 % , loss : 4.898746490478516\n",
      "step : 58.15 % , loss : 4.926220893859863\n",
      "step : 58.59 % , loss : 4.879480361938477\n",
      "step : 59.03 % , loss : 4.89599084854126\n",
      "step : 59.47 % , loss : 4.897969722747803\n",
      "step : 59.91 % , loss : 4.855325698852539\n",
      "step : 60.35 % , loss : 4.832739353179932\n",
      "step : 60.79 % , loss : 4.807971477508545\n",
      "step : 61.23 % , loss : 4.761254787445068\n",
      "step : 61.67 % , loss : 4.87692403793335\n",
      "step : 62.11 % , loss : 4.971404075622559\n",
      "step : 62.56 % , loss : 4.751805782318115\n",
      "step : 63.0 % , loss : 4.702693462371826\n",
      "step : 63.44 % , loss : 4.843865394592285\n",
      "step : 63.88 % , loss : 4.9141154289245605\n",
      "step : 64.32 % , loss : 4.779856204986572\n",
      "step : 64.76 % , loss : 4.886302471160889\n",
      "step : 65.2 % , loss : 4.796115398406982\n",
      "step : 65.64 % , loss : 4.928067684173584\n",
      "step : 66.08 % , loss : 4.8473358154296875\n",
      "step : 66.52 % , loss : 4.752950191497803\n",
      "step : 66.96 % , loss : 4.7928595542907715\n",
      "step : 67.4 % , loss : 4.885457992553711\n",
      "step : 67.84 % , loss : 4.852010726928711\n",
      "step : 68.28 % , loss : 4.792535305023193\n",
      "step : 68.72 % , loss : 4.876861572265625\n",
      "step : 69.16 % , loss : 4.768765926361084\n",
      "step : 69.6 % , loss : 4.890920162200928\n",
      "step : 70.04 % , loss : 4.799072265625\n",
      "step : 70.48 % , loss : 4.8502302169799805\n",
      "step : 70.93 % , loss : 4.805387496948242\n",
      "step : 71.37 % , loss : 4.750735282897949\n",
      "step : 71.81 % , loss : 4.906373500823975\n",
      "step : 72.25 % , loss : 4.747853755950928\n",
      "step : 72.69 % , loss : 4.800072193145752\n",
      "step : 73.13 % , loss : 4.846496105194092\n",
      "step : 73.57 % , loss : 4.838695526123047\n",
      "step : 74.01 % , loss : 4.895008563995361\n",
      "step : 74.45 % , loss : 4.742944240570068\n",
      "step : 74.89 % , loss : 4.969197750091553\n",
      "step : 75.33 % , loss : 4.807961940765381\n",
      "step : 75.77 % , loss : 4.911640167236328\n",
      "step : 76.21 % , loss : 4.7890543937683105\n",
      "step : 76.65 % , loss : 4.858163833618164\n",
      "step : 77.09 % , loss : 4.822673320770264\n",
      "step : 77.53 % , loss : 4.783943176269531\n",
      "step : 77.97 % , loss : 4.956460952758789\n",
      "step : 78.41 % , loss : 4.8671159744262695\n",
      "step : 78.85 % , loss : 4.751643657684326\n",
      "step : 79.3 % , loss : 4.777224540710449\n",
      "step : 79.74 % , loss : 4.685399532318115\n",
      "step : 80.18 % , loss : 4.7832136154174805\n",
      "step : 80.62 % , loss : 4.780890941619873\n",
      "step : 81.06 % , loss : 4.973996162414551\n",
      "step : 81.5 % , loss : 4.812682628631592\n",
      "step : 81.94 % , loss : 4.774809837341309\n",
      "step : 82.38 % , loss : 4.8072509765625\n",
      "step : 82.82 % , loss : 4.74702787399292\n",
      "step : 83.26 % , loss : 4.801754951477051\n",
      "step : 83.7 % , loss : 4.709157466888428\n",
      "step : 84.14 % , loss : 4.740916728973389\n",
      "step : 84.58 % , loss : 4.9065351486206055\n",
      "step : 85.02 % , loss : 4.868922710418701\n",
      "step : 85.46 % , loss : 4.8238396644592285\n",
      "step : 85.9 % , loss : 4.939382076263428\n",
      "step : 86.34 % , loss : 4.9105658531188965\n",
      "step : 86.78 % , loss : 4.779565334320068\n",
      "step : 87.22 % , loss : 4.862126350402832\n",
      "step : 87.67 % , loss : 4.898091793060303\n",
      "step : 88.11 % , loss : 4.842615604400635\n",
      "step : 88.55 % , loss : 4.797882556915283\n",
      "step : 88.99 % , loss : 4.835596084594727\n",
      "step : 89.43 % , loss : 4.8062520027160645\n",
      "step : 89.87 % , loss : 4.82355260848999\n",
      "step : 90.31 % , loss : 4.81089448928833\n",
      "step : 90.75 % , loss : 4.910960674285889\n",
      "step : 91.19 % , loss : 4.662423133850098\n",
      "step : 91.63 % , loss : 4.822749137878418\n",
      "step : 92.07 % , loss : 4.908289432525635\n",
      "step : 92.51 % , loss : 4.908486843109131\n",
      "step : 92.95 % , loss : 4.853370189666748\n",
      "step : 93.39 % , loss : 4.8474602699279785\n",
      "step : 93.83 % , loss : 4.765491008758545\n",
      "step : 94.27 % , loss : 4.828594207763672\n",
      "step : 94.71 % , loss : 4.811393737792969\n",
      "step : 95.15 % , loss : 4.888746738433838\n",
      "step : 95.59 % , loss : 4.867306709289551\n",
      "step : 96.04 % , loss : 4.931687355041504\n",
      "step : 96.48 % , loss : 4.909895420074463\n",
      "step : 96.92 % , loss : 4.88325834274292\n",
      "step : 97.36 % , loss : 4.774334907531738\n",
      "step : 97.8 % , loss : 4.79176139831543\n",
      "step : 98.24 % , loss : 4.92104959487915\n",
      "step : 98.68 % , loss : 4.755141735076904\n",
      "step : 99.12 % , loss : 4.859990119934082\n",
      "step : 99.56 % , loss : 4.852422714233398\n",
      "\tTrain Loss: 4.835\n",
      "step : 0.0 % , loss : 4.909050464630127\n",
      "step : 0.44 % , loss : 4.839058876037598\n",
      "step : 0.88 % , loss : 4.885664939880371\n",
      "step : 1.32 % , loss : 4.844353675842285\n",
      "step : 1.76 % , loss : 4.8340935707092285\n",
      "step : 2.2 % , loss : 4.869577407836914\n",
      "step : 2.64 % , loss : 4.870769500732422\n",
      "step : 3.08 % , loss : 4.817224502563477\n",
      "step : 3.52 % , loss : 4.782907009124756\n",
      "step : 3.96 % , loss : 4.755189418792725\n",
      "step : 4.41 % , loss : 4.822588920593262\n",
      "step : 4.85 % , loss : 4.916865348815918\n",
      "step : 5.29 % , loss : 4.890320777893066\n",
      "step : 5.73 % , loss : 4.7592973709106445\n",
      "step : 6.17 % , loss : 4.774092197418213\n",
      "step : 6.61 % , loss : 4.7591071128845215\n",
      "step : 7.05 % , loss : 4.695486068725586\n",
      "step : 7.49 % , loss : 4.751037120819092\n",
      "step : 7.93 % , loss : 4.775288105010986\n",
      "step : 8.37 % , loss : 4.826085090637207\n",
      "step : 8.81 % , loss : 4.802854061126709\n",
      "step : 9.25 % , loss : 4.877124786376953\n",
      "step : 9.69 % , loss : 4.774735450744629\n",
      "step : 10.13 % , loss : 4.855361461639404\n",
      "step : 10.57 % , loss : 4.839346885681152\n",
      "step : 11.01 % , loss : 4.784440994262695\n",
      "step : 11.45 % , loss : 4.7935075759887695\n",
      "step : 11.89 % , loss : 4.810361862182617\n",
      "step : 12.33 % , loss : 4.785161972045898\n",
      "step : 12.78 % , loss : 4.833533763885498\n",
      "step : 13.22 % , loss : 4.90545654296875\n",
      "step : 13.66 % , loss : 4.724515438079834\n",
      "step : 14.1 % , loss : 4.7595906257629395\n",
      "step : 14.54 % , loss : 4.844791889190674\n",
      "step : 14.98 % , loss : 4.933592796325684\n",
      "step : 15.42 % , loss : 4.75917387008667\n",
      "step : 15.86 % , loss : 4.875728130340576\n",
      "step : 16.3 % , loss : 4.78814172744751\n",
      "step : 16.74 % , loss : 4.862125396728516\n",
      "step : 17.18 % , loss : 4.723191738128662\n",
      "step : 17.62 % , loss : 4.857994079589844\n",
      "step : 18.06 % , loss : 4.844587326049805\n",
      "step : 18.5 % , loss : 4.881204605102539\n",
      "step : 18.94 % , loss : 4.82373046875\n",
      "step : 19.38 % , loss : 4.758800983428955\n",
      "step : 19.82 % , loss : 4.880904197692871\n",
      "step : 20.26 % , loss : 4.863547325134277\n",
      "step : 20.7 % , loss : 4.890537261962891\n",
      "step : 21.15 % , loss : 4.856691360473633\n",
      "step : 21.59 % , loss : 4.863563537597656\n",
      "step : 22.03 % , loss : 4.876245498657227\n",
      "step : 22.47 % , loss : 4.867582321166992\n",
      "step : 22.91 % , loss : 4.8237128257751465\n",
      "step : 23.35 % , loss : 4.773859977722168\n",
      "step : 23.79 % , loss : 4.894630432128906\n",
      "step : 24.23 % , loss : 4.919980525970459\n",
      "step : 24.67 % , loss : 4.94831657409668\n",
      "step : 25.11 % , loss : 4.726643085479736\n",
      "step : 25.55 % , loss : 4.762499809265137\n",
      "step : 25.99 % , loss : 4.699423313140869\n",
      "step : 26.43 % , loss : 4.6872124671936035\n",
      "step : 26.87 % , loss : 4.699605464935303\n",
      "step : 27.31 % , loss : 4.850625038146973\n",
      "step : 27.75 % , loss : 4.883521556854248\n",
      "step : 28.19 % , loss : 4.687572002410889\n",
      "step : 28.63 % , loss : 4.774989604949951\n",
      "step : 29.07 % , loss : 4.881791591644287\n",
      "step : 29.52 % , loss : 4.758667469024658\n",
      "step : 29.96 % , loss : 4.852967739105225\n",
      "step : 30.4 % , loss : 4.872954368591309\n",
      "step : 30.84 % , loss : 4.952545642852783\n",
      "step : 31.28 % , loss : 4.942590236663818\n",
      "step : 31.72 % , loss : 4.933481693267822\n",
      "step : 32.16 % , loss : 4.784605026245117\n",
      "step : 32.6 % , loss : 4.793025493621826\n",
      "step : 33.04 % , loss : 4.86232328414917\n",
      "step : 33.48 % , loss : 4.853073596954346\n",
      "step : 33.92 % , loss : 4.836589813232422\n",
      "step : 34.36 % , loss : 4.866878509521484\n",
      "step : 34.8 % , loss : 4.91717004776001\n",
      "step : 35.24 % , loss : 4.862891674041748\n",
      "step : 35.68 % , loss : 4.80205774307251\n",
      "step : 36.12 % , loss : 4.747393608093262\n",
      "step : 36.56 % , loss : 4.863621234893799\n",
      "step : 37.0 % , loss : 4.776580333709717\n",
      "step : 37.44 % , loss : 4.77650260925293\n",
      "step : 37.89 % , loss : 4.838853359222412\n",
      "step : 38.33 % , loss : 4.694281101226807\n",
      "step : 38.77 % , loss : 4.805776596069336\n",
      "step : 39.21 % , loss : 4.918765068054199\n",
      "step : 39.65 % , loss : 4.865262508392334\n",
      "step : 40.09 % , loss : 4.8614068031311035\n",
      "step : 40.53 % , loss : 4.765270233154297\n",
      "step : 40.97 % , loss : 4.745141506195068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 41.41 % , loss : 4.86216402053833\n",
      "step : 41.85 % , loss : 4.773407459259033\n",
      "step : 42.29 % , loss : 4.782778263092041\n",
      "step : 42.73 % , loss : 4.8167901039123535\n",
      "step : 43.17 % , loss : 4.788980484008789\n",
      "step : 43.61 % , loss : 4.894235134124756\n",
      "step : 44.05 % , loss : 4.6940741539001465\n",
      "step : 44.49 % , loss : 4.858147621154785\n",
      "step : 44.93 % , loss : 4.926585674285889\n",
      "step : 45.37 % , loss : 4.855001926422119\n",
      "step : 45.81 % , loss : 4.707961082458496\n",
      "step : 46.26 % , loss : 4.805462837219238\n",
      "step : 46.7 % , loss : 4.845595836639404\n",
      "step : 47.14 % , loss : 4.8132524490356445\n",
      "step : 47.58 % , loss : 4.82627534866333\n",
      "step : 48.02 % , loss : 4.790200233459473\n",
      "step : 48.46 % , loss : 4.872283935546875\n",
      "step : 48.9 % , loss : 4.755117893218994\n",
      "step : 49.34 % , loss : 4.810283184051514\n",
      "step : 49.78 % , loss : 4.765456199645996\n",
      "step : 50.22 % , loss : 4.859495162963867\n",
      "step : 50.66 % , loss : 4.91596794128418\n",
      "step : 51.1 % , loss : 4.764493465423584\n",
      "step : 51.54 % , loss : 4.767622947692871\n",
      "step : 51.98 % , loss : 4.746711254119873\n",
      "step : 52.42 % , loss : 4.827075481414795\n",
      "step : 52.86 % , loss : 4.797136306762695\n",
      "step : 53.3 % , loss : 4.968511581420898\n",
      "step : 53.74 % , loss : 4.798119068145752\n",
      "step : 54.19 % , loss : 4.779406547546387\n",
      "step : 54.63 % , loss : 4.78916597366333\n",
      "step : 55.07 % , loss : 4.781472206115723\n",
      "step : 55.51 % , loss : 4.847188472747803\n",
      "step : 55.95 % , loss : 4.786365032196045\n",
      "step : 56.39 % , loss : 4.85383415222168\n",
      "step : 56.83 % , loss : 4.910156726837158\n",
      "step : 57.27 % , loss : 4.756734371185303\n",
      "step : 57.71 % , loss : 4.827354431152344\n",
      "step : 58.15 % , loss : 4.889887809753418\n",
      "step : 58.59 % , loss : 4.94541072845459\n",
      "step : 59.03 % , loss : 4.777995586395264\n",
      "step : 59.47 % , loss : 4.691353797912598\n",
      "step : 59.91 % , loss : 4.738396644592285\n",
      "step : 60.35 % , loss : 4.856715679168701\n",
      "step : 60.79 % , loss : 4.793270587921143\n",
      "step : 61.23 % , loss : 4.815699577331543\n",
      "step : 61.67 % , loss : 4.841609001159668\n",
      "step : 62.11 % , loss : 4.855417728424072\n",
      "step : 62.56 % , loss : 4.665595531463623\n",
      "step : 63.0 % , loss : 4.851382732391357\n",
      "step : 63.44 % , loss : 4.948451519012451\n",
      "step : 63.88 % , loss : 4.696377754211426\n",
      "step : 64.32 % , loss : 4.768255233764648\n",
      "step : 64.76 % , loss : 4.734549522399902\n",
      "step : 65.2 % , loss : 4.8332743644714355\n",
      "step : 65.64 % , loss : 4.74423885345459\n",
      "step : 66.08 % , loss : 4.83987283706665\n",
      "step : 66.52 % , loss : 4.762584209442139\n",
      "step : 66.96 % , loss : 4.93997859954834\n",
      "step : 67.4 % , loss : 4.773147106170654\n",
      "step : 67.84 % , loss : 4.734443664550781\n",
      "step : 68.28 % , loss : 4.85682487487793\n",
      "step : 68.72 % , loss : 4.827432155609131\n",
      "step : 69.16 % , loss : 4.897577285766602\n",
      "step : 69.6 % , loss : 4.932078838348389\n",
      "step : 70.04 % , loss : 4.729144096374512\n",
      "step : 70.48 % , loss : 4.786731719970703\n",
      "step : 70.93 % , loss : 4.805960655212402\n",
      "step : 71.37 % , loss : 4.749909400939941\n",
      "step : 71.81 % , loss : 4.770173072814941\n",
      "step : 72.25 % , loss : 4.785505294799805\n",
      "step : 72.69 % , loss : 4.765625953674316\n",
      "step : 73.13 % , loss : 4.84922456741333\n",
      "step : 73.57 % , loss : 4.704899787902832\n",
      "step : 74.01 % , loss : 4.893487453460693\n",
      "step : 74.45 % , loss : 4.801564693450928\n",
      "step : 74.89 % , loss : 4.811944007873535\n",
      "step : 75.33 % , loss : 4.832364559173584\n",
      "step : 75.77 % , loss : 4.935810565948486\n",
      "step : 76.21 % , loss : 4.856312274932861\n",
      "step : 76.65 % , loss : 4.764266490936279\n",
      "step : 77.09 % , loss : 4.859389305114746\n",
      "step : 77.53 % , loss : 4.761081695556641\n",
      "step : 77.97 % , loss : 4.768085479736328\n",
      "step : 78.41 % , loss : 4.854832172393799\n",
      "step : 78.85 % , loss : 4.788538455963135\n",
      "step : 79.3 % , loss : 4.917636871337891\n",
      "step : 79.74 % , loss : 4.879931926727295\n",
      "step : 80.18 % , loss : 4.945332050323486\n",
      "step : 80.62 % , loss : 4.9420318603515625\n",
      "step : 81.06 % , loss : 4.833199501037598\n",
      "step : 81.5 % , loss : 4.8973236083984375\n",
      "step : 81.94 % , loss : 4.951788425445557\n",
      "step : 82.38 % , loss : 4.839996814727783\n",
      "step : 82.82 % , loss : 4.783204078674316\n",
      "step : 83.26 % , loss : 4.74080753326416\n",
      "step : 83.7 % , loss : 4.833220481872559\n",
      "step : 84.14 % , loss : 4.85101318359375\n",
      "step : 84.58 % , loss : 4.893424034118652\n",
      "step : 85.02 % , loss : 4.790786266326904\n",
      "step : 85.46 % , loss : 4.632341384887695\n",
      "step : 85.9 % , loss : 4.744771480560303\n",
      "step : 86.34 % , loss : 4.764090538024902\n",
      "step : 86.78 % , loss : 4.854087829589844\n",
      "step : 87.22 % , loss : 4.725657939910889\n",
      "step : 87.67 % , loss : 4.738964557647705\n",
      "step : 88.11 % , loss : 4.776887893676758\n",
      "step : 88.55 % , loss : 4.75521993637085\n",
      "step : 88.99 % , loss : 4.748648643493652\n",
      "step : 89.43 % , loss : 4.819456100463867\n",
      "step : 89.87 % , loss : 4.847989559173584\n",
      "step : 90.31 % , loss : 4.771659851074219\n",
      "step : 90.75 % , loss : 4.8257527351379395\n",
      "step : 91.19 % , loss : 4.8555216789245605\n",
      "step : 91.63 % , loss : 4.750351905822754\n",
      "step : 92.07 % , loss : 4.749418258666992\n",
      "step : 92.51 % , loss : 4.708101749420166\n",
      "step : 92.95 % , loss : 4.780097484588623\n",
      "step : 93.39 % , loss : 4.798812389373779\n",
      "step : 93.83 % , loss : 4.913283348083496\n",
      "step : 94.27 % , loss : 4.78675651550293\n",
      "step : 94.71 % , loss : 4.957359313964844\n",
      "step : 95.15 % , loss : 4.8461222648620605\n",
      "step : 95.59 % , loss : 4.861496925354004\n",
      "step : 96.04 % , loss : 4.7849650382995605\n",
      "step : 96.48 % , loss : 4.836035251617432\n",
      "step : 96.92 % , loss : 4.697066307067871\n",
      "step : 97.36 % , loss : 4.902564525604248\n",
      "step : 97.8 % , loss : 4.68447732925415\n",
      "step : 98.24 % , loss : 4.79928731918335\n",
      "step : 98.68 % , loss : 4.806141376495361\n",
      "step : 99.12 % , loss : 4.8371758460998535\n",
      "step : 99.56 % , loss : 4.783804893493652\n",
      "\tTrain Loss: 4.818\n",
      "step : 0.0 % , loss : 4.776216506958008\n",
      "step : 0.44 % , loss : 4.85163688659668\n",
      "step : 0.88 % , loss : 4.866529941558838\n",
      "step : 1.32 % , loss : 4.846283435821533\n",
      "step : 1.76 % , loss : 4.749754905700684\n",
      "step : 2.2 % , loss : 4.942446231842041\n",
      "step : 2.64 % , loss : 4.801258563995361\n",
      "step : 3.08 % , loss : 4.822722911834717\n",
      "step : 3.52 % , loss : 4.817563056945801\n",
      "step : 3.96 % , loss : 4.860171318054199\n",
      "step : 4.41 % , loss : 4.710506916046143\n",
      "step : 4.85 % , loss : 4.834676265716553\n",
      "step : 5.29 % , loss : 4.720322608947754\n",
      "step : 5.73 % , loss : 4.766005992889404\n",
      "step : 6.17 % , loss : 4.719924449920654\n",
      "step : 6.61 % , loss : 4.876309871673584\n",
      "step : 7.05 % , loss : 4.750657081604004\n",
      "step : 7.49 % , loss : 4.777854919433594\n",
      "step : 7.93 % , loss : 4.925086498260498\n",
      "step : 8.37 % , loss : 4.859912872314453\n",
      "step : 8.81 % , loss : 4.8607096672058105\n",
      "step : 9.25 % , loss : 4.775798797607422\n",
      "step : 9.69 % , loss : 4.854959487915039\n",
      "step : 10.13 % , loss : 4.691553592681885\n",
      "step : 10.57 % , loss : 4.846286773681641\n",
      "step : 11.01 % , loss : 4.723412990570068\n",
      "step : 11.45 % , loss : 4.857034206390381\n",
      "step : 11.89 % , loss : 4.768710136413574\n",
      "step : 12.33 % , loss : 4.710514068603516\n",
      "step : 12.78 % , loss : 4.897793769836426\n",
      "step : 13.22 % , loss : 4.862881660461426\n",
      "step : 13.66 % , loss : 4.904153347015381\n",
      "step : 14.1 % , loss : 4.908843517303467\n",
      "step : 14.54 % , loss : 4.843137741088867\n",
      "step : 14.98 % , loss : 4.789414882659912\n",
      "step : 15.42 % , loss : 4.775986194610596\n",
      "step : 15.86 % , loss : 4.852555274963379\n",
      "step : 16.3 % , loss : 4.887887477874756\n",
      "step : 16.74 % , loss : 4.713691234588623\n",
      "step : 17.18 % , loss : 4.840670108795166\n",
      "step : 17.62 % , loss : 4.850073337554932\n",
      "step : 18.06 % , loss : 4.765041351318359\n",
      "step : 18.5 % , loss : 4.705235958099365\n",
      "step : 18.94 % , loss : 4.791891098022461\n",
      "step : 19.38 % , loss : 4.8549275398254395\n",
      "step : 19.82 % , loss : 4.776672840118408\n",
      "step : 20.26 % , loss : 4.81868839263916\n",
      "step : 20.7 % , loss : 4.7615251541137695\n",
      "step : 21.15 % , loss : 4.872397422790527\n",
      "step : 21.59 % , loss : 4.858623504638672\n",
      "step : 22.03 % , loss : 4.726015090942383\n",
      "step : 22.47 % , loss : 4.716917991638184\n",
      "step : 22.91 % , loss : 4.850677490234375\n",
      "step : 23.35 % , loss : 4.792703628540039\n",
      "step : 23.79 % , loss : 4.727493762969971\n",
      "step : 24.23 % , loss : 4.822652339935303\n",
      "step : 24.67 % , loss : 4.732405662536621\n",
      "step : 25.11 % , loss : 4.8178629875183105\n",
      "step : 25.55 % , loss : 4.805711269378662\n",
      "step : 25.99 % , loss : 4.763829708099365\n",
      "step : 26.43 % , loss : 4.79392147064209\n",
      "step : 26.87 % , loss : 4.783661842346191\n",
      "step : 27.31 % , loss : 4.85931396484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 27.75 % , loss : 4.720597743988037\n",
      "step : 28.19 % , loss : 4.846248626708984\n",
      "step : 28.63 % , loss : 4.870975971221924\n",
      "step : 29.07 % , loss : 4.8156046867370605\n",
      "step : 29.52 % , loss : 4.775539398193359\n",
      "step : 29.96 % , loss : 4.819634914398193\n",
      "step : 30.4 % , loss : 4.80328893661499\n",
      "step : 30.84 % , loss : 4.8835320472717285\n",
      "step : 31.28 % , loss : 4.746180534362793\n",
      "step : 31.72 % , loss : 4.989577770233154\n",
      "step : 32.16 % , loss : 4.7102484703063965\n",
      "step : 32.6 % , loss : 4.686503887176514\n",
      "step : 33.04 % , loss : 4.81505823135376\n",
      "step : 33.48 % , loss : 4.8702898025512695\n",
      "step : 33.92 % , loss : 4.693925380706787\n",
      "step : 34.36 % , loss : 4.838398456573486\n",
      "step : 34.8 % , loss : 4.839297771453857\n",
      "step : 35.24 % , loss : 4.880918502807617\n",
      "step : 35.68 % , loss : 4.770684719085693\n",
      "step : 36.12 % , loss : 4.805480003356934\n",
      "step : 36.56 % , loss : 4.784658908843994\n",
      "step : 37.0 % , loss : 4.701117992401123\n",
      "step : 37.44 % , loss : 4.8293538093566895\n",
      "step : 37.89 % , loss : 4.7994303703308105\n",
      "step : 38.33 % , loss : 4.777553081512451\n",
      "step : 38.77 % , loss : 4.841445446014404\n",
      "step : 39.21 % , loss : 4.711884021759033\n",
      "step : 39.65 % , loss : 4.770750522613525\n",
      "step : 40.09 % , loss : 4.76036262512207\n",
      "step : 40.53 % , loss : 4.70242166519165\n",
      "step : 40.97 % , loss : 4.593944549560547\n",
      "step : 41.41 % , loss : 4.840959072113037\n",
      "step : 41.85 % , loss : 4.948813438415527\n",
      "step : 42.29 % , loss : 4.771902561187744\n",
      "step : 42.73 % , loss : 4.836675643920898\n",
      "step : 43.17 % , loss : 4.753053188323975\n",
      "step : 43.61 % , loss : 4.811558723449707\n",
      "step : 44.05 % , loss : 4.771335601806641\n",
      "step : 44.49 % , loss : 4.889420032501221\n",
      "step : 44.93 % , loss : 4.731882095336914\n",
      "step : 45.37 % , loss : 4.752406120300293\n",
      "step : 45.81 % , loss : 4.669148921966553\n",
      "step : 46.26 % , loss : 4.883821964263916\n",
      "step : 46.7 % , loss : 4.71980094909668\n",
      "step : 47.14 % , loss : 4.882184028625488\n",
      "step : 47.58 % , loss : 4.782132148742676\n",
      "step : 48.02 % , loss : 4.764649391174316\n",
      "step : 48.46 % , loss : 4.882350444793701\n",
      "step : 48.9 % , loss : 4.837065696716309\n",
      "step : 49.34 % , loss : 4.806364059448242\n",
      "step : 49.78 % , loss : 4.80766487121582\n",
      "step : 50.22 % , loss : 4.773678302764893\n",
      "step : 50.66 % , loss : 4.777187347412109\n",
      "step : 51.1 % , loss : 4.896878242492676\n",
      "step : 51.54 % , loss : 4.857357025146484\n",
      "step : 51.98 % , loss : 4.817883491516113\n",
      "step : 52.42 % , loss : 4.6356024742126465\n",
      "step : 52.86 % , loss : 4.776257038116455\n",
      "step : 53.3 % , loss : 4.78272819519043\n",
      "step : 53.74 % , loss : 4.697178840637207\n",
      "step : 54.19 % , loss : 4.803261756896973\n",
      "step : 54.63 % , loss : 4.826874732971191\n",
      "step : 55.07 % , loss : 4.938779830932617\n",
      "step : 55.51 % , loss : 4.813423156738281\n",
      "step : 55.95 % , loss : 4.796374797821045\n",
      "step : 56.39 % , loss : 4.810699462890625\n",
      "step : 56.83 % , loss : 4.845620632171631\n",
      "step : 57.27 % , loss : 4.789806365966797\n",
      "step : 57.71 % , loss : 4.806532859802246\n",
      "step : 58.15 % , loss : 4.761585235595703\n",
      "step : 58.59 % , loss : 4.737604141235352\n",
      "step : 59.03 % , loss : 4.827552795410156\n",
      "step : 59.47 % , loss : 4.86375617980957\n",
      "step : 59.91 % , loss : 4.874170780181885\n",
      "step : 60.35 % , loss : 4.787251949310303\n",
      "step : 60.79 % , loss : 4.7666120529174805\n",
      "step : 61.23 % , loss : 4.705499172210693\n",
      "step : 61.67 % , loss : 4.875088691711426\n",
      "step : 62.11 % , loss : 4.693721294403076\n",
      "step : 62.56 % , loss : 4.847864627838135\n",
      "step : 63.0 % , loss : 4.767660140991211\n",
      "step : 63.44 % , loss : 4.817698955535889\n",
      "step : 63.88 % , loss : 4.755295753479004\n",
      "step : 64.32 % , loss : 4.82299280166626\n",
      "step : 64.76 % , loss : 4.950934886932373\n",
      "step : 65.2 % , loss : 4.78256893157959\n",
      "step : 65.64 % , loss : 4.768003940582275\n",
      "step : 66.08 % , loss : 4.798595428466797\n",
      "step : 66.52 % , loss : 4.865268707275391\n",
      "step : 66.96 % , loss : 4.632798671722412\n",
      "step : 67.4 % , loss : 4.771389961242676\n",
      "step : 67.84 % , loss : 4.7461838722229\n",
      "step : 68.28 % , loss : 4.8189287185668945\n",
      "step : 68.72 % , loss : 4.869558334350586\n",
      "step : 69.16 % , loss : 4.7401227951049805\n",
      "step : 69.6 % , loss : 4.8788533210754395\n",
      "step : 70.04 % , loss : 4.784100532531738\n",
      "step : 70.48 % , loss : 4.695331573486328\n",
      "step : 70.93 % , loss : 4.753055572509766\n",
      "step : 71.37 % , loss : 4.774631500244141\n",
      "step : 71.81 % , loss : 4.733822345733643\n",
      "step : 72.25 % , loss : 4.772269248962402\n",
      "step : 72.69 % , loss : 4.808496475219727\n",
      "step : 73.13 % , loss : 4.802499294281006\n",
      "step : 73.57 % , loss : 4.828991889953613\n",
      "step : 74.01 % , loss : 4.648550510406494\n",
      "step : 74.45 % , loss : 4.685606956481934\n",
      "step : 74.89 % , loss : 4.873826026916504\n",
      "step : 75.33 % , loss : 4.857579231262207\n",
      "step : 75.77 % , loss : 4.841906547546387\n",
      "step : 76.21 % , loss : 4.8676018714904785\n",
      "step : 76.65 % , loss : 4.761343955993652\n",
      "step : 77.09 % , loss : 4.836099147796631\n",
      "step : 77.53 % , loss : 4.85952091217041\n",
      "step : 77.97 % , loss : 4.839476585388184\n",
      "step : 78.41 % , loss : 4.852019309997559\n",
      "step : 78.85 % , loss : 4.797913551330566\n",
      "step : 79.3 % , loss : 4.8476152420043945\n",
      "step : 79.74 % , loss : 4.7229323387146\n",
      "step : 80.18 % , loss : 4.75687837600708\n",
      "step : 80.62 % , loss : 4.778257846832275\n",
      "step : 81.06 % , loss : 4.778814792633057\n",
      "step : 81.5 % , loss : 4.946459770202637\n",
      "step : 81.94 % , loss : 4.863323211669922\n",
      "step : 82.38 % , loss : 4.761712551116943\n",
      "step : 82.82 % , loss : 4.80600118637085\n",
      "step : 83.26 % , loss : 4.832977771759033\n",
      "step : 83.7 % , loss : 4.746058464050293\n",
      "step : 84.14 % , loss : 4.80850076675415\n",
      "step : 84.58 % , loss : 4.816185474395752\n",
      "step : 85.02 % , loss : 4.920576095581055\n",
      "step : 85.46 % , loss : 4.782383918762207\n",
      "step : 85.9 % , loss : 4.777858257293701\n",
      "step : 86.34 % , loss : 4.8032450675964355\n",
      "step : 86.78 % , loss : 4.85842752456665\n",
      "step : 87.22 % , loss : 4.961337089538574\n",
      "step : 87.67 % , loss : 4.808829307556152\n",
      "step : 88.11 % , loss : 4.826940536499023\n",
      "step : 88.55 % , loss : 4.789247989654541\n",
      "step : 88.99 % , loss : 4.984029769897461\n",
      "step : 89.43 % , loss : 4.760471820831299\n",
      "step : 89.87 % , loss : 4.93416166305542\n",
      "step : 90.31 % , loss : 4.7128682136535645\n",
      "step : 90.75 % , loss : 4.935100555419922\n",
      "step : 91.19 % , loss : 4.697065353393555\n",
      "step : 91.63 % , loss : 4.822323799133301\n",
      "step : 92.07 % , loss : 4.812440395355225\n",
      "step : 92.51 % , loss : 4.715008735656738\n",
      "step : 92.95 % , loss : 4.746243953704834\n",
      "step : 93.39 % , loss : 4.798055648803711\n",
      "step : 93.83 % , loss : 4.792618751525879\n",
      "step : 94.27 % , loss : 4.694965839385986\n",
      "step : 94.71 % , loss : 4.810758113861084\n",
      "step : 95.15 % , loss : 4.765407562255859\n",
      "step : 95.59 % , loss : 4.663370609283447\n",
      "step : 96.04 % , loss : 4.753547668457031\n",
      "step : 96.48 % , loss : 4.779667377471924\n",
      "step : 96.92 % , loss : 4.90422248840332\n",
      "step : 97.36 % , loss : 4.777648448944092\n",
      "step : 97.8 % , loss : 4.735235691070557\n",
      "step : 98.24 % , loss : 4.742321014404297\n",
      "step : 98.68 % , loss : 4.8057355880737305\n",
      "step : 99.12 % , loss : 4.805323600769043\n",
      "step : 99.56 % , loss : 4.7534918785095215\n",
      "\tTrain Loss: 4.801\n",
      "step : 0.0 % , loss : 4.863330364227295\n",
      "step : 0.44 % , loss : 4.779609203338623\n",
      "step : 0.88 % , loss : 4.685933589935303\n",
      "step : 1.32 % , loss : 4.763045310974121\n",
      "step : 1.76 % , loss : 4.777523994445801\n",
      "step : 2.2 % , loss : 4.7709550857543945\n",
      "step : 2.64 % , loss : 4.7368574142456055\n",
      "step : 3.08 % , loss : 4.7495293617248535\n",
      "step : 3.52 % , loss : 4.7839813232421875\n",
      "step : 3.96 % , loss : 4.712357521057129\n",
      "step : 4.41 % , loss : 4.794741630554199\n",
      "step : 4.85 % , loss : 4.893794536590576\n",
      "step : 5.29 % , loss : 4.656345367431641\n",
      "step : 5.73 % , loss : 4.680438041687012\n",
      "step : 6.17 % , loss : 4.831048488616943\n",
      "step : 6.61 % , loss : 4.754817485809326\n",
      "step : 7.05 % , loss : 4.615755558013916\n",
      "step : 7.49 % , loss : 4.727126598358154\n",
      "step : 7.93 % , loss : 4.655483722686768\n",
      "step : 8.37 % , loss : 4.818137168884277\n",
      "step : 8.81 % , loss : 4.734907150268555\n",
      "step : 9.25 % , loss : 4.764122486114502\n",
      "step : 9.69 % , loss : 4.752922534942627\n",
      "step : 10.13 % , loss : 4.808223247528076\n",
      "step : 10.57 % , loss : 4.76291036605835\n",
      "step : 11.01 % , loss : 4.867005348205566\n",
      "step : 11.45 % , loss : 4.8817853927612305\n",
      "step : 11.89 % , loss : 4.813769340515137\n",
      "step : 12.33 % , loss : 4.730124473571777\n",
      "step : 12.78 % , loss : 4.690074920654297\n",
      "step : 13.22 % , loss : 4.947356700897217\n",
      "step : 13.66 % , loss : 4.804770469665527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 14.1 % , loss : 4.848330497741699\n",
      "step : 14.54 % , loss : 4.667087078094482\n",
      "step : 14.98 % , loss : 4.82794713973999\n",
      "step : 15.42 % , loss : 4.840816497802734\n",
      "step : 15.86 % , loss : 4.774176120758057\n",
      "step : 16.3 % , loss : 4.728699684143066\n",
      "step : 16.74 % , loss : 4.766938209533691\n",
      "step : 17.18 % , loss : 4.778580188751221\n",
      "step : 17.62 % , loss : 4.6984477043151855\n",
      "step : 18.06 % , loss : 4.88330078125\n",
      "step : 18.5 % , loss : 4.748808860778809\n",
      "step : 18.94 % , loss : 4.772173881530762\n",
      "step : 19.38 % , loss : 4.851687908172607\n",
      "step : 19.82 % , loss : 4.82758903503418\n",
      "step : 20.26 % , loss : 4.811828136444092\n",
      "step : 20.7 % , loss : 4.749439239501953\n",
      "step : 21.15 % , loss : 4.760043621063232\n",
      "step : 21.59 % , loss : 4.931126594543457\n",
      "step : 22.03 % , loss : 4.703945159912109\n",
      "step : 22.47 % , loss : 4.849902629852295\n",
      "step : 22.91 % , loss : 4.738818645477295\n",
      "step : 23.35 % , loss : 4.713039875030518\n",
      "step : 23.79 % , loss : 4.77268123626709\n",
      "step : 24.23 % , loss : 4.7113871574401855\n",
      "step : 24.67 % , loss : 4.815268039703369\n",
      "step : 25.11 % , loss : 4.839544773101807\n",
      "step : 25.55 % , loss : 4.800400733947754\n",
      "step : 25.99 % , loss : 4.729218482971191\n",
      "step : 26.43 % , loss : 4.707685470581055\n",
      "step : 26.87 % , loss : 4.695400714874268\n",
      "step : 27.31 % , loss : 4.80850887298584\n",
      "step : 27.75 % , loss : 4.721612930297852\n",
      "step : 28.19 % , loss : 4.754085063934326\n",
      "step : 28.63 % , loss : 4.750570774078369\n",
      "step : 29.07 % , loss : 4.819750785827637\n",
      "step : 29.52 % , loss : 4.8988728523254395\n",
      "step : 29.96 % , loss : 4.845770835876465\n",
      "step : 30.4 % , loss : 4.808850288391113\n",
      "step : 30.84 % , loss : 4.767975330352783\n",
      "step : 31.28 % , loss : 4.80224084854126\n",
      "step : 31.72 % , loss : 4.810870170593262\n",
      "step : 32.16 % , loss : 4.860759258270264\n",
      "step : 32.6 % , loss : 4.817227840423584\n",
      "step : 33.04 % , loss : 4.780646324157715\n",
      "step : 33.48 % , loss : 4.764326095581055\n",
      "step : 33.92 % , loss : 4.794857501983643\n",
      "step : 34.36 % , loss : 4.808880805969238\n",
      "step : 34.8 % , loss : 4.930546283721924\n",
      "step : 35.24 % , loss : 4.850308418273926\n",
      "step : 35.68 % , loss : 4.798519611358643\n",
      "step : 36.12 % , loss : 4.7439398765563965\n",
      "step : 36.56 % , loss : 4.763666152954102\n",
      "step : 37.0 % , loss : 4.8149309158325195\n",
      "step : 37.44 % , loss : 4.857433795928955\n",
      "step : 37.89 % , loss : 4.7490997314453125\n",
      "step : 38.33 % , loss : 4.865854263305664\n",
      "step : 38.77 % , loss : 4.822240829467773\n",
      "step : 39.21 % , loss : 4.79514217376709\n",
      "step : 39.65 % , loss : 4.677649021148682\n",
      "step : 40.09 % , loss : 4.804150581359863\n",
      "step : 40.53 % , loss : 4.68396520614624\n",
      "step : 40.97 % , loss : 4.800216197967529\n",
      "step : 41.41 % , loss : 4.842578887939453\n",
      "step : 41.85 % , loss : 4.758314609527588\n",
      "step : 42.29 % , loss : 4.699507236480713\n",
      "step : 42.73 % , loss : 4.72899055480957\n",
      "step : 43.17 % , loss : 4.777859687805176\n",
      "step : 43.61 % , loss : 4.7407331466674805\n",
      "step : 44.05 % , loss : 4.794258117675781\n",
      "step : 44.49 % , loss : 4.828859329223633\n",
      "step : 44.93 % , loss : 4.683644771575928\n",
      "step : 45.37 % , loss : 4.8315277099609375\n",
      "step : 45.81 % , loss : 4.700927734375\n",
      "step : 46.26 % , loss : 4.807826995849609\n",
      "step : 46.7 % , loss : 4.766676425933838\n",
      "step : 47.14 % , loss : 4.82603120803833\n",
      "step : 47.58 % , loss : 4.859320163726807\n",
      "step : 48.02 % , loss : 4.81118631362915\n",
      "step : 48.46 % , loss : 4.827241897583008\n",
      "step : 48.9 % , loss : 4.802400588989258\n",
      "step : 49.34 % , loss : 4.78424596786499\n",
      "step : 49.78 % , loss : 4.794839859008789\n",
      "step : 50.22 % , loss : 4.766907691955566\n",
      "step : 50.66 % , loss : 4.886888027191162\n",
      "step : 51.1 % , loss : 4.664048194885254\n",
      "step : 51.54 % , loss : 4.748021125793457\n",
      "step : 51.98 % , loss : 4.831099033355713\n",
      "step : 52.42 % , loss : 4.825921535491943\n",
      "step : 52.86 % , loss : 4.735827922821045\n",
      "step : 53.3 % , loss : 4.819705963134766\n",
      "step : 53.74 % , loss : 4.853857517242432\n",
      "step : 54.19 % , loss : 4.809656620025635\n",
      "step : 54.63 % , loss : 4.831501483917236\n",
      "step : 55.07 % , loss : 4.839873313903809\n",
      "step : 55.51 % , loss : 4.770253658294678\n",
      "step : 55.95 % , loss : 4.822756290435791\n",
      "step : 56.39 % , loss : 4.762527942657471\n",
      "step : 56.83 % , loss : 4.699401378631592\n",
      "step : 57.27 % , loss : 4.727423667907715\n",
      "step : 57.71 % , loss : 4.558027267456055\n",
      "step : 58.15 % , loss : 4.9234299659729\n",
      "step : 58.59 % , loss : 4.871963024139404\n",
      "step : 59.03 % , loss : 4.914172649383545\n",
      "step : 59.47 % , loss : 4.701785564422607\n",
      "step : 59.91 % , loss : 4.721101760864258\n",
      "step : 60.35 % , loss : 4.815394878387451\n",
      "step : 60.79 % , loss : 4.888320446014404\n",
      "step : 61.23 % , loss : 4.822132587432861\n",
      "step : 61.67 % , loss : 4.793981552124023\n",
      "step : 62.11 % , loss : 4.759574890136719\n",
      "step : 62.56 % , loss : 4.792962074279785\n",
      "step : 63.0 % , loss : 4.792144775390625\n",
      "step : 63.44 % , loss : 4.770476818084717\n",
      "step : 63.88 % , loss : 4.698196887969971\n",
      "step : 64.32 % , loss : 4.762389183044434\n",
      "step : 64.76 % , loss : 4.786367416381836\n",
      "step : 65.2 % , loss : 4.844412326812744\n",
      "step : 65.64 % , loss : 4.836465358734131\n",
      "step : 66.08 % , loss : 4.689791679382324\n",
      "step : 66.52 % , loss : 4.679120063781738\n",
      "step : 66.96 % , loss : 4.647568225860596\n",
      "step : 67.4 % , loss : 4.807526588439941\n",
      "step : 67.84 % , loss : 4.807783603668213\n",
      "step : 68.28 % , loss : 4.780375957489014\n",
      "step : 68.72 % , loss : 4.759511470794678\n",
      "step : 69.16 % , loss : 4.934333801269531\n",
      "step : 69.6 % , loss : 4.807653427124023\n",
      "step : 70.04 % , loss : 4.802406311035156\n",
      "step : 70.48 % , loss : 4.7706475257873535\n",
      "step : 70.93 % , loss : 4.747355937957764\n",
      "step : 71.37 % , loss : 4.770883560180664\n",
      "step : 71.81 % , loss : 4.741582870483398\n",
      "step : 72.25 % , loss : 4.874369144439697\n",
      "step : 72.69 % , loss : 4.732712745666504\n",
      "step : 73.13 % , loss : 4.854156494140625\n",
      "step : 73.57 % , loss : 4.833742141723633\n",
      "step : 74.01 % , loss : 4.766781330108643\n",
      "step : 74.45 % , loss : 4.690456867218018\n",
      "step : 74.89 % , loss : 4.674361228942871\n",
      "step : 75.33 % , loss : 4.751904010772705\n",
      "step : 75.77 % , loss : 4.740082263946533\n",
      "step : 76.21 % , loss : 4.814224720001221\n",
      "step : 76.65 % , loss : 4.695173740386963\n",
      "step : 77.09 % , loss : 4.788783550262451\n",
      "step : 77.53 % , loss : 4.7667236328125\n",
      "step : 77.97 % , loss : 4.722498893737793\n",
      "step : 78.41 % , loss : 4.78964376449585\n",
      "step : 78.85 % , loss : 4.723104000091553\n",
      "step : 79.3 % , loss : 4.74049186706543\n",
      "step : 79.74 % , loss : 4.815507888793945\n",
      "step : 80.18 % , loss : 4.778311729431152\n",
      "step : 80.62 % , loss : 4.832401275634766\n",
      "step : 81.06 % , loss : 4.705657958984375\n",
      "step : 81.5 % , loss : 4.808320999145508\n",
      "step : 81.94 % , loss : 4.761710166931152\n",
      "step : 82.38 % , loss : 4.85225248336792\n",
      "step : 82.82 % , loss : 4.774903774261475\n",
      "step : 83.26 % , loss : 4.6065216064453125\n",
      "step : 83.7 % , loss : 4.823905944824219\n",
      "step : 84.14 % , loss : 4.764747619628906\n",
      "step : 84.58 % , loss : 4.770211696624756\n",
      "step : 85.02 % , loss : 4.742483615875244\n",
      "step : 85.46 % , loss : 4.833102226257324\n",
      "step : 85.9 % , loss : 4.812372207641602\n",
      "step : 86.34 % , loss : 4.791097164154053\n",
      "step : 86.78 % , loss : 4.819771766662598\n",
      "step : 87.22 % , loss : 4.865655422210693\n",
      "step : 87.67 % , loss : 4.781768798828125\n",
      "step : 88.11 % , loss : 4.742240905761719\n",
      "step : 88.55 % , loss : 4.963576793670654\n",
      "step : 88.99 % , loss : 4.744073390960693\n",
      "step : 89.43 % , loss : 4.932579040527344\n",
      "step : 89.87 % , loss : 4.718173980712891\n",
      "step : 90.31 % , loss : 4.65495491027832\n",
      "step : 90.75 % , loss : 4.770120143890381\n",
      "step : 91.19 % , loss : 4.701423168182373\n",
      "step : 91.63 % , loss : 4.856309413909912\n",
      "step : 92.07 % , loss : 4.716569900512695\n",
      "step : 92.51 % , loss : 4.868880271911621\n",
      "step : 92.95 % , loss : 4.72147274017334\n",
      "step : 93.39 % , loss : 4.815124988555908\n",
      "step : 93.83 % , loss : 4.779753684997559\n",
      "step : 94.27 % , loss : 4.791260719299316\n",
      "step : 94.71 % , loss : 4.806293964385986\n",
      "step : 95.15 % , loss : 4.824365139007568\n",
      "step : 95.59 % , loss : 4.715490818023682\n",
      "step : 96.04 % , loss : 4.722054958343506\n",
      "step : 96.48 % , loss : 4.722409725189209\n",
      "step : 96.92 % , loss : 4.785881519317627\n",
      "step : 97.36 % , loss : 4.710122108459473\n",
      "step : 97.8 % , loss : 4.683222770690918\n",
      "step : 98.24 % , loss : 4.785828113555908\n",
      "step : 98.68 % , loss : 4.877722263336182\n",
      "step : 99.12 % , loss : 4.802685260772705\n",
      "step : 99.56 % , loss : 4.825936317443848\n",
      "\tTrain Loss: 4.781\n",
      "step : 0.0 % , loss : 4.806375980377197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0.44 % , loss : 4.841033935546875\n",
      "step : 0.88 % , loss : 4.610548496246338\n",
      "step : 1.32 % , loss : 4.6839494705200195\n",
      "step : 1.76 % , loss : 4.900564670562744\n",
      "step : 2.2 % , loss : 4.722233295440674\n",
      "step : 2.64 % , loss : 4.772395133972168\n",
      "step : 3.08 % , loss : 4.829907417297363\n",
      "step : 3.52 % , loss : 4.737273693084717\n",
      "step : 3.96 % , loss : 4.643975734710693\n",
      "step : 4.41 % , loss : 4.85781192779541\n",
      "step : 4.85 % , loss : 4.732597827911377\n",
      "step : 5.29 % , loss : 4.697195053100586\n",
      "step : 5.73 % , loss : 4.758936405181885\n",
      "step : 6.17 % , loss : 4.700862407684326\n",
      "step : 6.61 % , loss : 4.7127861976623535\n",
      "step : 7.05 % , loss : 4.717142581939697\n",
      "step : 7.49 % , loss : 4.877801895141602\n",
      "step : 7.93 % , loss : 4.763076305389404\n",
      "step : 8.37 % , loss : 4.692983150482178\n",
      "step : 8.81 % , loss : 4.644166946411133\n",
      "step : 9.25 % , loss : 4.760845184326172\n",
      "step : 9.69 % , loss : 4.731588363647461\n",
      "step : 10.13 % , loss : 4.623787879943848\n",
      "step : 10.57 % , loss : 4.707147598266602\n",
      "step : 11.01 % , loss : 4.726891994476318\n",
      "step : 11.45 % , loss : 4.7311553955078125\n",
      "step : 11.89 % , loss : 4.692798614501953\n",
      "step : 12.33 % , loss : 4.738437652587891\n",
      "step : 12.78 % , loss : 4.689600944519043\n",
      "step : 13.22 % , loss : 4.700474262237549\n",
      "step : 13.66 % , loss : 4.697152614593506\n",
      "step : 14.1 % , loss : 4.773582935333252\n",
      "step : 14.54 % , loss : 4.68243932723999\n",
      "step : 14.98 % , loss : 4.7629265785217285\n",
      "step : 15.42 % , loss : 4.676896572113037\n",
      "step : 15.86 % , loss : 4.746785640716553\n",
      "step : 16.3 % , loss : 4.801422595977783\n",
      "step : 16.74 % , loss : 4.805469989776611\n",
      "step : 17.18 % , loss : 4.753185272216797\n",
      "step : 17.62 % , loss : 4.759835720062256\n",
      "step : 18.06 % , loss : 4.730951309204102\n",
      "step : 18.5 % , loss : 4.891505718231201\n",
      "step : 18.94 % , loss : 4.683810234069824\n",
      "step : 19.38 % , loss : 4.798287868499756\n",
      "step : 19.82 % , loss : 4.76922082901001\n",
      "step : 20.26 % , loss : 4.787743091583252\n",
      "step : 20.7 % , loss : 4.770382881164551\n",
      "step : 21.15 % , loss : 4.678558826446533\n",
      "step : 21.59 % , loss : 4.92218017578125\n",
      "step : 22.03 % , loss : 4.715127944946289\n",
      "step : 22.47 % , loss : 4.825239181518555\n",
      "step : 22.91 % , loss : 4.891517162322998\n",
      "step : 23.35 % , loss : 4.730924606323242\n",
      "step : 23.79 % , loss : 4.867317199707031\n",
      "step : 24.23 % , loss : 4.8419508934021\n",
      "step : 24.67 % , loss : 4.718936443328857\n",
      "step : 25.11 % , loss : 4.68856143951416\n",
      "step : 25.55 % , loss : 4.915828704833984\n",
      "step : 25.99 % , loss : 4.7694621086120605\n",
      "step : 26.43 % , loss : 4.617144584655762\n",
      "step : 26.87 % , loss : 4.827837944030762\n",
      "step : 27.31 % , loss : 4.763779640197754\n",
      "step : 27.75 % , loss : 4.896172046661377\n",
      "step : 28.19 % , loss : 4.630303382873535\n",
      "step : 28.63 % , loss : 4.778964042663574\n",
      "step : 29.07 % , loss : 4.832127094268799\n",
      "step : 29.52 % , loss : 4.838978290557861\n",
      "step : 29.96 % , loss : 4.870701789855957\n",
      "step : 30.4 % , loss : 4.778195381164551\n",
      "step : 30.84 % , loss : 4.716433525085449\n",
      "step : 31.28 % , loss : 4.714510440826416\n",
      "step : 31.72 % , loss : 4.7499494552612305\n",
      "step : 32.16 % , loss : 4.723887920379639\n",
      "step : 32.6 % , loss : 4.7332634925842285\n",
      "step : 33.04 % , loss : 4.71398401260376\n",
      "step : 33.48 % , loss : 4.721139430999756\n",
      "step : 33.92 % , loss : 4.851120948791504\n",
      "step : 34.36 % , loss : 4.637357234954834\n",
      "step : 34.8 % , loss : 4.893843173980713\n",
      "step : 35.24 % , loss : 4.671950340270996\n",
      "step : 35.68 % , loss : 4.780425071716309\n",
      "step : 36.12 % , loss : 4.776655197143555\n",
      "step : 36.56 % , loss : 4.820346832275391\n",
      "step : 37.0 % , loss : 4.813663959503174\n",
      "step : 37.44 % , loss : 4.662549018859863\n",
      "step : 37.89 % , loss : 4.869655132293701\n",
      "step : 38.33 % , loss : 4.736087322235107\n",
      "step : 38.77 % , loss : 4.714657306671143\n",
      "step : 39.21 % , loss : 4.803141117095947\n",
      "step : 39.65 % , loss : 4.810111999511719\n",
      "step : 40.09 % , loss : 4.745439052581787\n",
      "step : 40.53 % , loss : 4.657440185546875\n",
      "step : 40.97 % , loss : 4.806559085845947\n",
      "step : 41.41 % , loss : 4.67840051651001\n",
      "step : 41.85 % , loss : 4.8400492668151855\n",
      "step : 42.29 % , loss : 4.768938064575195\n",
      "step : 42.73 % , loss : 4.698092937469482\n",
      "step : 43.17 % , loss : 4.81231164932251\n",
      "step : 43.61 % , loss : 4.762528896331787\n",
      "step : 44.05 % , loss : 4.704591751098633\n",
      "step : 44.49 % , loss : 4.767148494720459\n",
      "step : 44.93 % , loss : 4.807575702667236\n",
      "step : 45.37 % , loss : 4.756235122680664\n",
      "step : 45.81 % , loss : 4.8393144607543945\n",
      "step : 46.26 % , loss : 4.791966915130615\n",
      "step : 46.7 % , loss : 4.7401347160339355\n",
      "step : 47.14 % , loss : 4.721044063568115\n",
      "step : 47.58 % , loss : 4.709245204925537\n",
      "step : 48.02 % , loss : 4.886860370635986\n",
      "step : 48.46 % , loss : 4.701390743255615\n",
      "step : 48.9 % , loss : 4.664486885070801\n",
      "step : 49.34 % , loss : 4.720727920532227\n",
      "step : 49.78 % , loss : 4.661651611328125\n",
      "step : 50.22 % , loss : 4.709517002105713\n",
      "step : 50.66 % , loss : 4.635497093200684\n",
      "step : 51.1 % , loss : 4.5922040939331055\n",
      "step : 51.54 % , loss : 4.776927471160889\n",
      "step : 51.98 % , loss : 4.78509521484375\n",
      "step : 52.42 % , loss : 4.662938594818115\n",
      "step : 52.86 % , loss : 4.783190727233887\n",
      "step : 53.3 % , loss : 4.726060390472412\n",
      "step : 53.74 % , loss : 4.772339344024658\n",
      "step : 54.19 % , loss : 4.72430419921875\n",
      "step : 54.63 % , loss : 4.796805381774902\n",
      "step : 55.07 % , loss : 4.767807960510254\n",
      "step : 55.51 % , loss : 4.663918972015381\n",
      "step : 55.95 % , loss : 4.890873432159424\n",
      "step : 56.39 % , loss : 4.5536932945251465\n",
      "step : 56.83 % , loss : 4.726081848144531\n",
      "step : 57.27 % , loss : 4.726101875305176\n",
      "step : 57.71 % , loss : 4.767776966094971\n",
      "step : 58.15 % , loss : 4.703945636749268\n",
      "step : 58.59 % , loss : 4.615948677062988\n",
      "step : 59.03 % , loss : 4.754296779632568\n",
      "step : 59.47 % , loss : 4.768984794616699\n",
      "step : 59.91 % , loss : 4.728832244873047\n",
      "step : 60.35 % , loss : 4.676787376403809\n",
      "step : 60.79 % , loss : 4.727514266967773\n",
      "step : 61.23 % , loss : 4.684218883514404\n",
      "step : 61.67 % , loss : 4.638261318206787\n",
      "step : 62.11 % , loss : 4.659633159637451\n",
      "step : 62.56 % , loss : 4.753489971160889\n",
      "step : 63.0 % , loss : 4.806643486022949\n",
      "step : 63.44 % , loss : 4.595522403717041\n",
      "step : 63.88 % , loss : 4.728490829467773\n",
      "step : 64.32 % , loss : 4.775211811065674\n",
      "step : 64.76 % , loss : 4.766770362854004\n",
      "step : 65.2 % , loss : 4.646275043487549\n",
      "step : 65.64 % , loss : 4.715096473693848\n",
      "step : 66.08 % , loss : 4.72678279876709\n",
      "step : 66.52 % , loss : 4.786435127258301\n",
      "step : 66.96 % , loss : 4.743750095367432\n",
      "step : 67.4 % , loss : 4.790654182434082\n",
      "step : 67.84 % , loss : 4.79940938949585\n",
      "step : 68.28 % , loss : 4.666820049285889\n",
      "step : 68.72 % , loss : 4.768335819244385\n",
      "step : 69.16 % , loss : 4.668823719024658\n",
      "step : 69.6 % , loss : 4.79078483581543\n",
      "step : 70.04 % , loss : 4.825676441192627\n",
      "step : 70.48 % , loss : 4.665195465087891\n",
      "step : 70.93 % , loss : 4.802708148956299\n",
      "step : 71.37 % , loss : 4.725009918212891\n",
      "step : 71.81 % , loss : 4.845476150512695\n",
      "step : 72.25 % , loss : 4.695215702056885\n",
      "step : 72.69 % , loss : 4.706002235412598\n",
      "step : 73.13 % , loss : 4.799590587615967\n",
      "step : 73.57 % , loss : 4.676781177520752\n",
      "step : 74.01 % , loss : 4.767577648162842\n",
      "step : 74.45 % , loss : 4.5364580154418945\n",
      "step : 74.89 % , loss : 4.896242618560791\n",
      "step : 75.33 % , loss : 4.78647518157959\n",
      "step : 75.77 % , loss : 4.717689514160156\n",
      "step : 76.21 % , loss : 4.79428243637085\n",
      "step : 76.65 % , loss : 4.827334403991699\n",
      "step : 77.09 % , loss : 4.769039630889893\n",
      "step : 77.53 % , loss : 4.772899627685547\n",
      "step : 77.97 % , loss : 4.735368728637695\n",
      "step : 78.41 % , loss : 4.735217094421387\n",
      "step : 78.85 % , loss : 4.693591594696045\n",
      "step : 79.3 % , loss : 4.694590091705322\n",
      "step : 79.74 % , loss : 4.672016620635986\n",
      "step : 80.18 % , loss : 4.8480658531188965\n",
      "step : 80.62 % , loss : 4.693435192108154\n",
      "step : 81.06 % , loss : 4.864085674285889\n",
      "step : 81.5 % , loss : 4.864124298095703\n",
      "step : 81.94 % , loss : 4.734684944152832\n",
      "step : 82.38 % , loss : 4.842048168182373\n",
      "step : 82.82 % , loss : 4.788309574127197\n",
      "step : 83.26 % , loss : 4.723018169403076\n",
      "step : 83.7 % , loss : 4.734647274017334\n",
      "step : 84.14 % , loss : 4.8096604347229\n",
      "step : 84.58 % , loss : 4.75921106338501\n",
      "step : 85.02 % , loss : 4.891010284423828\n",
      "step : 85.46 % , loss : 4.674811363220215\n",
      "step : 85.9 % , loss : 4.730905055999756\n",
      "step : 86.34 % , loss : 4.753931045532227\n",
      "step : 86.78 % , loss : 4.722154140472412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 87.22 % , loss : 4.7835259437561035\n",
      "step : 87.67 % , loss : 4.896514892578125\n",
      "step : 88.11 % , loss : 4.8505754470825195\n",
      "step : 88.55 % , loss : 4.7061920166015625\n",
      "step : 88.99 % , loss : 4.6461687088012695\n",
      "step : 89.43 % , loss : 4.681893348693848\n",
      "step : 89.87 % , loss : 4.72335147857666\n",
      "step : 90.31 % , loss : 4.803304672241211\n",
      "step : 90.75 % , loss : 4.77405309677124\n",
      "step : 91.19 % , loss : 4.636674404144287\n",
      "step : 91.63 % , loss : 4.6953558921813965\n",
      "step : 92.07 % , loss : 4.8267717361450195\n",
      "step : 92.51 % , loss : 4.798348426818848\n",
      "step : 92.95 % , loss : 4.68461799621582\n",
      "step : 93.39 % , loss : 4.763818740844727\n",
      "step : 93.83 % , loss : 4.723352909088135\n",
      "step : 94.27 % , loss : 4.761002063751221\n",
      "step : 94.71 % , loss : 4.811795711517334\n",
      "step : 95.15 % , loss : 4.75836706161499\n",
      "step : 95.59 % , loss : 4.692195892333984\n",
      "step : 96.04 % , loss : 4.75598669052124\n",
      "step : 96.48 % , loss : 4.749587059020996\n",
      "step : 96.92 % , loss : 4.590898036956787\n",
      "step : 97.36 % , loss : 4.7060723304748535\n",
      "step : 97.8 % , loss : 4.693413257598877\n",
      "step : 98.24 % , loss : 4.699646949768066\n",
      "step : 98.68 % , loss : 4.744532585144043\n",
      "step : 99.12 % , loss : 4.703464508056641\n",
      "step : 99.56 % , loss : 4.5810160636901855\n",
      "\tTrain Loss: 4.747\n",
      "step : 0.0 % , loss : 4.710019111633301\n",
      "step : 0.44 % , loss : 4.774290561676025\n",
      "step : 0.88 % , loss : 4.741103172302246\n",
      "step : 1.32 % , loss : 4.713411808013916\n",
      "step : 1.76 % , loss : 4.791575908660889\n",
      "step : 2.2 % , loss : 4.698587417602539\n",
      "step : 2.64 % , loss : 4.699361324310303\n",
      "step : 3.08 % , loss : 4.6313090324401855\n",
      "step : 3.52 % , loss : 4.814198017120361\n",
      "step : 3.96 % , loss : 4.613511085510254\n",
      "step : 4.41 % , loss : 4.715252876281738\n",
      "step : 4.85 % , loss : 4.754829406738281\n",
      "step : 5.29 % , loss : 4.645261287689209\n",
      "step : 5.73 % , loss : 4.729618072509766\n",
      "step : 6.17 % , loss : 4.804911136627197\n",
      "step : 6.61 % , loss : 4.681789875030518\n",
      "step : 7.05 % , loss : 4.760834217071533\n",
      "step : 7.49 % , loss : 4.705941677093506\n",
      "step : 7.93 % , loss : 4.7423176765441895\n",
      "step : 8.37 % , loss : 4.745432376861572\n",
      "step : 8.81 % , loss : 4.705063819885254\n",
      "step : 9.25 % , loss : 4.774597644805908\n",
      "step : 9.69 % , loss : 4.7631449699401855\n",
      "step : 10.13 % , loss : 4.569891929626465\n",
      "step : 10.57 % , loss : 4.765843391418457\n",
      "step : 11.01 % , loss : 4.727180004119873\n",
      "step : 11.45 % , loss : 4.659955978393555\n",
      "step : 11.89 % , loss : 4.686493873596191\n",
      "step : 12.33 % , loss : 4.717064380645752\n",
      "step : 12.78 % , loss : 4.7305474281311035\n",
      "step : 13.22 % , loss : 4.824143886566162\n",
      "step : 13.66 % , loss : 4.6964569091796875\n",
      "step : 14.1 % , loss : 4.788514614105225\n",
      "step : 14.54 % , loss : 4.647547721862793\n",
      "step : 14.98 % , loss : 4.731135368347168\n",
      "step : 15.42 % , loss : 4.635477542877197\n",
      "step : 15.86 % , loss : 4.72886848449707\n",
      "step : 16.3 % , loss : 4.712601184844971\n",
      "step : 16.74 % , loss : 4.710288047790527\n",
      "step : 17.18 % , loss : 4.788864612579346\n",
      "step : 17.62 % , loss : 4.614582061767578\n",
      "step : 18.06 % , loss : 4.736405849456787\n",
      "step : 18.5 % , loss : 4.798583984375\n",
      "step : 18.94 % , loss : 4.6880621910095215\n",
      "step : 19.38 % , loss : 4.684876918792725\n",
      "step : 19.82 % , loss : 4.711780071258545\n",
      "step : 20.26 % , loss : 4.820841312408447\n",
      "step : 20.7 % , loss : 4.763671398162842\n",
      "step : 21.15 % , loss : 4.750243663787842\n",
      "step : 21.59 % , loss : 4.772367477416992\n",
      "step : 22.03 % , loss : 4.640286445617676\n",
      "step : 22.47 % , loss : 4.653631210327148\n",
      "step : 22.91 % , loss : 4.763872146606445\n",
      "step : 23.35 % , loss : 4.772361755371094\n",
      "step : 23.79 % , loss : 4.635989189147949\n",
      "step : 24.23 % , loss : 4.80604362487793\n",
      "step : 24.67 % , loss : 4.772088050842285\n",
      "step : 25.11 % , loss : 4.636746883392334\n",
      "step : 25.55 % , loss : 4.706594467163086\n",
      "step : 25.99 % , loss : 4.768083095550537\n",
      "step : 26.43 % , loss : 4.667340278625488\n",
      "step : 26.87 % , loss : 4.657013893127441\n",
      "step : 27.31 % , loss : 4.620790481567383\n",
      "step : 27.75 % , loss : 4.608727931976318\n",
      "step : 28.19 % , loss : 4.710395336151123\n",
      "step : 28.63 % , loss : 4.737964630126953\n",
      "step : 29.07 % , loss : 4.833921909332275\n",
      "step : 29.52 % , loss : 4.743653297424316\n",
      "step : 29.96 % , loss : 4.679619789123535\n",
      "step : 30.4 % , loss : 4.72403621673584\n",
      "step : 30.84 % , loss : 4.68731164932251\n",
      "step : 31.28 % , loss : 4.6782660484313965\n",
      "step : 31.72 % , loss : 4.677796363830566\n",
      "step : 32.16 % , loss : 4.766749858856201\n",
      "step : 32.6 % , loss : 4.768332004547119\n",
      "step : 33.04 % , loss : 4.778030872344971\n",
      "step : 33.48 % , loss : 4.769669055938721\n",
      "step : 33.92 % , loss : 4.641899585723877\n",
      "step : 34.36 % , loss : 4.67131233215332\n",
      "step : 34.8 % , loss : 4.795582294464111\n",
      "step : 35.24 % , loss : 4.781533241271973\n",
      "step : 35.68 % , loss : 4.734323501586914\n",
      "step : 36.12 % , loss : 4.681881427764893\n",
      "step : 36.56 % , loss : 4.683096885681152\n",
      "step : 37.0 % , loss : 4.683635711669922\n",
      "step : 37.44 % , loss : 4.6260504722595215\n",
      "step : 37.89 % , loss : 4.671803951263428\n",
      "step : 38.33 % , loss : 4.6860504150390625\n",
      "step : 38.77 % , loss : 4.687917232513428\n",
      "step : 39.21 % , loss : 4.681972980499268\n",
      "step : 39.65 % , loss : 4.719601631164551\n",
      "step : 40.09 % , loss : 4.68738317489624\n",
      "step : 40.53 % , loss : 4.658123970031738\n",
      "step : 40.97 % , loss : 4.760289192199707\n",
      "step : 41.41 % , loss : 4.787145614624023\n",
      "step : 41.85 % , loss : 4.74495792388916\n",
      "step : 42.29 % , loss : 4.689070224761963\n",
      "step : 42.73 % , loss : 4.659762382507324\n",
      "step : 43.17 % , loss : 4.764365196228027\n",
      "step : 43.61 % , loss : 4.648310661315918\n",
      "step : 44.05 % , loss : 4.627232074737549\n",
      "step : 44.49 % , loss : 4.691471099853516\n",
      "step : 44.93 % , loss : 4.758864879608154\n",
      "step : 45.37 % , loss : 4.783892631530762\n",
      "step : 45.81 % , loss : 4.684016227722168\n",
      "step : 46.26 % , loss : 4.758243083953857\n",
      "step : 46.7 % , loss : 4.730037689208984\n",
      "step : 47.14 % , loss : 4.730918884277344\n",
      "step : 47.58 % , loss : 4.660416126251221\n",
      "step : 48.02 % , loss : 4.746977806091309\n",
      "step : 48.46 % , loss : 4.819311141967773\n",
      "step : 48.9 % , loss : 4.74031400680542\n",
      "step : 49.34 % , loss : 4.737002372741699\n",
      "step : 49.78 % , loss : 4.789440631866455\n",
      "step : 50.22 % , loss : 4.572778224945068\n",
      "step : 50.66 % , loss : 4.774038314819336\n",
      "step : 51.1 % , loss : 4.7416558265686035\n",
      "step : 51.54 % , loss : 4.821760177612305\n",
      "step : 51.98 % , loss : 4.674449920654297\n",
      "step : 52.42 % , loss : 4.8173112869262695\n",
      "step : 52.86 % , loss : 4.764598369598389\n",
      "step : 53.3 % , loss : 4.86041259765625\n",
      "step : 53.74 % , loss : 4.712968349456787\n",
      "step : 54.19 % , loss : 4.715915679931641\n",
      "step : 54.63 % , loss : 4.6202263832092285\n",
      "step : 55.07 % , loss : 4.731175422668457\n",
      "step : 55.51 % , loss : 4.6432390213012695\n",
      "step : 55.95 % , loss : 4.58133602142334\n",
      "step : 56.39 % , loss : 4.826578140258789\n",
      "step : 56.83 % , loss : 4.658970355987549\n",
      "step : 57.27 % , loss : 4.562615394592285\n",
      "step : 57.71 % , loss : 4.77329683303833\n",
      "step : 58.15 % , loss : 4.806751251220703\n",
      "step : 58.59 % , loss : 4.758910655975342\n",
      "step : 59.03 % , loss : 4.601126670837402\n",
      "step : 59.47 % , loss : 4.704789638519287\n",
      "step : 59.91 % , loss : 4.817462921142578\n",
      "step : 60.35 % , loss : 4.726628303527832\n",
      "step : 60.79 % , loss : 4.618104457855225\n",
      "step : 61.23 % , loss : 4.65949010848999\n",
      "step : 61.67 % , loss : 4.702670097351074\n",
      "step : 62.11 % , loss : 4.716458797454834\n",
      "step : 62.56 % , loss : 4.793375492095947\n",
      "step : 63.0 % , loss : 4.6554460525512695\n",
      "step : 63.44 % , loss : 4.7080841064453125\n",
      "step : 63.88 % , loss : 4.6677350997924805\n",
      "step : 64.32 % , loss : 4.738305568695068\n",
      "step : 64.76 % , loss : 4.711075305938721\n",
      "step : 65.2 % , loss : 4.572690486907959\n",
      "step : 65.64 % , loss : 4.632153034210205\n",
      "step : 66.08 % , loss : 4.5741400718688965\n",
      "step : 66.52 % , loss : 4.806164264678955\n",
      "step : 66.96 % , loss : 4.661708354949951\n",
      "step : 67.4 % , loss : 4.694648742675781\n",
      "step : 67.84 % , loss : 4.623062610626221\n",
      "step : 68.28 % , loss : 4.702587604522705\n",
      "step : 68.72 % , loss : 4.7753424644470215\n",
      "step : 69.16 % , loss : 4.629566669464111\n",
      "step : 69.6 % , loss : 4.623702049255371\n",
      "step : 70.04 % , loss : 4.6840925216674805\n",
      "step : 70.48 % , loss : 4.74392557144165\n",
      "step : 70.93 % , loss : 4.703388690948486\n",
      "step : 71.37 % , loss : 4.783131122589111\n",
      "step : 71.81 % , loss : 4.687614917755127\n",
      "step : 72.25 % , loss : 4.688215732574463\n",
      "step : 72.69 % , loss : 4.629574298858643\n",
      "step : 73.13 % , loss : 4.7109293937683105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 73.57 % , loss : 4.620175361633301\n",
      "step : 74.01 % , loss : 4.697422504425049\n",
      "step : 74.45 % , loss : 4.720780372619629\n",
      "step : 74.89 % , loss : 4.728488922119141\n",
      "step : 75.33 % , loss : 4.7698893547058105\n",
      "step : 75.77 % , loss : 4.693301200866699\n",
      "step : 76.21 % , loss : 4.714695930480957\n",
      "step : 76.65 % , loss : 4.671791076660156\n",
      "step : 77.09 % , loss : 4.714427471160889\n",
      "step : 77.53 % , loss : 4.6954803466796875\n",
      "step : 77.97 % , loss : 4.674928188323975\n",
      "step : 78.41 % , loss : 4.787058353424072\n",
      "step : 78.85 % , loss : 4.715897560119629\n",
      "step : 79.3 % , loss : 4.591354846954346\n",
      "step : 79.74 % , loss : 4.7899675369262695\n",
      "step : 80.18 % , loss : 4.661395072937012\n",
      "step : 80.62 % , loss : 4.655679702758789\n",
      "step : 81.06 % , loss : 4.658688068389893\n",
      "step : 81.5 % , loss : 4.708578109741211\n",
      "step : 81.94 % , loss : 4.670774936676025\n",
      "step : 82.38 % , loss : 4.775449752807617\n",
      "step : 82.82 % , loss : 4.721155643463135\n",
      "step : 83.26 % , loss : 4.634347915649414\n",
      "step : 83.7 % , loss : 4.705411911010742\n",
      "step : 84.14 % , loss : 4.830935478210449\n",
      "step : 84.58 % , loss : 4.713573455810547\n",
      "step : 85.02 % , loss : 4.663311958312988\n",
      "step : 85.46 % , loss : 4.687574863433838\n",
      "step : 85.9 % , loss : 4.800604343414307\n",
      "step : 86.34 % , loss : 4.74379825592041\n",
      "step : 86.78 % , loss : 4.621105194091797\n",
      "step : 87.22 % , loss : 4.649444580078125\n",
      "step : 87.67 % , loss : 4.706561088562012\n",
      "step : 88.11 % , loss : 4.747704029083252\n",
      "step : 88.55 % , loss : 4.784579277038574\n",
      "step : 88.99 % , loss : 4.692141532897949\n",
      "step : 89.43 % , loss : 4.783736228942871\n",
      "step : 89.87 % , loss : 4.624140739440918\n",
      "step : 90.31 % , loss : 4.671049118041992\n",
      "step : 90.75 % , loss : 4.547400951385498\n",
      "step : 91.19 % , loss : 4.862729072570801\n",
      "step : 91.63 % , loss : 4.765889644622803\n",
      "step : 92.07 % , loss : 4.66740608215332\n",
      "step : 92.51 % , loss : 4.619548320770264\n",
      "step : 92.95 % , loss : 4.6188530921936035\n",
      "step : 93.39 % , loss : 4.608329772949219\n",
      "step : 93.83 % , loss : 4.6885857582092285\n",
      "step : 94.27 % , loss : 4.627542018890381\n",
      "step : 94.71 % , loss : 4.643870830535889\n",
      "step : 95.15 % , loss : 4.787621021270752\n",
      "step : 95.59 % , loss : 4.807596683502197\n",
      "step : 96.04 % , loss : 4.602598667144775\n",
      "step : 96.48 % , loss : 4.847988128662109\n",
      "step : 96.92 % , loss : 4.604287147521973\n",
      "step : 97.36 % , loss : 4.604614734649658\n",
      "step : 97.8 % , loss : 4.720920085906982\n",
      "step : 98.24 % , loss : 4.66612434387207\n",
      "step : 98.68 % , loss : 4.792884349822998\n",
      "step : 99.12 % , loss : 4.677950859069824\n",
      "step : 99.56 % , loss : 4.774925231933594\n",
      "\tTrain Loss: 4.709\n",
      "step : 0.0 % , loss : 4.683480739593506\n",
      "step : 0.44 % , loss : 4.674598693847656\n",
      "step : 0.88 % , loss : 4.719980239868164\n",
      "step : 1.32 % , loss : 4.709841728210449\n",
      "step : 1.76 % , loss : 4.736136436462402\n",
      "step : 2.2 % , loss : 4.6277756690979\n",
      "step : 2.64 % , loss : 4.758681774139404\n",
      "step : 3.08 % , loss : 4.570324897766113\n",
      "step : 3.52 % , loss : 4.640499591827393\n",
      "step : 3.96 % , loss : 4.750033855438232\n",
      "step : 4.41 % , loss : 4.630970478057861\n",
      "step : 4.85 % , loss : 4.643562316894531\n",
      "step : 5.29 % , loss : 4.708858966827393\n",
      "step : 5.73 % , loss : 4.630438804626465\n",
      "step : 6.17 % , loss : 4.646488189697266\n",
      "step : 6.61 % , loss : 4.622763156890869\n",
      "step : 7.05 % , loss : 4.728816509246826\n",
      "step : 7.49 % , loss : 4.633243560791016\n",
      "step : 7.93 % , loss : 4.727352619171143\n",
      "step : 8.37 % , loss : 4.661543846130371\n",
      "step : 8.81 % , loss : 4.721443176269531\n",
      "step : 9.25 % , loss : 4.747708797454834\n",
      "step : 9.69 % , loss : 4.648444175720215\n",
      "step : 10.13 % , loss : 4.646150588989258\n",
      "step : 10.57 % , loss : 4.723201751708984\n",
      "step : 11.01 % , loss : 4.622490406036377\n",
      "step : 11.45 % , loss : 4.7023162841796875\n",
      "step : 11.89 % , loss : 4.709920406341553\n",
      "step : 12.33 % , loss : 4.61413049697876\n",
      "step : 12.78 % , loss : 4.773925304412842\n",
      "step : 13.22 % , loss : 4.659788131713867\n",
      "step : 13.66 % , loss : 4.714914321899414\n",
      "step : 14.1 % , loss : 4.754593849182129\n",
      "step : 14.54 % , loss : 4.6970624923706055\n",
      "step : 14.98 % , loss : 4.776515007019043\n",
      "step : 15.42 % , loss : 4.773430347442627\n",
      "step : 15.86 % , loss : 4.668765068054199\n",
      "step : 16.3 % , loss : 4.7636308670043945\n",
      "step : 16.74 % , loss : 4.673610210418701\n",
      "step : 17.18 % , loss : 4.593384265899658\n",
      "step : 17.62 % , loss : 4.705843448638916\n",
      "step : 18.06 % , loss : 4.705261707305908\n",
      "step : 18.5 % , loss : 4.6754069328308105\n",
      "step : 18.94 % , loss : 4.727304458618164\n",
      "step : 19.38 % , loss : 4.718386173248291\n",
      "step : 19.82 % , loss : 4.69573974609375\n",
      "step : 20.26 % , loss : 4.741326332092285\n",
      "step : 20.7 % , loss : 4.568914890289307\n",
      "step : 21.15 % , loss : 4.637031078338623\n",
      "step : 21.59 % , loss : 4.7910051345825195\n",
      "step : 22.03 % , loss : 4.60709810256958\n",
      "step : 22.47 % , loss : 4.679103851318359\n",
      "step : 22.91 % , loss : 4.7137298583984375\n",
      "step : 23.35 % , loss : 4.715002536773682\n",
      "step : 23.79 % , loss : 4.732505798339844\n",
      "step : 24.23 % , loss : 4.674777984619141\n",
      "step : 24.67 % , loss : 4.72047758102417\n",
      "step : 25.11 % , loss : 4.703464508056641\n",
      "step : 25.55 % , loss : 4.581698417663574\n",
      "step : 25.99 % , loss : 4.658032417297363\n",
      "step : 26.43 % , loss : 4.519987106323242\n",
      "step : 26.87 % , loss : 4.6948161125183105\n",
      "step : 27.31 % , loss : 4.697027206420898\n",
      "step : 27.75 % , loss : 4.75451135635376\n",
      "step : 28.19 % , loss : 4.697081565856934\n",
      "step : 28.63 % , loss : 4.554884433746338\n",
      "step : 29.07 % , loss : 4.653386116027832\n",
      "step : 29.52 % , loss : 4.6095662117004395\n",
      "step : 29.96 % , loss : 4.761026859283447\n",
      "step : 30.4 % , loss : 4.650528430938721\n",
      "step : 30.84 % , loss : 4.587433338165283\n",
      "step : 31.28 % , loss : 4.682972431182861\n",
      "step : 31.72 % , loss : 4.740058898925781\n",
      "step : 32.16 % , loss : 4.584746837615967\n",
      "step : 32.6 % , loss : 4.786062717437744\n",
      "step : 33.04 % , loss : 4.5427470207214355\n",
      "step : 33.48 % , loss : 4.709836006164551\n",
      "step : 33.92 % , loss : 4.724263668060303\n",
      "step : 34.36 % , loss : 4.557607650756836\n",
      "step : 34.8 % , loss : 4.733199119567871\n",
      "step : 35.24 % , loss : 4.665130615234375\n",
      "step : 35.68 % , loss : 4.658507347106934\n",
      "step : 36.12 % , loss : 4.76122522354126\n",
      "step : 36.56 % , loss : 4.574409484863281\n",
      "step : 37.0 % , loss : 4.554780006408691\n",
      "step : 37.44 % , loss : 4.739372253417969\n",
      "step : 37.89 % , loss : 4.550773620605469\n",
      "step : 38.33 % , loss : 4.66494083404541\n",
      "step : 38.77 % , loss : 4.651982307434082\n",
      "step : 39.21 % , loss : 4.55437707901001\n",
      "step : 39.65 % , loss : 4.623912811279297\n",
      "step : 40.09 % , loss : 4.695624828338623\n",
      "step : 40.53 % , loss : 4.684125900268555\n",
      "step : 40.97 % , loss : 4.755493640899658\n",
      "step : 41.41 % , loss : 4.649128437042236\n",
      "step : 41.85 % , loss : 4.561883449554443\n",
      "step : 42.29 % , loss : 4.722175598144531\n",
      "step : 42.73 % , loss : 4.736556053161621\n",
      "step : 43.17 % , loss : 4.639764785766602\n",
      "step : 43.61 % , loss : 4.636984348297119\n",
      "step : 44.05 % , loss : 4.617498397827148\n",
      "step : 44.49 % , loss : 4.8311872482299805\n",
      "step : 44.93 % , loss : 4.713573455810547\n",
      "step : 45.37 % , loss : 4.708652973175049\n",
      "step : 45.81 % , loss : 4.686882495880127\n",
      "step : 46.26 % , loss : 4.711766242980957\n",
      "step : 46.7 % , loss : 4.659265518188477\n",
      "step : 47.14 % , loss : 4.617991924285889\n",
      "step : 47.58 % , loss : 4.679888725280762\n",
      "step : 48.02 % , loss : 4.617098331451416\n",
      "step : 48.46 % , loss : 4.642971038818359\n",
      "step : 48.9 % , loss : 4.601572036743164\n",
      "step : 49.34 % , loss : 4.668905735015869\n",
      "step : 49.78 % , loss : 4.630745887756348\n",
      "step : 50.22 % , loss : 4.607555389404297\n",
      "step : 50.66 % , loss : 4.669600963592529\n",
      "step : 51.1 % , loss : 4.724725723266602\n",
      "step : 51.54 % , loss : 4.841169834136963\n",
      "step : 51.98 % , loss : 4.714022159576416\n",
      "step : 52.42 % , loss : 4.591026782989502\n",
      "step : 52.86 % , loss : 4.740661144256592\n",
      "step : 53.3 % , loss : 4.619678974151611\n",
      "step : 53.74 % , loss : 4.5864644050598145\n",
      "step : 54.19 % , loss : 4.655666351318359\n",
      "step : 54.63 % , loss : 4.637356758117676\n",
      "step : 55.07 % , loss : 4.728848457336426\n",
      "step : 55.51 % , loss : 4.6418256759643555\n",
      "step : 55.95 % , loss : 4.678906440734863\n",
      "step : 56.39 % , loss : 4.724621772766113\n",
      "step : 56.83 % , loss : 4.510075092315674\n",
      "step : 57.27 % , loss : 4.777553558349609\n",
      "step : 57.71 % , loss : 4.680967330932617\n",
      "step : 58.15 % , loss : 4.827908039093018\n",
      "step : 58.59 % , loss : 4.610604763031006\n",
      "step : 59.03 % , loss : 4.621089935302734\n",
      "step : 59.47 % , loss : 4.735662937164307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 59.91 % , loss : 4.752631664276123\n",
      "step : 60.35 % , loss : 4.731271266937256\n",
      "step : 60.79 % , loss : 4.615355014801025\n",
      "step : 61.23 % , loss : 4.721549987792969\n",
      "step : 61.67 % , loss : 4.751374244689941\n",
      "step : 62.11 % , loss : 4.640102863311768\n",
      "step : 62.56 % , loss : 4.697137832641602\n",
      "step : 63.0 % , loss : 4.699040412902832\n",
      "step : 63.44 % , loss : 4.744566917419434\n",
      "step : 63.88 % , loss : 4.65928316116333\n",
      "step : 64.32 % , loss : 4.717610836029053\n",
      "step : 64.76 % , loss : 4.699338436126709\n",
      "step : 65.2 % , loss : 4.673746109008789\n",
      "step : 65.64 % , loss : 4.753176689147949\n",
      "step : 66.08 % , loss : 4.570651531219482\n",
      "step : 66.52 % , loss : 4.645303726196289\n",
      "step : 66.96 % , loss : 4.629889488220215\n",
      "step : 67.4 % , loss : 4.614069938659668\n",
      "step : 67.84 % , loss : 4.599217891693115\n",
      "step : 68.28 % , loss : 4.691149711608887\n",
      "step : 68.72 % , loss : 4.758734703063965\n",
      "step : 69.16 % , loss : 4.588000774383545\n",
      "step : 69.6 % , loss : 4.6021270751953125\n",
      "step : 70.04 % , loss : 4.549618244171143\n",
      "step : 70.48 % , loss : 4.7773356437683105\n",
      "step : 70.93 % , loss : 4.733514308929443\n",
      "step : 71.37 % , loss : 4.7293782234191895\n",
      "step : 71.81 % , loss : 4.669174671173096\n",
      "step : 72.25 % , loss : 4.662484169006348\n",
      "step : 72.69 % , loss : 4.6515793800354\n",
      "step : 73.13 % , loss : 4.738888263702393\n",
      "step : 73.57 % , loss : 4.53568696975708\n",
      "step : 74.01 % , loss : 4.585002422332764\n",
      "step : 74.45 % , loss : 4.694631576538086\n",
      "step : 74.89 % , loss : 4.583137512207031\n",
      "step : 75.33 % , loss : 4.6373138427734375\n",
      "step : 75.77 % , loss : 4.674610137939453\n",
      "step : 76.21 % , loss : 4.789409160614014\n",
      "step : 76.65 % , loss : 4.537264823913574\n",
      "step : 77.09 % , loss : 4.6486382484436035\n",
      "step : 77.53 % , loss : 4.654249668121338\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-cd73fb4f5d16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mwarmup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-40ba556dd9b2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_reshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iter_max = 100\n",
    "# iter_max = 1000\n",
    "train_losses = []\n",
    "for step in range(iter_max):\n",
    "        train_loss = train(model, train_iter, optimizer, criterion, clip)\n",
    "\n",
    "        if step > warmup:\n",
    "            scheduler.step(valid_loss)\n",
    "        train_losses.append(train_loss)\n",
    "        f = open('result/train_loss.txt', 'w')\n",
    "        f.write(str(train_losses))\n",
    "        f.close()\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "torch.save(model.state_dict(), 'model-final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "print(train_losses)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(train_losses, 'r', label='train')\n",
    "plt.title('training result')\n",
    "plt.grid(True, which='both', axis='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
